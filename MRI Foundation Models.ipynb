{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b6b822-71f3-49ce-aeed-22c8b4febb7f",
   "metadata": {},
   "source": [
    "# MRI Foundation Models: Pre-Trained Models' Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653b3cf-1728-4a6d-b2aa-f01ea0a5dadb",
   "metadata": {},
   "source": [
    "## SPM/CAT12 Preprocessing | AnatCL | HBN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c2d56-25ab-4b65-a537-a8f2288012f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/links/projects/rrg-glatard/arelbaha/HBN_BIDS/new_cat12_preprocessing.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import boutiques\n",
    "from boutiques import bosh\n",
    "from boutiques.descriptor2func import function\n",
    "\n",
    "boutiques_descriptor = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/descriptors/cat12_prepro.json\"\n",
    "base_dir = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/HBN_BIDS\"\n",
    "output_dir = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/preprocessed_output\"\n",
    "\n",
    "bosh([\"exec\", \n",
    "      \"prepare\", \n",
    "      boutiques_descriptor, \n",
    "      \"--imagepath\", \n",
    "      \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif\"\n",
    "])\n",
    "\n",
    "cat12 = boutiques.descriptor2func.function(boutiques_descriptor)\n",
    "\n",
    "t1_nii_files = glob.glob(os.path.join(base_dir, \"sub-*\", \"ses-*\", \"anat\", \"sub-*_T1w.nii\"))\n",
    "print(f\"Found {len(t1_nii_files)} T1w files.\")\n",
    "\n",
    "task_id = int(os.environ[\"SLURM_ARRAY_TASK_ID\"])\n",
    "\n",
    "if task_id >= len(t1_nii_files) or task_id < 0:\n",
    "    print(f\"SLURM_ARRAY_TASK_ID={task_id} out of range\")\n",
    "    exit(1)\n",
    "\n",
    "input_file = t1_nii_files[task_id]\n",
    "\n",
    "path_parts = input_file.split(os.sep)\n",
    "subject_id = next((part for part in path_parts if part.startswith(\"sub-\")), None)\n",
    "\n",
    "if subject_id is None:\n",
    "        print(f\"No subject ID in path: {input_file}\")\n",
    "        exit(1)\n",
    "\n",
    "filename = os.path.basename(input_file)\n",
    "\n",
    "subject_output_dir = os.path.join(output_dir, subject_id)\n",
    "os.makedirs(subject_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Processing={task_id} -> file: {filename} (subject: {subject_id})\")\n",
    "\n",
    "result = cat12('--imagepath=/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif', \n",
    "                   input_file=input_file, \n",
    "                   output_dir=subject_output_dir)\n",
    "\n",
    "result_dict = vars(result)\n",
    "print(\"Available result keys:\", result_dict.keys())\n",
    "print(\"\\nExit code:\", result_dict.get(\"exit_code\"))\n",
    "print(\"\\nStdout:\\n\", result_dict.get (\"stdout\" ))\n",
    "print(\"\\nStderr:\\n\", result_dict.get (\"stderr\"))\n",
    "print(\"\\nOutput files:\\n\", result_dict.get(\"output_files\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d9624-6f3c-41b0-9fdc-54d3878e50ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch --array=500-520\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=CAT12_preproc\n",
    "#SBATCH --account=rrg-glatard\n",
    "#SBATCH --mem=12G\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --output=CAT12_preproc_%A_%a.out\n",
    "#SBATCH --error=CAT12_preproc_%A_%a.err\n",
    "#SBATCH --time=1:0:0\n",
    "\n",
    "source ~/.venvs/jupyter_py3/bin/activate\n",
    "\n",
    "module load apptainer\n",
    "\n",
    "cd ~/links/projects/rrg-glatard/arelbaha/HBN_BIDS #Location of preprocessing data and script\n",
    "\n",
    "echo \"Running task ID: $SLURM_ARRAY_TASK_ID\"\n",
    "\n",
    "python new_cat12_preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56370de7-6f6c-47d8-bf17-ad17b7dbfabb",
   "metadata": {},
   "source": [
    "## SPM/CAT12 Preprocessing | AnatCL | PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dcbe07-4e28-43bc-b009-ae1d394712a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/links/projects/def-glatard/arelbaha/data/inputs/ppmicat12_preprocessing.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import boutiques\n",
    "from boutiques import bosh\n",
    "from boutiques.descriptor2func import function\n",
    "\n",
    "#Downloading Container Part + Paths\n",
    "boutiques_descriptor = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/descriptors/cat12_prepro.json\"\n",
    "base_dir = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data/inputs\"\n",
    "output_dir = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/preprocessed_output\"\n",
    "\n",
    "bosh([\"exec\", \"prepare\", boutiques_descriptor, \"--imagepath\", \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif\"])\n",
    "cat12 = boutiques.descriptor2func.function(boutiques_descriptor)\n",
    "\n",
    "#Task ID Extraction\n",
    "data = glob.glob(os.path.join(base_dir, \"sub-*\", \"ses-*\", \"anat\", \"*.nii\"))\n",
    "print(f\"Found {len(data)}\")\n",
    "\n",
    "task_id = int(os.environ[\"SLURM_ARRAY_TASK_ID\"])\n",
    "\n",
    "if task_id >= len(data) or task_id < 0:\n",
    "    print(f\"SLURM_ARRAY_TASK_ID={task_id} out of range\")\n",
    "    exit(1)\n",
    "\n",
    "input_file = data[task_id]\n",
    "\n",
    "path_parts = input_file.split(os.sep)\n",
    "subject_id = next((part for part in path_parts if part.startswith(\"sub-\")), None)\n",
    "\n",
    "if subject_id is None:\n",
    "        print(f\"Could not find subject ID in path: {input_file}\")\n",
    "        exit(1)\n",
    "\n",
    "filename = os.path.basename(input_file)\n",
    "\n",
    "subject_output_dir = os.path.join(output_dir, subject_id)\n",
    "os.makedirs(subject_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Processing SLURM_ARRAY_TASK_ID ={task_id} -> file: {filename} (subject: {subject_id})\")\n",
    "\n",
    "result = cat12('--imagepath=/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif', \n",
    "                   input_file=input_file, \n",
    "                   output_dir=subject_output_dir)\n",
    "\n",
    "result_dict = vars(result)\n",
    "print(\"Available result keys:\", result_dict.keys())\n",
    "print(\"\\nExit code:\", result_dict.get(\"exit_code\"))\n",
    "print(\"\\nStdout:\\n\", result_dict.get (\"stdout\" ))\n",
    "print(\"\\nStderr:\\n\", result_dict.get (\"stderr\"))\n",
    "print(\"\\nOutput files:\\n\", result_dict.get(\"output_files\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472cc4a-c309-45a3-bc27-2ff39eed37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch --array=1040-1040\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=CAT12_parkinson_preproc\n",
    "#SBATCH --account=rrg-glatard\n",
    "#SBATCH --mem=12G\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --output=parkinson_preproc_%A_%a.out\n",
    "#SBATCH --error=parkinson_preproc_%A_%a.err\n",
    "#SBATCH --time=1:0:0\n",
    "\n",
    "source ~/.venvs/jupyter_py3/bin/activate\n",
    "\n",
    "module load apptainer\n",
    "\n",
    "cd ~/links/projects/def-glatard/arelbaha/data/inputs #Location of preprocessing data and script\n",
    "\n",
    "echo \"Running task ID: $SLURM_ARRAY_TASK_ID\"\n",
    "\n",
    "python classification_parkinson.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015677a-ccfc-42e3-8a47-ee62150d4954",
   "metadata": {},
   "source": [
    "### BrainIAC and CNN Preprocessing | HD-BET | PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c39d8-15dd-469b-913a-c243854f1583",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch --array=23,24\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-glatard\n",
    "#SBATCH --time=20:00:00\n",
    "#SBATCH --mem=16G\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --job-name=brainiac_batch\n",
    "#SBATCH --output=logs/prep_batch_%a.out\n",
    "#SBATCH --error=logs/prep_batch_%a.err\n",
    "\n",
    "export BASE_DIR=\"/home/arelbaha/links/projects/def-glatard/arelbaha/data\"\n",
    "export RAW_DIR=\"${BASE_DIR}/raw_files_brainiac\"\n",
    "export OUTPUT_DIR=\"${BASE_DIR}/processed_outputs\"\n",
    "\n",
    "# Load environment\n",
    "module load python/3.11\n",
    "module load opencv\n",
    "source /home/arelbaha/.venvs/brainiac_env/bin/activate\n",
    "\n",
    "BATCH_DIR=\"${RAW_DIR}/batch_${SLURM_ARRAY_TASK_ID}\"\n",
    "BATCH_OUTPUT=\"${OUTPUT_DIR}/batch_${SLURM_ARRAY_TASK_ID}\"\n",
    "\n",
    "mkdir -p \"$BATCH_OUTPUT\"\n",
    "\n",
    "echo \"Processing batch ${SLURM_ARRAY_TASK_ID} from ${BATCH_DIR}\"\n",
    "\n",
    "python /home/arelbaha/.venvs/brainiac_env/lib/python3.11/site-packages/BrainIAC/src/preprocessing/mri_preprocess_3d_simple.py \\\n",
    "    --temp_img /home/arelbaha/.venvs/brainiac_env/lib/python3.11/site-packages/BrainIAC/src/preprocessing/atlases/temp_head.nii.gz \\\n",
    "    --input_dir \"$BATCH_DIR\" \\\n",
    "    --output_dir \"$BATCH_OUTPUT\"\n",
    "\n",
    "echo \"Completed batch ${SLURM_ARRAY_TASK_ID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06103194-9cdf-45bb-89c0-955135a533e4",
   "metadata": {},
   "source": [
    "## PPMI Multi-task Evaluation | AnatCL vs BrainIAC vs CNN vs FreeSurfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "561b885a-5e97-498e-ab61-2bc89068c747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/arelbaha/links/projects/def-glatard/arelbaha/data/inputs/ppmi_multimodel_comparison.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/links/projects/def-glatard/arelbaha/data/inputs/ppmi_multimodel_comparison.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import nibabel as nib\n",
    "from anatcl import AnatCL\n",
    "from monai.networks.nets import ViT\n",
    "import torch.nn.functional as F\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.set_num_threads(1)\n",
    "torch.set_num_interop_threads(1)\n",
    "\n",
    "CAT12_BASE_DIR = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data/inputs\"\n",
    "DATA_DIR = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data\"\n",
    "LABELS_PATH = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data/processed_cohort_with_mri.csv\"\n",
    "BRAINIAC_MAPPING_CSV = os.path.join(DATA_DIR, \"processed_files_mapping.csv\")\n",
    "ANATCL_ENCODER_PATH = \"/home/arelbaha/.venvs/jupyter_py3/bin\"\n",
    "BRAINIAC_CKPT = \"/home/arelbaha/.venvs/jupyter_py3/bin/BrainIAC.ckpt\"\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Random seed: {SEED}\\n\")\n",
    "\n",
    "# Load demographic labels\n",
    "print(\"Loading demographics\")\n",
    "\n",
    "labels_df = pd.read_csv(LABELS_PATH)\n",
    "id_sex_dict = {}\n",
    "id_parkinson_dict = {}\n",
    "id_age_dict = {}\n",
    "\n",
    "for _, row in labels_df.iterrows():\n",
    "    patno = str(int(row['PATNO']))\n",
    "    sex = row['Sex'].strip().upper()\n",
    "    if sex == 'F':\n",
    "        id_sex_dict[patno] = 1  # Female = 1 (same as HBN)\n",
    "    elif sex == 'M':\n",
    "        id_sex_dict[patno] = 0  # Male = 0 (same as HBN)\n",
    "    group = row['Group'].strip()\n",
    "    if group == 'PD':\n",
    "        id_parkinson_dict[patno] = 1\n",
    "    elif group == 'HC':\n",
    "        id_parkinson_dict[patno] = 0\n",
    "    id_age_dict[patno] = row['Age']\n",
    "\n",
    "print(f\"Loaded demographics for {len(id_sex_dict)} subjects\")\n",
    "print(f\"Sex distribution: {sum(id_sex_dict.values())} Female, {len(id_sex_dict) - sum(id_sex_dict.values())} Male\")\n",
    "print(f\"PD distribution: {sum(id_parkinson_dict.values())} PD, {len(id_parkinson_dict) - sum(id_parkinson_dict.values())} HC\")\n",
    "print(f\"Age range: {min(id_age_dict.values()):.1f} - {max(id_age_dict.values()):.1f} years\\n\")\n",
    "\n",
    "def extract_patno_from_path(filepath):\n",
    "    for part in filepath.split(os.sep):\n",
    "        if part.startswith('sub-'):\n",
    "            return part[4:]\n",
    "    return None\n",
    "\n",
    "# Load CAT12 VBM data\n",
    "print(\"Finding CAT12 (s6mwp1) files\")\n",
    "\n",
    "cat12_files = glob.glob(os.path.join(CAT12_BASE_DIR, \"**\", \"*s6mwp1*.nii*\"), recursive=True)\n",
    "cat12_data = {}\n",
    "for f in cat12_files:\n",
    "    if not os.path.isfile(f):\n",
    "        continue\n",
    "    patno = extract_patno_from_path(f)\n",
    "    if patno and patno in id_sex_dict and patno in id_parkinson_dict and patno in id_age_dict:\n",
    "        cat12_data[patno] = f\n",
    "\n",
    "print(f\"Found {len(cat12_data)} CAT12 files\\n\")\n",
    "\n",
    "# Load BrainIAC data\n",
    "print(\"Finding BrainIAC preprocessed files\")\n",
    "\n",
    "brainiac_df = pd.read_csv(BRAINIAC_MAPPING_CSV).dropna(subset=[\"processed_file\", \"Age\", \"subject_id\"])\n",
    "brainiac_data = {}\n",
    "for _, row in brainiac_df.iterrows():\n",
    "    patno = str(row['subject_id'])\n",
    "    if patno in id_sex_dict and patno in id_parkinson_dict and patno in id_age_dict:\n",
    "        if os.path.exists(row['processed_file']):\n",
    "            brainiac_data[patno] = row['processed_file']\n",
    "\n",
    "print(f\"Found {len(brainiac_data)} BrainIAC files\\n\")\n",
    "\n",
    "# Load FreeSurfer data\n",
    "print(\"Extracting FreeSurfer features\")\n",
    "\n",
    "fs_cth_df = pd.read_csv(os.path.join(CAT12_BASE_DIR, \"FS7_APARC_CTH_23Oct2025.csv\"))\n",
    "fs_sa_df = pd.read_csv(os.path.join(CAT12_BASE_DIR, \"FS7_APARC_SA_23Oct2025.csv\"))\n",
    "fs_cth_df = fs_cth_df[fs_cth_df['EVENT_ID'] == 'BL'].copy()\n",
    "fs_sa_df = fs_sa_df[fs_sa_df['EVENT_ID'] == 'BL'].copy()\n",
    "fs_cth_df['PATNO'] = fs_cth_df['PATNO'].astype(str)\n",
    "fs_sa_df['PATNO'] = fs_sa_df['PATNO'].astype(str)\n",
    "cth_features = [c for c in fs_cth_df.columns if c not in ['PATNO', 'EVENT_ID']]\n",
    "sa_features = [c for c in fs_sa_df.columns if c not in ['PATNO', 'EVENT_ID']]\n",
    "\n",
    "\n",
    "fs_feature_info = []\n",
    "for feat in cth_features:\n",
    "    fs_feature_info.append({\n",
    "        'feature_name': feat,\n",
    "        'feature_type': 'Thickness',\n",
    "        'source_file': 'FS7_APARC_CTH_23Oct2025.csv'\n",
    "    })\n",
    "for feat in sa_features:\n",
    "    fs_feature_info.append({\n",
    "        'feature_name': feat,\n",
    "        'feature_type': 'Surface_Area',\n",
    "        'source_file': 'FS7_APARC_SA_23Oct2025.csv'\n",
    "    })\n",
    "\n",
    "print(f\"Stored feature info: {len(cth_features)} Thickness + {len(sa_features)} Surface_Area features\")\n",
    "print(f\"Total: {len(fs_feature_info)} FreeSurfer features\\n\")\n",
    "\n",
    "# Overall Common Subjects\n",
    "print(\"Overall common subjects across all modalities\")\n",
    "\n",
    "common_subjects = sorted(list(set(cat12_data.keys()) & set(brainiac_data.keys())))\n",
    "fs_data = {}\n",
    "for patno in common_subjects:\n",
    "    cth_row = fs_cth_df[fs_cth_df['PATNO'] == patno]\n",
    "    sa_row = fs_sa_df[fs_sa_df['PATNO'] == patno]\n",
    "    if len(cth_row) > 0 and len(sa_row) > 0:\n",
    "        combined = np.concatenate([cth_row[cth_features].values.flatten(), \n",
    "                                  sa_row[sa_features].values.flatten()])\n",
    "        if not np.any(np.isnan(combined)):\n",
    "            fs_data[patno] = combined\n",
    "\n",
    "common_subjects = sorted(list(set(cat12_data.keys()) & set(brainiac_data.keys()) & set(fs_data.keys())))\n",
    "\n",
    "print(f\"Subjects with CAT12: {len(cat12_data)}\")\n",
    "print(f\"Subjects with BrainIAC: {len(brainiac_data)}\")\n",
    "print(f\"Subjects with FreeSurfer: {len(fs_data)}\")\n",
    "print(f\"Common subjects (all modalities): {len(common_subjects)}\\n\")\n",
    "\n",
    "if len(common_subjects) == 0:\n",
    "    print(\"ERROR: No common subjects found across all modalities\")\n",
    "    exit(1)\n",
    "\n",
    "cat12_paths = [cat12_data[p] for p in common_subjects]\n",
    "brainiac_paths = [brainiac_data[p] for p in common_subjects]\n",
    "fs_features = np.array([fs_data[p] for p in common_subjects])\n",
    "sex_labels = np.array([id_sex_dict[p] for p in common_subjects])\n",
    "parkinson_labels = np.array([id_parkinson_dict[p] for p in common_subjects])\n",
    "age_labels = np.array([id_age_dict[p] for p in common_subjects])\n",
    "\n",
    "print(f\"Final matched dataset: {len(common_subjects)} subjects\")\n",
    "print(f\"FreeSurfer features shape: {fs_features.shape}\")\n",
    "print(f\"Sex distribution: {np.sum(sex_labels)} Female, {len(sex_labels) - np.sum(sex_labels)} Male\")\n",
    "print(f\"PD distribution: {np.sum(parkinson_labels)} PD, {len(parkinson_labels) - np.sum(parkinson_labels)} HC\")\n",
    "print(f\"Age: mean={age_labels.mean():.1f}, range={age_labels.min():.1f}-{age_labels.max():.1f}\\n\")\n",
    "\n",
    "pd.DataFrame({\n",
    "    'subject_id': common_subjects,\n",
    "    'cat12_path': cat12_paths,\n",
    "    'brainiac_path': brainiac_paths,\n",
    "    'sex': sex_labels,\n",
    "    'parkinson': parkinson_labels,\n",
    "    'age': age_labels\n",
    "}).to_csv('ppmi_matched_subjects.csv', index=False)\n",
    "print(\"Saved: ppmi_matched_subjects.csv\\n\")\n",
    "\n",
    "#Dataset Classes\n",
    "\n",
    "class CAT12VBMDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform):\n",
    "        self.data, self.labels, self.transform = data, labels, transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.transform(nib.load(self.data[idx]).get_fdata()).unsqueeze(0)\n",
    "        return img, torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "class BrainIACDataset(Dataset):\n",
    "    def __init__(self, paths, labels, transform):\n",
    "        self.paths, self.labels, self.transform = paths, labels, transform\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.paths[idx]).get_fdata().astype(np.float32)\n",
    "        if img.shape != (96, 96, 96):\n",
    "            img = F.interpolate(torch.from_numpy(img[None, None]), size=(96, 96, 96), \n",
    "                              mode='trilinear', align_corners=False).squeeze().numpy()\n",
    "        img_tensor = torch.from_numpy(img)\n",
    "        img_normalized = self.transform(img_tensor)\n",
    "        return img_normalized[None], torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "class CNNDataset(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.paths[idx]).get_fdata().astype(np.float32)\n",
    "        if img.shape != (92, 110, 92):\n",
    "            img = ndi.zoom(img, [92/img.shape[0], 110/img.shape[1], 92/img.shape[2]], order=1)\n",
    "        img = (img - img.mean()) / (img.std() + 1e-6)\n",
    "        return torch.from_numpy(img[None]).float()\n",
    "\n",
    "class CNN3D(nn.Module):\n",
    "    def __init__(self, dropout_rate=DROPOUT_RATE):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 64, 3, padding=1)\n",
    "        self.bn1, self.pool1, self.drop1 = nn.BatchNorm3d(64), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv2 = nn.Conv3d(64, 64, 3, padding=1)\n",
    "        self.bn2, self.pool2, self.drop2 = nn.BatchNorm3d(64), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv3 = nn.Conv3d(64, 128, 3, padding=1)\n",
    "        self.bn3, self.pool3, self.drop3 = nn.BatchNorm3d(128), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv4 = nn.Conv3d(128, 256, 3, padding=1)\n",
    "        self.bn4, self.pool4, self.drop4 = nn.BatchNorm3d(256), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc1, self.drop5 = nn.Linear(256, 512), nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop1(self.bn1(self.pool1(F.relu(self.conv1(x)))))\n",
    "        x = self.drop2(self.bn2(self.pool2(F.relu(self.conv2(x)))))\n",
    "        x = self.drop3(self.bn3(self.pool3(F.relu(self.conv3(x)))))\n",
    "        x = self.drop4(self.bn4(self.pool4(F.relu(self.conv4(x)))))\n",
    "        return self.drop5(F.relu(self.fc1(self.global_pool(x).view(x.size(0), -1))))\n",
    "\n",
    "def load_brainiac_vit(ckpt_path, device):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    vit = ViT(in_channels=1, img_size=(96, 96, 96), patch_size=(16, 16, 16),\n",
    "              hidden_size=768, mlp_dim=3072, num_layers=12, num_heads=12, classification=False)\n",
    "    pe_key = \"backbone.patch_embedding.position_embeddings\"\n",
    "    if pe_key in ckpt.get(\"state_dict\", ckpt):\n",
    "        vit.patch_embedding.position_embeddings = nn.Parameter(ckpt[\"state_dict\"][pe_key].clone())\n",
    "    backbone_state = {k.replace(\"backbone.\", \"\"): v for k, v in ckpt.get(\"state_dict\", ckpt).items() \n",
    "                     if k.startswith(\"backbone.\")}\n",
    "    vit.load_state_dict(backbone_state, strict=False)\n",
    "    return vit.to(device).eval()\n",
    "\n",
    "# Extracting AnatCL Features\n",
    "print(\"Extracting AnatCL features (averaging across 5 cross-validation folds)\")\n",
    "\n",
    "anatcl_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: torch.from_numpy(x.copy()).float()),\n",
    "    transforms.Normalize(mean=0.0, std=1.0)\n",
    "])\n",
    "\n",
    "num_folds = 5\n",
    "all_fold_features = []\n",
    "\n",
    "for fold_idx in range(num_folds):\n",
    "    path = os.path.join(ANATCL_ENCODER_PATH, f\"fold{fold_idx}.pth\")\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"ERROR: fold{fold_idx}.pth not found at {path}\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"Loading fold {fold_idx}...\")\n",
    "    encoder = AnatCL(descriptor=\"global\", fold=fold_idx, pretrained=False).to(device).eval()\n",
    "    encoder.backbone.load_state_dict(torch.load(path, map_location=device, weights_only=False)['model'])\n",
    "    \n",
    "    for p in encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    dl = DataLoader(CAT12VBMDataset(cat12_paths, age_labels, anatcl_transform), \n",
    "                    batch_size=32, num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fold_features = torch.cat([encoder(vol.to(device)).cpu() for vol, _ in dl]).numpy()\n",
    "    \n",
    "    all_fold_features.append(fold_features)\n",
    "    print(f\"  Fold {fold_idx} features: {fold_features.shape}\")\n",
    "    \n",
    "    del encoder\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "anatcl_features = np.mean(all_fold_features, axis=0)\n",
    "\n",
    "print(f\"\\nAnatCL features (averaged across {num_folds} folds): {anatcl_features.shape}\\n\")\n",
    "\n",
    "#Extracting BrainIAC Features\n",
    "print(\"Extracting BrainIAC features\")\n",
    "\n",
    "brainiac_vit = load_brainiac_vit(BRAINIAC_CKPT, device)\n",
    "brainiac_transform = lambda x: (x - x.mean()) / (x.std() + 1e-6)\n",
    "dl = DataLoader(BrainIACDataset(brainiac_paths, age_labels, brainiac_transform), \n",
    "                batch_size=16, num_workers=0)\n",
    "\n",
    "brainiac_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, _ in dl:\n",
    "        out = brainiac_vit(x.to(device))\n",
    "        \n",
    "        # Handle tuple or tensor\n",
    "        if isinstance(out, tuple):\n",
    "            cls_token = out[0][:, 0]\n",
    "        else:\n",
    "            cls_token = out[:, 0]\n",
    "        \n",
    "        brainiac_features.append(cls_token.cpu().numpy())\n",
    "\n",
    "brainiac_features = np.vstack(brainiac_features)\n",
    "\n",
    "print(f\"BrainIAC features: {brainiac_features.shape}\")\n",
    "del brainiac_vit\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Extracting CNN Features\n",
    "print(\"Extracting CNN features\")\n",
    "\n",
    "cnn_model = CNN3D().to(device).eval()\n",
    "dl = DataLoader(CNNDataset(brainiac_paths), batch_size=8, num_workers=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    cnn_features = np.vstack([cnn_model(x.to(device)).cpu().numpy() for x in dl])\n",
    "\n",
    "print(f\"CNN features: {cnn_features.shape}\\n\")\n",
    "del cnn_model\n",
    "\n",
    "# Organize Features\n",
    "features_dict = {\n",
    "    'AnatCL': anatcl_features,\n",
    "    'BrainIAC': brainiac_features,\n",
    "    'CNN': cnn_features,\n",
    "    'FreeSurfer': fs_features\n",
    "}\n",
    "print(\"Feature extraction complete\")\n",
    "for name, feats in features_dict.items():\n",
    "    print(f\"{name}: {feats.shape}\")\n",
    "print()\n",
    "\n",
    "# Pre-filtering variance check\n",
    "print(\"Pre-filtering variance check:\")\n",
    "for model in ['AnatCL', 'BrainIAC', 'CNN', 'FreeSurfer']:\n",
    "    feats = StandardScaler().fit_transform(features_dict[model])\n",
    "    n_dead = np.sum(np.std(feats, axis=0) < 1e-10)\n",
    "    print(f\"{model}: {feats.shape[1]} total, {n_dead} dead ({100*n_dead/feats.shape[1]:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Correlation Analysis\n",
    "print(\"Computing cross-model feature correlations\")\n",
    "\n",
    "def compute_correlation_within_models(features_dict):\n",
    "    all_features = []\n",
    "    boundaries = [0]\n",
    "    reordered_indices = {}\n",
    "\n",
    "    for model in ['AnatCL', 'BrainIAC', 'CNN', 'FreeSurfer']:\n",
    "        if model not in features_dict:\n",
    "            continue\n",
    "\n",
    "        feats = features_dict[model]\n",
    "        feats = StandardScaler().fit_transform(feats)\n",
    "\n",
    "        valid = ~np.isnan(feats).any(axis=0) & (np.std(feats, axis=0) > 1e-10)\n",
    "        feats = feats[:, valid]\n",
    "\n",
    "        if feats.shape[1] == 0:\n",
    "            continue\n",
    "\n",
    "        print(f\"  {model}: {feats.shape[1]} features\")\n",
    "\n",
    "        corr = np.corrcoef(feats.T)\n",
    "        corr = np.nan_to_num((corr + corr.T) / 2, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        np.fill_diagonal(corr, 1.0)\n",
    "\n",
    "        dist = np.abs(1 - np.abs(corr))\n",
    "        np.fill_diagonal(dist, 0)\n",
    "        condensed = np.nan_to_num(squareform((dist + dist.T) / 2, checks=False), \n",
    "                                  nan=0.0, posinf=1.0, neginf=0.0)\n",
    "\n",
    "        reorder_idx = dendrogram(linkage(condensed, method='ward'), no_plot=True)['leaves']\n",
    "        all_features.append(feats[:, reorder_idx])\n",
    "        boundaries.append(boundaries[-1] + feats.shape[1])\n",
    "        reordered_indices[model] = reorder_idx\n",
    "\n",
    "    combined = np.hstack(all_features)\n",
    "    corr = np.nan_to_num((np.corrcoef(combined.T) + np.corrcoef(combined.T).T) / 2, \n",
    "                         nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "\n",
    "    print(f\"\\nTotal features: {combined.shape[1]}\")\n",
    "    \n",
    "    return corr, boundaries, ['AnatCL', 'BrainIAC', 'CNN', 'FreeSurfer'], reordered_indices\n",
    "\n",
    "corr_matrix, boundaries, model_names, reorder_indices = compute_correlation_within_models(features_dict)\n",
    "pd.DataFrame(corr_matrix).to_csv('ppmi_feature_correlation_matrix.csv', index=False)\n",
    "print(f\"Correlation matrix: {corr_matrix.shape}\")\n",
    "print(f\"Model boundaries: {boundaries}\\n\")\n",
    "\n",
    "# Plot correlation matrix with PDF output\n",
    "print(f\"Plotting FULL cross-model correlation matrix ({corr_matrix.shape[0]}x{corr_matrix.shape[1]} features)...\")\n",
    "\n",
    "try:\n",
    "    fig = plt.figure(figsize=(30, 28))\n",
    "    print(\"Figure created, generating heatmap\")\n",
    "    \n",
    "    ax = sns.heatmap(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1, center=0, \n",
    "                     square=True, linewidths=0, cbar_kws={\"shrink\": 0.3},\n",
    "                     xticklabels=False, yticklabels=False, rasterized=True)\n",
    "    print(\"Heatmap generated, adding boundaries\")\n",
    "    \n",
    "    for b in boundaries[1:-1]:\n",
    "        plt.axhline(y=b, color='black', linewidth=4)\n",
    "        plt.axvline(x=b, color='black', linewidth=4)\n",
    "    \n",
    "    for i, (pos, name) in enumerate(zip([(boundaries[i] + boundaries[i+1]) / 2 \n",
    "                                          for i in range(len(boundaries)-1)], model_names)):\n",
    "        plt.text(pos, -20, name, ha='center', fontsize=20, weight='bold')\n",
    "        plt.text(-20, pos, name, ha='center', va='center', fontsize=20, weight='bold', rotation=90)\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"Saving PDF...\")\n",
    "    plt.savefig('ppmi_cross_model_correlation.pdf', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved: ppmi_cross_model_correlation.pdf\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR with seaborn heatmap: {e}\")\n",
    "    print(\"Attempting fallback with matplotlib imshow\")\n",
    "    \n",
    "    try:\n",
    "        fig = plt.figure(figsize=(24, 22))\n",
    "        \n",
    "        im = plt.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1, aspect='equal', \n",
    "                       interpolation='nearest', origin='upper')\n",
    "        plt.colorbar(im, shrink=0.4)\n",
    "        \n",
    "        for b in boundaries[1:-1]:\n",
    "            plt.axhline(y=b-0.5, color='black', linewidth=3)\n",
    "            plt.axvline(x=b-0.5, color='black', linewidth=3)\n",
    "        \n",
    "        for i, (pos, name) in enumerate(zip([(boundaries[i] + boundaries[i+1]) / 2 \n",
    "                                              for i in range(len(boundaries)-1)], model_names)):\n",
    "            plt.text(pos, -15, name, ha='center', fontsize=16, weight='bold')\n",
    "            plt.text(-15, pos, name, ha='center', va='center', fontsize=16, weight='bold', rotation=90)\n",
    "        \n",
    "        plt.title(f'Cross-Model Feature Correlations ({corr_matrix.shape[0]} features)', \n",
    "                 fontsize=18, weight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('ppmi_cross_model_correlation.pdf', dpi=100, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Saved (imshow fallback): ppmi_cross_model_correlation.pdf\\n\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"ERROR with imshow too: {e2}\")\n",
    "        print(\"Correlation matrix NOT saved as image (CSV available)\\n\")\n",
    "\n",
    "# FREESURFER CORRELATION CLUSTER ANALYSIS\n",
    "print(\"ANALYZING FREESURFER CORRELATION CLUSTERS\")\n",
    "\n",
    "# Extract FreeSurfer block from correlation matrix\n",
    "fs_start_idx = boundaries[3]  # FreeSurfer is 4th model (index 3)\n",
    "fs_end_idx = boundaries[4]\n",
    "fs_corr_block = corr_matrix[fs_start_idx:fs_end_idx, fs_start_idx:fs_end_idx]\n",
    "\n",
    "print(f\"\\nFreeSurfer correlation block: {fs_corr_block.shape}\")\n",
    "\n",
    "# The features are already reordered by hierarchical clustering\n",
    "reordered_fs_indices = reorder_indices['FreeSurfer']\n",
    "\n",
    "# Use the feature info we stored earlier (based on source files)\n",
    "# No assumptions - just use what we stored\n",
    "print(f\"Using {len(fs_feature_info)} features from source files\")\n",
    "\n",
    "# Apply the reordering to get clustered feature labels\n",
    "clustered_features = []\n",
    "for idx in reordered_fs_indices:\n",
    "    feature_info = fs_feature_info[idx].copy()\n",
    "    feature_info['original_index'] = idx\n",
    "    clustered_features.append(feature_info)\n",
    "\n",
    "# Perform hierarchical clustering on FreeSurfer block to identify clusters\n",
    "dist_fs = np.abs(1 - np.abs(fs_corr_block))\n",
    "np.fill_diagonal(dist_fs, 0)\n",
    "condensed_fs = squareform(dist_fs, checks=False)\n",
    "linkage_fs = linkage(condensed_fs, method='ward')\n",
    "\n",
    "# Cut tree to get 2 main clusters\n",
    "cluster_assignments = fcluster(linkage_fs, 2, criterion='maxclust')\n",
    "\n",
    "print(f\"Identified {len(np.unique(cluster_assignments))} clusters\")\n",
    "print(f\"Cluster 1: {np.sum(cluster_assignments == 1)} features\")\n",
    "print(f\"Cluster 2: {np.sum(cluster_assignments == 2)} features\")\n",
    "\n",
    "# Create DataFrame with cluster assignments\n",
    "cluster_df = pd.DataFrame(clustered_features)\n",
    "cluster_df['cluster'] = cluster_assignments\n",
    "cluster_df['reordered_index'] = range(len(clustered_features))\n",
    "\n",
    "# Analyze cluster composition\n",
    "print(\"\\nCluster Composition:\")\n",
    "for cluster_id in [1, 2]:\n",
    "    cluster_data = cluster_df[cluster_df['cluster'] == cluster_id]\n",
    "    n_sa = np.sum(cluster_data['feature_type'] == 'Surface_Area')\n",
    "    n_thick = np.sum(cluster_data['feature_type'] == 'Thickness')\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    print(f\"  Surface Area (from SA file): {n_sa} features ({100*n_sa/len(cluster_data):.1f}%)\")\n",
    "    print(f\"  Thickness (from CTH file): {n_thick} features ({100*n_thick/len(cluster_data):.1f}%)\")\n",
    "\n",
    "# Save detailed cluster information\n",
    "cluster_df_sorted = cluster_df.sort_values(['cluster', 'feature_type', 'feature_name'])\n",
    "cluster_df_sorted.to_csv('ppmi_freesurfer_correlation_clusters.csv', index=False)\n",
    "print(\"\\nSaved: ppmi_freesurfer_correlation_clusters.csv\")\n",
    "\n",
    "# Create separate CSVs for each cluster\n",
    "for cluster_id in [1, 2]:\n",
    "    cluster_data = cluster_df[cluster_df['cluster'] == cluster_id].copy()\n",
    "    cluster_data = cluster_data.sort_values(['feature_type', 'feature_name'])\n",
    "    cluster_data.to_csv(f'ppmi_freesurfer_cluster{cluster_id}_features.csv', index=False)\n",
    "    print(f\"Saved: ppmi_freesurfer_cluster{cluster_id}_features.csv\")\n",
    "\n",
    "# Create summary of cluster characteristics\n",
    "cluster_summary = []\n",
    "for cluster_id in [1, 2]:\n",
    "    cluster_data = cluster_df[cluster_df['cluster'] == cluster_id]\n",
    "    for feat_type in ['Surface_Area', 'Thickness']:\n",
    "        type_data = cluster_data[cluster_data['feature_type'] == feat_type]\n",
    "        if len(type_data) > 0:\n",
    "            cluster_summary.append({\n",
    "                'Cluster': cluster_id,\n",
    "                'Feature_Type': feat_type,\n",
    "                'Count': len(type_data),\n",
    "                'Percentage': 100 * len(type_data) / len(cluster_data),\n",
    "                'Example_Regions': ', '.join(type_data['feature_name'].head(5).tolist())\n",
    "            })\n",
    "\n",
    "pd.DataFrame(cluster_summary).to_csv('ppmi_freesurfer_cluster_summary.csv', index=False)\n",
    "print(\"Saved: ppmi_freesurfer_cluster_summary.csv\")\n",
    "\n",
    "# Check for any \"misclassified\" features\n",
    "cluster1_data = cluster_df[cluster_df['cluster'] == 1]\n",
    "cluster2_data = cluster_df[cluster_df['cluster'] == 2]\n",
    "\n",
    "# Determine which cluster is predominantly SA vs Thickness\n",
    "cluster1_sa_pct = 100 * np.sum(cluster1_data['feature_type'] == 'Surface_Area') / len(cluster1_data)\n",
    "cluster2_sa_pct = 100 * np.sum(cluster2_data['feature_type'] == 'Surface_Area') / len(cluster2_data)\n",
    "\n",
    "if cluster1_sa_pct > 50:\n",
    "    sa_cluster = 1\n",
    "    thick_cluster = 2\n",
    "else:\n",
    "    sa_cluster = 2\n",
    "    thick_cluster = 1\n",
    "\n",
    "# Find thickness features in SA cluster (misclassified)\n",
    "sa_cluster_data = cluster_df[cluster_df['cluster'] == sa_cluster]\n",
    "misclassified_thick = sa_cluster_data[sa_cluster_data['feature_type'] == 'Thickness']\n",
    "\n",
    "# Find SA features in thickness cluster (misclassified)\n",
    "thick_cluster_data = cluster_df[cluster_df['cluster'] == thick_cluster]\n",
    "misclassified_sa = thick_cluster_data[thick_cluster_data['feature_type'] == 'Surface_Area']\n",
    "\n",
    "print(f\"\\nMisclassified Features:\")\n",
    "print(f\"  Thickness (CTH file) features in SA-dominant cluster: {len(misclassified_thick)}\")\n",
    "if len(misclassified_thick) > 0:\n",
    "    print(f\"    Features: {misclassified_thick['feature_name'].tolist()}\")\n",
    "print(f\"  Surface Area (SA file) features in Thickness-dominant cluster: {len(misclassified_sa)}\")\n",
    "if len(misclassified_sa) > 0:\n",
    "    print(f\"    Features: {misclassified_sa['feature_name'].tolist()}\")\n",
    "print()\n",
    "\n",
    "# Model Training Functions\n",
    "\n",
    "def run_classification(X_dict, y, test_size=0.1, task_name=\"Classification\"):\n",
    "    print(f\"Running {task_name}\")\n",
    "    results = {}\n",
    "    cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)), \n",
    "                                        stratify=y, random_state=SEED)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    alphas = [0.01, 0.05, 0.1, 0.15, 0.25, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "    print(\"DummyClassifier\")\n",
    "    dummy_val_auc = []\n",
    "    dummy_val_acc = []\n",
    "    \n",
    "    for train_rel, val_rel in skf.split(cv_idx, y[cv_idx]):\n",
    "        train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "        dummy = DummyClassifier(strategy='stratified', random_state=SEED)\n",
    "        dummy.fit(np.zeros((len(train_idx), 1)), y[train_idx])\n",
    "        \n",
    "        pred = dummy.predict(np.zeros((len(val_idx), 1)))\n",
    "        pred_proba = dummy.predict_proba(np.zeros((len(val_idx), 1)))[:, 1]\n",
    "        \n",
    "        dummy_val_acc.append(balanced_accuracy_score(y[val_idx], pred))\n",
    "        dummy_val_auc.append(roc_auc_score(y[val_idx], pred_proba))\n",
    "    \n",
    "    dummy = DummyClassifier(strategy='stratified', random_state=SEED)\n",
    "    dummy.fit(np.zeros((len(cv_idx), 1)), y[cv_idx])\n",
    "    dummy_test_pred = dummy.predict(np.zeros((len(test_idx), 1)))\n",
    "    dummy_test_proba = dummy.predict_proba(np.zeros((len(test_idx), 1)))[:, 1]\n",
    "    \n",
    "    results['DummyClassifier'] = {\n",
    "        'best_alpha': 1.0,\n",
    "        'test_balanced_accuracy': balanced_accuracy_score(y[test_idx], dummy_test_pred),\n",
    "        'test_auc': roc_auc_score(y[test_idx], dummy_test_proba),\n",
    "        'cv_auc_mean': np.mean(dummy_val_auc),\n",
    "        'cv_auc_std': np.std(dummy_val_auc),\n",
    "        'cv_acc_mean': np.mean(dummy_val_acc),\n",
    "        'cv_acc_std': np.std(dummy_val_acc),\n",
    "        'fold_results': {'val_auc': dummy_val_auc, 'val_acc': dummy_val_acc},\n",
    "        'test_idx': test_idx\n",
    "    }\n",
    "    print(f\"DummyClassifier: AUC={results['DummyClassifier']['test_auc']:.4f}, Bal_Acc={results['DummyClassifier']['test_balanced_accuracy']:.4f}\")\n",
    "\n",
    "    for model, X in X_dict.items():\n",
    "        val_auc = {a: [] for a in alphas}\n",
    "        val_acc = {a: [] for a in alphas}\n",
    "        train_auc = {a: [] for a in alphas}\n",
    "        train_acc = {a: [] for a in alphas}\n",
    "\n",
    "        for train_rel, val_rel in skf.split(cv_idx, y[cv_idx]):\n",
    "            train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "\n",
    "            for alpha in alphas:\n",
    "                n = int(alpha * len(train_idx))\n",
    "                try:\n",
    "                    tr_idx, _ = train_test_split(train_idx, train_size=n, stratify=y[train_idx], random_state=SEED)\n",
    "                except:\n",
    "                    tr_idx = np.random.choice(train_idx, n, replace=False)\n",
    "\n",
    "                if len(np.unique(y[tr_idx])) < 2:\n",
    "                    continue\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                rf = RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_split=5,\n",
    "                                           random_state=SEED, n_jobs=1, \n",
    "                                           max_features='sqrt', class_weight='balanced')\n",
    "                rf.fit(scaler.fit_transform(X[tr_idx]), y[tr_idx])\n",
    "\n",
    "                pred_proba = rf.predict_proba(scaler.transform(X[val_idx]))[:, 1]\n",
    "                val_acc[alpha].append(balanced_accuracy_score(y[val_idx], rf.predict(scaler.transform(X[val_idx]))))\n",
    "                val_auc[alpha].append(roc_auc_score(y[val_idx], pred_proba))\n",
    "                \n",
    "                train_pred_proba = rf.predict_proba(scaler.transform(X[tr_idx]))[:, 1]\n",
    "                train_acc[alpha].append(balanced_accuracy_score(y[tr_idx], rf.predict(scaler.transform(X[tr_idx]))))\n",
    "                train_auc[alpha].append(roc_auc_score(y[tr_idx], train_pred_proba))\n",
    "\n",
    "        avg_auc = {a: np.mean(val_auc[a]) for a in alphas if val_auc[a]}\n",
    "        best_alpha = max(avg_auc, key=avg_auc.get)\n",
    "\n",
    "        # Calculate train-val gaps for all alphas\n",
    "        train_val_gaps = {}\n",
    "        for alpha in alphas:\n",
    "            if train_auc[alpha] and val_auc[alpha]:\n",
    "                train_val_gaps[alpha] = np.mean(train_auc[alpha]) - np.mean(val_auc[alpha])\n",
    "\n",
    "        try:\n",
    "            final_tr, _ = train_test_split(cv_idx, train_size=int(best_alpha * len(cv_idx)), \n",
    "                                          stratify=y[cv_idx], random_state=SEED)\n",
    "        except:\n",
    "            final_tr = cv_idx\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        rf = RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_split=5,\n",
    "                                    random_state=SEED, n_jobs=1, \n",
    "                                    max_features='sqrt', class_weight='balanced')\n",
    "        rf.fit(scaler.fit_transform(X[final_tr]), y[final_tr])\n",
    "        test_pred_proba = rf.predict_proba(scaler.transform(X[test_idx]))[:, 1]\n",
    "\n",
    "        train_val_gap = train_val_gaps[best_alpha]\n",
    "        is_overfitting = train_val_gap > 0.25\n",
    "\n",
    "        results[model] = {\n",
    "            'best_alpha': best_alpha,\n",
    "            'test_balanced_accuracy': balanced_accuracy_score(y[test_idx], rf.predict(scaler.transform(X[test_idx]))),\n",
    "            'test_auc': roc_auc_score(y[test_idx], test_pred_proba),\n",
    "            'cv_auc_mean': np.mean(val_auc[best_alpha]),\n",
    "            'cv_auc_std': np.std(val_auc[best_alpha]),\n",
    "            'cv_acc_mean': np.mean(val_acc[best_alpha]),\n",
    "            'cv_acc_std': np.std(val_acc[best_alpha]),\n",
    "            'train_auc_mean': np.mean(train_auc[best_alpha]),\n",
    "            'train_val_gap': train_val_gap,\n",
    "            'train_val_gaps_all_alphas': train_val_gaps,\n",
    "            'is_overfitting': is_overfitting,\n",
    "            'fold_results': {\n",
    "                'val_auc': val_auc[best_alpha],\n",
    "                'val_acc': val_acc[best_alpha],\n",
    "                'train_auc': train_auc[best_alpha]\n",
    "            },\n",
    "            'cv_results': {\n",
    "                'alphas': alphas, \n",
    "                'acc': {a: np.mean(val_acc[a]) for a in alphas if val_acc[a]},\n",
    "                'auc': avg_auc,\n",
    "                'acc_std': {a: np.std(val_acc[a]) for a in alphas if val_acc[a]},\n",
    "                'auc_std': {a: np.std(val_auc[a]) for a in alphas if val_auc[a]},\n",
    "                'train_auc': {a: np.mean(train_auc[a]) for a in alphas if train_auc[a]},\n",
    "                'train_acc': {a: np.mean(train_acc[a]) for a in alphas if train_acc[a]}\n",
    "            },\n",
    "            'trained_model': rf,\n",
    "            'scaler': scaler,\n",
    "            'test_idx': test_idx\n",
    "        }\n",
    "        print(f\"{model}: AUC={results[model]['test_auc']:.4f}, Bal_Acc={results[model]['test_balanced_accuracy']:.4f}\" + \n",
    "              (f\" [OVERFITTING: train-val gap={train_val_gap:.3f}]\" if is_overfitting else \"\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_regression(X_dict, y, test_size=0.1, task_name=\"Regression\"):\n",
    "    print(f\"Running {task_name}\")\n",
    "    results = {}\n",
    "    cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)), random_state=SEED)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    alphas = [0.01, 0.05, 0.1, 0.15, 0.25, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "    print(\"  DummyRegressor\")\n",
    "    dummy_val_r2 = []\n",
    "    dummy_val_mae = []\n",
    "    \n",
    "    for train_rel, val_rel in kf.split(cv_idx):\n",
    "        train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "        dummy = DummyRegressor(strategy='mean')\n",
    "        dummy.fit(np.zeros((len(train_idx), 1)), y[train_idx])\n",
    "        \n",
    "        pred = dummy.predict(np.zeros((len(val_idx), 1)))\n",
    "        dummy_val_r2.append(r2_score(y[val_idx], pred))\n",
    "        dummy_val_mae.append(mean_absolute_error(y[val_idx], pred))\n",
    "    \n",
    "    dummy = DummyRegressor(strategy='mean')\n",
    "    dummy.fit(np.zeros((len(cv_idx), 1)), y[cv_idx])\n",
    "    dummy_test_pred = dummy.predict(np.zeros((len(test_idx), 1)))\n",
    "    \n",
    "    results['DummyRegressor'] = {\n",
    "        'best_alpha': 1.0,\n",
    "        'test_r2': r2_score(y[test_idx], dummy_test_pred),\n",
    "        'test_mae': mean_absolute_error(y[test_idx], dummy_test_pred),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y[test_idx], dummy_test_pred)),\n",
    "        'cv_r2_mean': np.mean(dummy_val_r2),\n",
    "        'cv_r2_std': np.std(dummy_val_r2),\n",
    "        'cv_mae_mean': np.mean(dummy_val_mae),\n",
    "        'cv_mae_std': np.std(dummy_val_mae),\n",
    "        'fold_results': {'val_r2': dummy_val_r2, 'val_mae': dummy_val_mae},\n",
    "        'test_idx': test_idx\n",
    "    }\n",
    "    print(f\"  DummyRegressor: R2={results['DummyRegressor']['test_r2']:.4f}, MAE={results['DummyRegressor']['test_mae']:.2f}\")\n",
    "\n",
    "    for model, X in X_dict.items():\n",
    "        val_r2 = {a: [] for a in alphas}\n",
    "        val_mae = {a: [] for a in alphas}\n",
    "        train_r2 = {a: [] for a in alphas}\n",
    "        train_mae = {a: [] for a in alphas}\n",
    "\n",
    "        for train_rel, val_rel in kf.split(cv_idx):\n",
    "            train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "\n",
    "            for alpha in alphas:\n",
    "                n = int(alpha * len(train_idx))\n",
    "                tr_idx, _ = train_test_split(train_idx, train_size=n, random_state=SEED) \\\n",
    "                           if n < len(train_idx) else (train_idx, None)\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                rf = RandomForestRegressor(n_estimators=200, max_depth=6, min_samples_split=5, \n",
    "                                          random_state=SEED, n_jobs=1)\n",
    "                rf.fit(scaler.fit_transform(X[tr_idx]), y[tr_idx])\n",
    "\n",
    "                pred = rf.predict(scaler.transform(X[val_idx]))\n",
    "                val_r2[alpha].append(r2_score(y[val_idx], pred))\n",
    "                val_mae[alpha].append(mean_absolute_error(y[val_idx], pred))\n",
    "                \n",
    "                train_pred = rf.predict(scaler.transform(X[tr_idx]))\n",
    "                train_r2[alpha].append(r2_score(y[tr_idx], train_pred))\n",
    "                train_mae[alpha].append(mean_absolute_error(y[tr_idx], train_pred))\n",
    "\n",
    "        avg_r2 = {a: np.mean(val_r2[a]) for a in alphas if val_r2[a]}\n",
    "        best_alpha = max(avg_r2, key=avg_r2.get)\n",
    "\n",
    "        # Calculate train-val gaps for all alphas\n",
    "        train_val_gaps = {}\n",
    "        for alpha in alphas:\n",
    "            if train_r2[alpha] and val_r2[alpha]:\n",
    "                train_val_gaps[alpha] = np.mean(train_r2[alpha]) - np.mean(val_r2[alpha])\n",
    "\n",
    "        n = int(best_alpha * len(cv_idx))\n",
    "        final_tr, _ = train_test_split(cv_idx, train_size=n, random_state=SEED) \\\n",
    "                     if n < len(cv_idx) else (cv_idx, None)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        rf = RandomForestRegressor(n_estimators=200, max_depth=6, min_samples_split=5, random_state=SEED, n_jobs=1)\n",
    "        rf.fit(scaler.fit_transform(X[final_tr]), y[final_tr])\n",
    "        test_pred = rf.predict(scaler.transform(X[test_idx]))\n",
    "\n",
    "        train_val_gap = train_val_gaps[best_alpha]\n",
    "        is_overfitting = train_val_gap > 0.35\n",
    "\n",
    "        results[model] = {\n",
    "            'best_alpha': best_alpha,\n",
    "            'test_r2': r2_score(y[test_idx], test_pred),\n",
    "            'test_mae': mean_absolute_error(y[test_idx], test_pred),\n",
    "            'test_rmse': np.sqrt(mean_squared_error(y[test_idx], test_pred)),\n",
    "            'predictions': test_pred,\n",
    "            'actual': y[test_idx],\n",
    "            'cv_r2_mean': np.mean(val_r2[best_alpha]),\n",
    "            'cv_r2_std': np.std(val_r2[best_alpha]),\n",
    "            'cv_mae_mean': np.mean(val_mae[best_alpha]),\n",
    "            'cv_mae_std': np.std(val_mae[best_alpha]),\n",
    "            'train_r2_mean': np.mean(train_r2[best_alpha]),\n",
    "            'train_val_gap': train_val_gap,\n",
    "            'train_val_gaps_all_alphas': train_val_gaps,\n",
    "            'is_overfitting': is_overfitting,\n",
    "            'fold_results': {\n",
    "                'val_r2': val_r2[best_alpha],\n",
    "                'val_mae': val_mae[best_alpha],\n",
    "                'train_r2': train_r2[best_alpha]\n",
    "            },\n",
    "            'cv_results': {\n",
    "                'alphas': alphas, \n",
    "                'r2': avg_r2, \n",
    "                'mae': {a: np.mean(val_mae[a]) for a in alphas if val_mae[a]},\n",
    "                'r2_std': {a: np.std(val_r2[a]) for a in alphas if val_r2[a]},\n",
    "                'mae_std': {a: np.std(val_mae[a]) for a in alphas if val_mae[a]},\n",
    "                'train_r2': {a: np.mean(train_r2[a]) for a in alphas if train_r2[a]},\n",
    "                'train_mae': {a: np.mean(train_mae[a]) for a in alphas if train_mae[a]}\n",
    "            },\n",
    "            'trained_model': rf,\n",
    "            'scaler': scaler,\n",
    "            'test_idx': test_idx\n",
    "        }\n",
    "        print(f\"  {model}: R2={results[model]['test_r2']:.4f}, MAE={results[model]['test_mae']:.2f}\" +\n",
    "              (f\" [OVERFITTING: train-val gap={train_val_gap:.3f}]\" if is_overfitting else \"\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run Sex Classification\n",
    "print(\"Sex Classification\")\n",
    "sex_results = run_classification(features_dict, sex_labels, test_size=0.1, task_name=\"Sex classification\")\n",
    "print()\n",
    "\n",
    "# Run Parkinson Classification\n",
    "print(\"Parkinson Classification\")\n",
    "parkinson_results = run_classification(features_dict, parkinson_labels, test_size=0.1, task_name=\"Parkinson classification\")\n",
    "print()\n",
    "\n",
    "# NEW: SEX-STRATIFIED AGE PREDICTION (matching HBN)\n",
    "print(\"AGE PREDICTION - SPLIT BY SEX\")\n",
    "\n",
    "# Male subjects (sex_labels == 0)\n",
    "male_indices = np.where(sex_labels == 0)[0]\n",
    "male_features_dict = {k: v[male_indices] for k, v in features_dict.items()}\n",
    "male_age_labels = age_labels[male_indices]\n",
    "\n",
    "print(f\"\\nMale subjects: N={len(male_indices)}\")\n",
    "print(f\"Male age: mean={male_age_labels.mean():.1f}, std={male_age_labels.std():.2f}, range={male_age_labels.min():.1f}-{male_age_labels.max():.1f}\")\n",
    "\n",
    "male_age_results = run_regression(male_features_dict, male_age_labels, test_size=0.1, task_name=\"Male age prediction\")\n",
    "print()\n",
    "\n",
    "# Female subjects (sex_labels == 1)\n",
    "female_indices = np.where(sex_labels == 1)[0]\n",
    "female_features_dict = {k: v[female_indices] for k, v in features_dict.items()}\n",
    "female_age_labels = age_labels[female_indices]\n",
    "\n",
    "print(f\"\\nFemale subjects: N={len(female_indices)}\")\n",
    "print(f\"Female age: mean={female_age_labels.mean():.1f}, std={female_age_labels.std():.2f}, range={female_age_labels.min():.1f}-{female_age_labels.max():.1f}\")\n",
    "\n",
    "female_age_results = run_regression(female_features_dict, female_age_labels, test_size=0.1, task_name=\"Female age prediction\")\n",
    "print()\n",
    "\n",
    "# Overfitting Analysis\n",
    "print(\"Overfitting Analysis\")\n",
    "overfitting_report = []\n",
    "for model in features_dict.keys():\n",
    "    sex_overfit = sex_results[model]['is_overfitting']\n",
    "    parkinson_overfit = parkinson_results[model]['is_overfitting']\n",
    "    male_age_overfit = male_age_results[model]['is_overfitting']\n",
    "    female_age_overfit = female_age_results[model]['is_overfitting']\n",
    "    \n",
    "    overfitting_report.append({\n",
    "        'Model': model,\n",
    "        'Sex_Overfitting': sex_overfit,\n",
    "        'Sex_Train_Val_Gap': sex_results[model]['train_val_gap'],\n",
    "        'Parkinson_Overfitting': parkinson_overfit,\n",
    "        'Parkinson_Train_Val_Gap': parkinson_results[model]['train_val_gap'],\n",
    "        'Male_Age_Overfitting': male_age_overfit,\n",
    "        'Male_Age_Train_Val_Gap': male_age_results[model]['train_val_gap'],\n",
    "        'Female_Age_Overfitting': female_age_overfit,\n",
    "        'Female_Age_Train_Val_Gap': female_age_results[model]['train_val_gap']\n",
    "    })\n",
    "    \n",
    "    if sex_overfit or parkinson_overfit or male_age_overfit or female_age_overfit:\n",
    "        print(f\"{model}:\")\n",
    "        if sex_overfit:\n",
    "            print(f\"  Sex classification: Train-Val AUC gap = {sex_results[model]['train_val_gap']:.3f}\")\n",
    "        if parkinson_overfit:\n",
    "            print(f\"  Parkinson classification: Train-Val AUC gap = {parkinson_results[model]['train_val_gap']:.3f}\")\n",
    "        if male_age_overfit:\n",
    "            print(f\"  Male age prediction: Train-Val R gap = {male_age_results[model]['train_val_gap']:.3f}\")\n",
    "        if female_age_overfit:\n",
    "            print(f\"  Female age prediction: Train-Val R gap = {female_age_results[model]['train_val_gap']:.3f}\")\n",
    "\n",
    "if not any(r['Sex_Overfitting'] or r['Parkinson_Overfitting'] or r['Male_Age_Overfitting'] or r['Female_Age_Overfitting'] for r in overfitting_report):\n",
    "    print(\"No significant overfitting detected\")\n",
    "\n",
    "pd.DataFrame(overfitting_report).to_csv('ppmi_overfitting_analysis.csv', index=False)\n",
    "print(\"\\nSaved: ppmi_overfitting_analysis.csv\\n\")\n",
    "\n",
    "# Save train-val gaps for all alphas\n",
    "print(\"Saving train-val gaps for all alphas...\")\n",
    "gap_rows = []\n",
    "for model in features_dict.keys():\n",
    "    model_alphas = sex_results[model]['cv_results']['alphas']\n",
    "    for alpha in model_alphas:\n",
    "        sex_gap = sex_results[model]['train_val_gaps_all_alphas'].get(alpha, np.nan)\n",
    "        parkinson_gap = parkinson_results[model]['train_val_gaps_all_alphas'].get(alpha, np.nan)\n",
    "        male_age_gap = male_age_results[model]['train_val_gaps_all_alphas'].get(alpha, np.nan)\n",
    "        female_age_gap = female_age_results[model]['train_val_gaps_all_alphas'].get(alpha, np.nan)\n",
    "        gap_rows.append({\n",
    "            'Model': model,\n",
    "            'Alpha': alpha,\n",
    "            'Sex_Train_Val_Gap': sex_gap,\n",
    "            'Parkinson_Train_Val_Gap': parkinson_gap,\n",
    "            'Male_Age_Train_Val_Gap': male_age_gap,\n",
    "            'Female_Age_Train_Val_Gap': female_age_gap\n",
    "        })\n",
    "\n",
    "pd.DataFrame(gap_rows).to_csv('ppmi_train_val_gaps_all_alphas.csv', index=False)\n",
    "print(\"Saved: ppmi_train_val_gaps_all_alphas.csv\\n\")\n",
    "\n",
    "# Saving Results\n",
    "print(\"=\"*80)\n",
    "print(\"Saving Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_models = ['DummyClassifier'] + list(features_dict.keys())\n",
    "\n",
    "pd.DataFrame([{'Model': m, 'Best_Alpha': sex_results[m]['best_alpha'], \n",
    "               'Test_Balanced_Accuracy': sex_results[m]['test_balanced_accuracy'], \n",
    "               'Test_AUC': sex_results[m]['test_auc'],\n",
    "               'CV_AUC_Mean': sex_results[m]['cv_auc_mean'],\n",
    "               'CV_AUC_Std': sex_results[m]['cv_auc_std']} \n",
    "              for m in all_models]).to_csv('ppmi_sex_classification_summary.csv', index=False)\n",
    "\n",
    "pd.DataFrame([{'Model': m, 'Best_Alpha': parkinson_results[m]['best_alpha'], \n",
    "               'Test_Balanced_Accuracy': parkinson_results[m]['test_balanced_accuracy'], \n",
    "               'Test_AUC': parkinson_results[m]['test_auc'],\n",
    "               'CV_AUC_Mean': parkinson_results[m]['cv_auc_mean'],\n",
    "               'CV_AUC_Std': parkinson_results[m]['cv_auc_std']} \n",
    "              for m in all_models]).to_csv('ppmi_parkinson_classification_summary.csv', index=False)\n",
    "\n",
    "all_models_reg = ['DummyRegressor'] + list(features_dict.keys())\n",
    "\n",
    "pd.DataFrame([{'Model': m, 'Best_Alpha': male_age_results[m]['best_alpha'], \n",
    "               'Test_R2': male_age_results[m]['test_r2'], \n",
    "               'Test_MAE': male_age_results[m]['test_mae'], \n",
    "               'Test_RMSE': male_age_results[m]['test_rmse'],\n",
    "               'CV_R2_Mean': male_age_results[m]['cv_r2_mean'],\n",
    "               'CV_R2_Std': male_age_results[m]['cv_r2_std'],\n",
    "               'CV_MAE_Mean': male_age_results[m]['cv_mae_mean'],\n",
    "               'CV_MAE_Std': male_age_results[m]['cv_mae_std']} \n",
    "              for m in all_models_reg]).to_csv('ppmi_male_age_prediction_summary.csv', index=False)\n",
    "\n",
    "pd.DataFrame([{'Model': m, 'Best_Alpha': female_age_results[m]['best_alpha'], \n",
    "               'Test_R2': female_age_results[m]['test_r2'], \n",
    "               'Test_MAE': female_age_results[m]['test_mae'], \n",
    "               'Test_RMSE': female_age_results[m]['test_rmse'],\n",
    "               'CV_R2_Mean': female_age_results[m]['cv_r2_mean'],\n",
    "               'CV_R2_Std': female_age_results[m]['cv_r2_std'],\n",
    "               'CV_MAE_Mean': female_age_results[m]['cv_mae_mean'],\n",
    "               'CV_MAE_Std': female_age_results[m]['cv_mae_std']} \n",
    "              for m in all_models_reg]).to_csv('ppmi_female_age_prediction_summary.csv', index=False)\n",
    "\n",
    "all_results = [\n",
    "    {'Model': m, \n",
    "     'Sex_AUC': sex_results[m]['test_auc'], \n",
    "     'Sex_Bal_Acc': sex_results[m]['test_balanced_accuracy'],\n",
    "     'Parkinson_AUC': parkinson_results[m]['test_auc'], \n",
    "     'Parkinson_Bal_Acc': parkinson_results[m]['test_balanced_accuracy'],\n",
    "     'Male_Age_R2': male_age_results[m]['test_r2'], \n",
    "     'Male_Age_MAE': male_age_results[m]['test_mae'], \n",
    "     'Male_Age_RMSE': male_age_results[m]['test_rmse'],\n",
    "     'Female_Age_R2': female_age_results[m]['test_r2'], \n",
    "     'Female_Age_MAE': female_age_results[m]['test_mae'], \n",
    "     'Female_Age_RMSE': female_age_results[m]['test_rmse']} \n",
    "    for m in list(features_dict.keys())\n",
    "]\n",
    "pd.DataFrame(all_results).to_csv('ppmi_detailed_comparison.csv', index=False)\n",
    "\n",
    "rankings = []\n",
    "for task in ['Sex_AUC', 'Parkinson_AUC', 'Male_Age_R2', 'Female_Age_R2']:\n",
    "    for rank, item in enumerate(sorted(all_results, key=lambda x: x[task], reverse=True), 1):\n",
    "        rankings.append({\n",
    "            'Task': task.replace('_', ' '), \n",
    "            'Rank': rank, \n",
    "            'Model': item['Model'], \n",
    "            'Score': item[task]\n",
    "        })\n",
    "pd.DataFrame(rankings).to_csv('ppmi_model_rankings.csv', index=False)\n",
    "\n",
    "print(\"Saved: ppmi_sex_classification_summary.csv\")\n",
    "print(\"Saved: ppmi_parkinson_classification_summary.csv\")\n",
    "print(\"Saved: ppmi_male_age_prediction_summary.csv\")\n",
    "print(\"Saved: ppmi_female_age_prediction_summary.csv\")\n",
    "print(\"Saved: ppmi_detailed_comparison.csv\")\n",
    "print(\"Saved: ppmi_model_rankings.csv\\n\")\n",
    "\n",
    "# Generate Learning Curve Visualizations (4 PLOTS)\n",
    "print(\"=\"*80)\n",
    "print(\"Generating Learning Curves\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models = list(features_dict.keys())\n",
    "\n",
    "# SAME MODEL STYLES AS HBN\n",
    "model_styles = {\n",
    "    'AnatCL': {'color': '#9B59B6', 'marker': '^', 'linestyle': '-', 'label_prefix': 'FM: '},\n",
    "    'BrainIAC': {'color': '#E74C3C', 'marker': '^', 'linestyle': '-', 'label_prefix': 'FM: '},\n",
    "    'CNN': {'color': '#3498DB', 'marker': 'o', 'linestyle': '--', 'label_prefix': ''},\n",
    "    'FreeSurfer': {'color': '#E67E22', 'marker': 's', 'linestyle': '-', 'label_prefix': ''}\n",
    "}\n",
    "\n",
    "n_cv_sex = len(sex_labels) - int(0.1 * len(sex_labels))\n",
    "n_cv_parkinson = len(parkinson_labels) - int(0.1 * len(parkinson_labels))\n",
    "n_cv_male = len(male_age_labels) - int(0.1 * len(male_age_labels))\n",
    "n_cv_female = len(female_age_labels) - int(0.1 * len(female_age_labels))\n",
    "\n",
    "alphas = [0.01, 0.05, 0.1, 0.15, 0.25, 0.4, 0.6, 0.8, 1.0]\n",
    "training_sizes_sex = [int(alpha * n_cv_sex * 0.8) for alpha in alphas]\n",
    "training_sizes_parkinson = [int(alpha * n_cv_parkinson * 0.8) for alpha in alphas]\n",
    "training_sizes_male = [int(alpha * n_cv_male * 0.8) for alpha in alphas]\n",
    "training_sizes_female = [int(alpha * n_cv_female * 0.8) for alpha in alphas]\n",
    "\n",
    "# 4 PLOTS: Parkinson, Male Age, Sex, Female Age\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 12))\n",
    "\n",
    "# PLOT 1: Parkinson classification\n",
    "for model in models:\n",
    "    style = model_styles[model]\n",
    "    acc_means = []\n",
    "    acc_stds = []\n",
    "    valid_sizes = []\n",
    "    \n",
    "    for i, alpha in enumerate(alphas):\n",
    "        if alpha in parkinson_results[model]['cv_results']['acc']:\n",
    "            acc_means.append(parkinson_results[model]['cv_results']['acc'][alpha])\n",
    "            acc_stds.append(parkinson_results[model]['cv_results']['acc_std'][alpha])\n",
    "            valid_sizes.append(training_sizes_parkinson[i])\n",
    "    \n",
    "    acc_means = np.array(acc_means)\n",
    "    acc_stds = np.array(acc_stds)\n",
    "    valid_sizes = np.array(valid_sizes)\n",
    "    \n",
    "    ax1.plot(valid_sizes, acc_means, marker=style['marker'], linestyle=style['linestyle'],\n",
    "             linewidth=3, markersize=10, label=style['label_prefix'] + model, \n",
    "             color=style['color'])\n",
    "    ax1.fill_between(valid_sizes, acc_means - acc_stds, acc_means + acc_stds, \n",
    "                      alpha=0.2, color=style['color'])\n",
    "\n",
    "dummy_acc_cv_pd = parkinson_results['DummyClassifier']['cv_acc_mean']\n",
    "ax1.axhline(y=dummy_acc_cv_pd, color='gray', linestyle=':', linewidth=2, \n",
    "            label=f'Baseline ({dummy_acc_cv_pd:.3f})', alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Number of Training Subjects', fontsize=12, weight='bold')\n",
    "ax1.set_ylabel('Balanced Accuracy', fontsize=12, weight='bold')\n",
    "ax1.set_title('Parkinson Classification', fontsize=14, weight='bold')\n",
    "ax1.legend(fontsize=9, loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# PLOT 2: Male age prediction\n",
    "for model in models:\n",
    "    style = model_styles[model]\n",
    "    mae_means = []\n",
    "    mae_stds = []\n",
    "    valid_sizes = []\n",
    "    \n",
    "    for i, alpha in enumerate(alphas):\n",
    "        if alpha in male_age_results[model]['cv_results']['mae']:\n",
    "            mae_means.append(male_age_results[model]['cv_results']['mae'][alpha])\n",
    "            mae_stds.append(male_age_results[model]['cv_results']['mae_std'][alpha])\n",
    "            valid_sizes.append(training_sizes_male[i])\n",
    "    \n",
    "    mae_means = np.array(mae_means)\n",
    "    mae_stds = np.array(mae_stds)\n",
    "    valid_sizes = np.array(valid_sizes)\n",
    "    \n",
    "    ax2.plot(valid_sizes, mae_means, marker=style['marker'], linestyle=style['linestyle'],\n",
    "             linewidth=3, markersize=10, label=style['label_prefix'] + model,\n",
    "             color=style['color'])\n",
    "    ax2.fill_between(valid_sizes, mae_means - mae_stds, mae_means + mae_stds,\n",
    "                      alpha=0.2, color=style['color'])\n",
    "\n",
    "dummy_mae_cv_male = male_age_results['DummyRegressor']['cv_mae_mean']\n",
    "ax2.axhline(y=dummy_mae_cv_male, color='gray', linestyle=':', linewidth=2, \n",
    "            label=f'Baseline ({dummy_mae_cv_male:.2f})', alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('Number of Training Subjects', fontsize=12, weight='bold')\n",
    "ax2.set_ylabel('MAE (years)', fontsize=12, weight='bold')\n",
    "ax2.set_title('Male Age Prediction', fontsize=14, weight='bold')\n",
    "ax2.legend(fontsize=9, loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# PLOT 3: Sex classification\n",
    "for model in models:\n",
    "    style = model_styles[model]\n",
    "    acc_means = []\n",
    "    acc_stds = []\n",
    "    valid_sizes = []\n",
    "    \n",
    "    for i, alpha in enumerate(alphas):\n",
    "        if alpha in sex_results[model]['cv_results']['acc']:\n",
    "            acc_means.append(sex_results[model]['cv_results']['acc'][alpha])\n",
    "            acc_stds.append(sex_results[model]['cv_results']['acc_std'][alpha])\n",
    "            valid_sizes.append(training_sizes_sex[i])\n",
    "    \n",
    "    acc_means = np.array(acc_means)\n",
    "    acc_stds = np.array(acc_stds)\n",
    "    valid_sizes = np.array(valid_sizes)\n",
    "    \n",
    "    ax3.plot(valid_sizes, acc_means, marker=style['marker'], linestyle=style['linestyle'],\n",
    "             linewidth=3, markersize=10, label=style['label_prefix'] + model, \n",
    "             color=style['color'])\n",
    "    ax3.fill_between(valid_sizes, acc_means - acc_stds, acc_means + acc_stds, \n",
    "                      alpha=0.2, color=style['color'])\n",
    "\n",
    "dummy_acc_cv_sex = sex_results['DummyClassifier']['cv_acc_mean']\n",
    "ax3.axhline(y=dummy_acc_cv_sex, color='gray', linestyle=':', linewidth=2, \n",
    "            label=f'Baseline ({dummy_acc_cv_sex:.3f})', alpha=0.7)\n",
    "\n",
    "ax3.set_xlabel('Number of Training Subjects', fontsize=12, weight='bold')\n",
    "ax3.set_ylabel('Balanced Accuracy', fontsize=12, weight='bold')\n",
    "ax3.set_title('Sex Classification', fontsize=14, weight='bold')\n",
    "ax3.legend(fontsize=9, loc='lower right')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim([0, 1])\n",
    "\n",
    "# PLOT 4: Female age prediction\n",
    "for model in models:\n",
    "    style = model_styles[model]\n",
    "    mae_means = []\n",
    "    mae_stds = []\n",
    "    valid_sizes = []\n",
    "    \n",
    "    for i, alpha in enumerate(alphas):\n",
    "        if alpha in female_age_results[model]['cv_results']['mae']:\n",
    "            mae_means.append(female_age_results[model]['cv_results']['mae'][alpha])\n",
    "            mae_stds.append(female_age_results[model]['cv_results']['mae_std'][alpha])\n",
    "            valid_sizes.append(training_sizes_female[i])\n",
    "    \n",
    "    mae_means = np.array(mae_means)\n",
    "    mae_stds = np.array(mae_stds)\n",
    "    valid_sizes = np.array(valid_sizes)\n",
    "    \n",
    "    ax4.plot(valid_sizes, mae_means, marker=style['marker'], linestyle=style['linestyle'],\n",
    "             linewidth=3, markersize=10, label=style['label_prefix'] + model,\n",
    "             color=style['color'])\n",
    "    ax4.fill_between(valid_sizes, mae_means - mae_stds, mae_means + mae_stds,\n",
    "                      alpha=0.2, color=style['color'])\n",
    "\n",
    "dummy_mae_cv_female = female_age_results['DummyRegressor']['cv_mae_mean']\n",
    "ax4.axhline(y=dummy_mae_cv_female, color='gray', linestyle=':', linewidth=2, \n",
    "            label=f'Baseline ({dummy_mae_cv_female:.2f})', alpha=0.7)\n",
    "\n",
    "ax4.set_xlabel('Number of Training Subjects', fontsize=12, weight='bold')\n",
    "ax4.set_ylabel('MAE (years)', fontsize=12, weight='bold')\n",
    "ax4.set_title('Female Age Prediction', fontsize=14, weight='bold')\n",
    "ax4.legend(fontsize=9, loc='upper right')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ppmi_cv_learning_curves.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: ppmi_cv_learning_curves.pdf\\n\")\n",
    "\n",
    "print(f\"\\nDataset: {len(common_subjects)} subjects\")\n",
    "print(f\"Age range: {age_labels.min():.1f} - {age_labels.max():.1f} years\")\n",
    "print(f\"Sex: {np.sum(sex_labels)} Female, {len(sex_labels) - np.sum(sex_labels)} Male\")\n",
    "print(f\"Males: {len(male_indices)} subjects\")\n",
    "print(f\"Females: {len(female_indices)} subjects\")\n",
    "print(f\"PD: {np.sum(parkinson_labels)} PD, {len(parkinson_labels) - np.sum(parkinson_labels)} HC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc4053-2a9c-4966-887a-aea3c11b52f1",
   "metadata": {},
   "source": [
    "## BrainIAC and CNN Preprocessing | HD-BET | HBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfebf287-f626-4640-967a-b16c3dc1333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch --array=1-100\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-glatard\n",
    "#SBATCH --time=12:00:00\n",
    "#SBATCH --mem=16G\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --job-name=brainiac_batch\n",
    "#SBATCH --output=logs/prep_batch_%a.out\n",
    "#SBATCH --error=logs/prep_batch_%a.err\n",
    "\n",
    "export BASE_DIR=\"/home/arelbaha/links/projects/rrg-glatard/arelbaha\"\n",
    "export RAW_DIR=\"${BASE_DIR}/brainiac_p_files\"\n",
    "export OUTPUT_DIR=\"${BASE_DIR}/brainiac_p_outputs\"\n",
    "\n",
    "module load python/3.11\n",
    "module load opencv\n",
    "source /home/arelbaha/.venvs/brainiac_env/bin/activate\n",
    "\n",
    "#Batch\n",
    "BATCH_NUM=$(printf \"%03d\" ${SLURM_ARRAY_TASK_ID})\n",
    "BATCH_DIR=\"${RAW_DIR}/batch_${BATCH_NUM}\"\n",
    "BATCH_OUTPUT=\"${OUTPUT_DIR}/batch_${BATCH_NUM}\"\n",
    "\n",
    "mkdir -p \"$BATCH_OUTPUT\"\n",
    "echo \"Processing batch ${BATCH_NUM} from ${BATCH_DIR}\"\n",
    "\n",
    "#Preprocessing\n",
    "python /home/arelbaha/.venvs/brainiac_env/lib/python3.11/site-packages/BrainIAC/src/preprocessing/mri_preprocess_3d_simple.py \\\n",
    "    --temp_img /home/arelbaha/.venvs/brainiac_env/lib/python3.11/site-packages/BrainIAC/src/preprocessing/atlases/temp_head.nii.gz \\\n",
    "    --input_dir \"$BATCH_DIR\" \\\n",
    "    --output_dir \"$BATCH_OUTPUT\"\n",
    "\n",
    "echo \"Completed batch ${BATCH_NUM}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb41b4-95b0-4a46-9c0b-74079bdb4e0b",
   "metadata": {},
   "source": [
    "## HBN Multi-task Evaluation | AnatCl vs BrainIAC vs CNN vs FreeSurfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97843354-48ff-48e4-9083-dbc9f8dd226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/arelbaha/links/projects/rrg-glatard/arelbaha/debug_hbn_multimodel_comparison.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/arelbaha/links/projects/rrg-glatard/arelbaha/debug_hbn_multimodel_comparison.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fnmatch\n",
    "import json\n",
    "import pickle\n",
    "from nilearn import plotting, datasets\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import nibabel as nib\n",
    "from anatcl import AnatCL\n",
    "from monai.networks.nets import ViT\n",
    "import torch.nn.functional as F\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.ndimage as ndi\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.set_num_threads(1)\n",
    "torch.set_num_interop_threads(1)\n",
    "\n",
    "# PATHS\n",
    "HBN_BIDS = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/HBN_BIDS\"\n",
    "HBN_BIDS_LOWER = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/hbn_bids\"\n",
    "BRAINIAC_OUT = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/brainiac_p_outputs\"\n",
    "FREESURFER_DIR = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/HBN_FreeSurfer/freesurfer\"\n",
    "DEMO_FILE = os.path.join(HBN_BIDS, \"final_preprocessed_subjects_with_demographics.tsv\")\n",
    "ANATCL_ENCODER_PATH = \"/home/arelbaha/.venvs/jupyter_py3/bin\"\n",
    "BRAINIAC_CKPT = \"/home/arelbaha/.venvs/jupyter_py3/bin/BrainIAC.ckpt\"\n",
    "DROPOUT_RATE = 0.3\n",
    "PARCELLATION = \"Schaefer2018_400Parcels_17Networks_order\"\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "#Loading Demographics\n",
    "print(\"Loading demographics\")\n",
    "\n",
    "demo_df = pd.read_csv(DEMO_FILE, sep='\\t')\n",
    "demo_df['participant_id'] = demo_df['participant_id'].astype(str)\n",
    "\n",
    "id_sex_dict = {}\n",
    "id_age_dict = {}\n",
    "\n",
    "for _, row in demo_df.iterrows():\n",
    "    subject_id = row['participant_id']\n",
    "    sex = row['sex'].strip()\n",
    "    if sex == 'Female':\n",
    "        id_sex_dict[subject_id] = 1\n",
    "    elif sex == 'Male':\n",
    "        id_sex_dict[subject_id] = 0\n",
    "    id_age_dict[subject_id] = row['age']\n",
    "\n",
    "print(f\"Loaded demographics for {len(id_sex_dict)} subjects\")\n",
    "print(f\"Sex distribution: {sum(id_sex_dict.values())} Female, {len(id_sex_dict) - sum(id_sex_dict.values())} Male\")\n",
    "print(f\"Age range: {min(id_age_dict.values()):.1f} - {max(id_age_dict.values()):.1f} years\\n\")\n",
    "\n",
    "#Finding CAT12 Files\n",
    "print(\"Finding CAT12 (s6mwp1) files\")\n",
    "\n",
    "cat12_data = {}\n",
    "\n",
    "print(f\"Searching in {HBN_BIDS}\")\n",
    "for subject_id in id_sex_dict.keys():\n",
    "    pattern = os.path.join(HBN_BIDS, f\"sub-{subject_id}\", \"ses-*\", \"anat\", \"mri\", \"s6mwp1sub*.nii\")\n",
    "    files = glob.glob(pattern)\n",
    "    if files:\n",
    "        cat12_data[subject_id] = files[0]\n",
    "\n",
    "print(f\"Searching in {HBN_BIDS_LOWER}\")\n",
    "for subject_id in id_sex_dict.keys():\n",
    "    if subject_id not in cat12_data:\n",
    "        pattern = os.path.join(HBN_BIDS_LOWER, f\"sub-{subject_id}\", \"ses-*\", \"anat\", \"mri\", \"s6mwp1sub*.nii\")\n",
    "        files = glob.glob(pattern)\n",
    "        if files:\n",
    "            cat12_data[subject_id] = files[0]\n",
    "\n",
    "print(f\"Found {len(cat12_data)} CAT12 files\\n\")\n",
    "\n",
    "#BrainIAC Files\n",
    "print(\"Finding BrainIAC preprocessed files\")\n",
    "\n",
    "brainiac_data = {}\n",
    "batch_dirs = glob.glob(os.path.join(BRAINIAC_OUT, \"batch_*\"))\n",
    "print(f\"Searching in {len(batch_dirs)} batch directories...\")\n",
    "\n",
    "for batch_dir in sorted(batch_dirs):\n",
    "    files = glob.glob(os.path.join(batch_dir, \"sub-*_0000.nii.gz\"))\n",
    "    for f in files:\n",
    "        basename = os.path.basename(f)\n",
    "        subject_id = basename.split('_')[0].replace('sub-', '')\n",
    "        \n",
    "        if subject_id in id_sex_dict and subject_id not in brainiac_data:\n",
    "            brainiac_data[subject_id] = f\n",
    "\n",
    "print(f\"Found {len(brainiac_data)} BrainIAC files\\n\")\n",
    "\n",
    "#Extracting FreeSurfer Features\n",
    "print(\"Extracting FreeSurfer features\")\n",
    "\n",
    "fs_data = {}\n",
    "fs_region_info = {}  # Store region names and hemisphere info\n",
    "\n",
    "for subject_id in id_sex_dict.keys():\n",
    "    subject_dir = os.path.join(FREESURFER_DIR, f\"sub-{subject_id}\")\n",
    "    stats_file = os.path.join(subject_dir, f\"sub-{subject_id}_regionsurfacestats.tsv\")\n",
    "    \n",
    "    if os.path.exists(stats_file):\n",
    "        try:\n",
    "            df = pd.read_csv(stats_file, sep='\\t')\n",
    "            filtered_df = df[df[\"atlas\"] == PARCELLATION]\n",
    "            \n",
    "            if not filtered_df.empty:\n",
    "                filtered_df = filtered_df.sort_values(\"StructName\")\n",
    "                \n",
    "                if \"SurfArea\" in filtered_df.columns and \"ThickAvg\" in filtered_df.columns:\n",
    "                    surf_area = filtered_df[\"SurfArea\"].values[:400]\n",
    "                    thick_avg = filtered_df[\"ThickAvg\"].values[:400]\n",
    "                    combined = np.concatenate([surf_area, thick_avg])\n",
    "                    \n",
    "                    if not np.any(np.isnan(combined)):\n",
    "                        fs_data[subject_id] = combined\n",
    "                        \n",
    "                        # Store region info from first valid subject\n",
    "                        if not fs_region_info:\n",
    "                            fs_region_info = {\n",
    "                                'region_names': filtered_df['StructName'].values[:400].tolist(),\n",
    "                                'hemisphere': filtered_df['hemisphere'].values[:400].tolist(),\n",
    "                                'parcellation': PARCELLATION\n",
    "                            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {stats_file}: {e}\")\n",
    "\n",
    "print(f\"Found {len(fs_data)} FreeSurfer subjects with 800 features (400 SurfArea + 400 ThickAvg)\\n\")\n",
    "\n",
    "#Overall Common Subjects\n",
    "print(\"Overall common subjects across all modalities\")\n",
    "\n",
    "common_subjects = sorted(list(\n",
    "    set(cat12_data.keys()) & \n",
    "    set(brainiac_data.keys()) & \n",
    "    set(fs_data.keys())\n",
    "))\n",
    "\n",
    "print(f\"Subjects with CAT12: {len(cat12_data)}\")\n",
    "print(f\"Subjects with BrainIAC: {len(brainiac_data)}\")\n",
    "print(f\"Subjects with FreeSurfer: {len(fs_data)}\")\n",
    "print(f\"Common subjects (all modalities): {len(common_subjects)}\\n\")\n",
    "\n",
    "if len(common_subjects) == 0:\n",
    "    print(\"ERROR: No common subjects found across all modalities\")\n",
    "    exit(1)\n",
    "\n",
    "cat12_paths = [cat12_data[s] for s in common_subjects]\n",
    "brainiac_paths = [brainiac_data[s] for s in common_subjects]\n",
    "fs_features = np.array([fs_data[s] for s in common_subjects])\n",
    "sex_labels = np.array([id_sex_dict[s] for s in common_subjects])\n",
    "age_labels = np.array([id_age_dict[s] for s in common_subjects])\n",
    "\n",
    "print(f\"Final matched dataset: {len(common_subjects)} subjects\")\n",
    "print(f\"FreeSurfer features shape: {fs_features.shape}\")\n",
    "print(f\"Sex labels: {np.sum(sex_labels)} Female, {len(sex_labels) - np.sum(sex_labels)} Male\")\n",
    "print(f\"Age: mean={age_labels.mean():.1f}, std={age_labels.std():.2f}, range={age_labels.min():.1f}-{age_labels.max():.1f}\")\n",
    "print(f\"Expected baseline MAE: {age_labels.std() * np.sqrt(2/np.pi):.2f} years\\n\")\n",
    "\n",
    "pd.DataFrame({\n",
    "    'subject_id': common_subjects,\n",
    "    'cat12_path': cat12_paths,\n",
    "    'brainiac_path': brainiac_paths,\n",
    "    'sex': sex_labels,\n",
    "    'age': age_labels\n",
    "}).to_csv('hbn_matched_subjects.csv', index=False)\n",
    "print(\"Saved: hbn_matched_subjects.csv\\n\")\n",
    "\n",
    "#Dataset Classes\n",
    "\n",
    "class CAT12VBMDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform):\n",
    "        self.data, self.labels, self.transform = data, labels, transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.transform(nib.load(self.data[idx]).get_fdata()).unsqueeze(0)\n",
    "        return img, torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "class BrainIACDataset(Dataset):\n",
    "    def __init__(self, paths, labels, transform):\n",
    "        self.paths, self.labels, self.transform = paths, labels, transform\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.paths[idx]).get_fdata().astype(np.float32)\n",
    "        if img.shape != (96, 96, 96):\n",
    "            img = F.interpolate(torch.from_numpy(img[None, None]), size=(96, 96, 96), \n",
    "                              mode='trilinear', align_corners=False).squeeze().numpy()\n",
    "        img_tensor = torch.from_numpy(img)\n",
    "        img_normalized = self.transform(img_tensor)\n",
    "        return img_normalized[None], torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "class CNNDataset(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.paths[idx]).get_fdata().astype(np.float32)\n",
    "        if img.shape != (92, 110, 92):\n",
    "            img = ndi.zoom(img, [92/img.shape[0], 110/img.shape[1], 92/img.shape[2]], order=1)\n",
    "        img = (img - img.mean()) / (img.std() + 1e-6)\n",
    "        return torch.from_numpy(img[None]).float()\n",
    "\n",
    "class CNN3D(nn.Module):\n",
    "    def __init__(self, dropout_rate=DROPOUT_RATE):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 64, 3, padding=1)\n",
    "        self.bn1, self.pool1, self.drop1 = nn.BatchNorm3d(64), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv2 = nn.Conv3d(64, 64, 3, padding=1)\n",
    "        self.bn2, self.pool2, self.drop2 = nn.BatchNorm3d(64), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv3 = nn.Conv3d(64, 128, 3, padding=1)\n",
    "        self.bn3, self.pool3, self.drop3 = nn.BatchNorm3d(128), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv4 = nn.Conv3d(128, 256, 3, padding=1)\n",
    "        self.bn4, self.pool4, self.drop4 = nn.BatchNorm3d(256), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc1, self.drop5 = nn.Linear(256, 512), nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop1(self.bn1(self.pool1(F.relu(self.conv1(x)))))\n",
    "        x = self.drop2(self.bn2(self.pool2(F.relu(self.conv2(x)))))\n",
    "        x = self.drop3(self.bn3(self.pool3(F.relu(self.conv3(x)))))\n",
    "        x = self.drop4(self.bn4(self.pool4(F.relu(self.conv4(x)))))\n",
    "        return self.drop5(F.relu(self.fc1(self.global_pool(x).view(x.size(0), -1))))\n",
    "\n",
    "def load_brainiac_vit(ckpt_path, device):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    vit = ViT(in_channels=1, img_size=(96, 96, 96), patch_size=(16, 16, 16),\n",
    "              hidden_size=768, mlp_dim=3072, num_layers=12, num_heads=12, classification=False)\n",
    "    pe_key = \"backbone.patch_embedding.position_embeddings\"\n",
    "    if pe_key in ckpt.get(\"state_dict\", ckpt):\n",
    "        vit.patch_embedding.position_embeddings = nn.Parameter(ckpt[\"state_dict\"][pe_key].clone())\n",
    "    backbone_state = {k.replace(\"backbone.\", \"\"): v for k, v in ckpt.get(\"state_dict\", ckpt).items() \n",
    "                     if k.startswith(\"backbone.\")}\n",
    "    vit.load_state_dict(backbone_state, strict=False)\n",
    "    return vit.to(device).eval()\n",
    "\n",
    "#AnatCL Features\n",
    "\n",
    "anatcl_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: torch.from_numpy(x.copy()).float()),\n",
    "    transforms.Normalize(mean=0.0, std=1.0)\n",
    "])\n",
    "\n",
    "num_folds = 5\n",
    "all_fold_features = []\n",
    "\n",
    "for fold_idx in range(num_folds):\n",
    "    path = os.path.join(ANATCL_ENCODER_PATH, f\"fold{fold_idx}.pth\")\n",
    "    \n",
    "    print(f\"Loading fold {fold_idx}...\")\n",
    "    encoder = AnatCL(descriptor=\"global\", fold=fold_idx, pretrained=False).to(device).eval()\n",
    "    encoder.backbone.load_state_dict(torch.load(path, map_location=device, weights_only=False)['model'])\n",
    "    \n",
    "    for p in encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    dl = DataLoader(CAT12VBMDataset(cat12_paths, age_labels, anatcl_transform), \n",
    "                    batch_size=32, num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fold_features = torch.cat([encoder(vol.to(device)).cpu() for vol, _ in dl]).numpy()\n",
    "    \n",
    "    all_fold_features.append(fold_features)\n",
    "    print(f\"Fold {fold_idx} features: {fold_features.shape}\")\n",
    "    \n",
    "    del encoder\n",
    "\n",
    "anatcl_features = np.mean(all_fold_features, axis=0)\n",
    "\n",
    "print(f\"\\nAnatCL features (averaged across {num_folds} fold): {anatcl_features.shape}\\n\")\n",
    "\n",
    "#BrainIAC Features\n",
    "print(\"BrainIAC features\")\n",
    "\n",
    "brainiac_vit = load_brainiac_vit(BRAINIAC_CKPT, device)\n",
    "brainiac_transform = lambda x: (x - x.mean()) / (x.std() + 1e-6)\n",
    "dl = DataLoader(BrainIACDataset(brainiac_paths, age_labels, brainiac_transform), \n",
    "                batch_size=16, num_workers=0)\n",
    "\n",
    "brainiac_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, _ in dl:\n",
    "        out = brainiac_vit(x.to(device))\n",
    "        \n",
    "        #Handle tuple or tensor\n",
    "        if isinstance(out, tuple):\n",
    "            cls_token = out[0][:, 0]\n",
    "        else:\n",
    "            cls_token = out[:, 0]\n",
    "        \n",
    "        brainiac_features.append(cls_token.cpu().numpy())\n",
    "\n",
    "brainiac_features = np.vstack(brainiac_features)\n",
    "\n",
    "print(f\"BrainIAC features: {brainiac_features.shape}\")\n",
    "del brainiac_vit\n",
    "\n",
    "#CNN Features\n",
    "print(\"CNN features\")\n",
    "\n",
    "cnn_model = CNN3D().to(device).eval()\n",
    "dl = DataLoader(CNNDataset(brainiac_paths), batch_size=8, num_workers=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    cnn_features = np.vstack([cnn_model(x.to(device)).cpu().numpy() for x in dl])\n",
    "\n",
    "print(f\"CNN features: {cnn_features.shape}\\n\")\n",
    "del cnn_model\n",
    "\n",
    "#Organize Features\n",
    "features_dict = {\n",
    "    'AnatCL': anatcl_features,\n",
    "    'BrainIAC': brainiac_features,\n",
    "    'CNN': cnn_features,\n",
    "    'FreeSurfer': fs_features\n",
    "}\n",
    "print(\"Feature extraction complete\")\n",
    "for name, feats in features_dict.items():\n",
    "    print(f\"{name}: {feats.shape}\")\n",
    "print()\n",
    "\n",
    "#Pre-filtering variance check\n",
    "print(\"Pre-filtering variance check:\")\n",
    "for model in ['AnatCL', 'BrainIAC', 'CNN', 'FreeSurfer']:\n",
    "    feats = StandardScaler().fit_transform(features_dict[model])\n",
    "    n_dead = np.sum(np.std(feats, axis=0) < 1e-10)\n",
    "    print(f\"{model}: {feats.shape[1]} total, {n_dead} dead ({100*n_dead/feats.shape[1]:.1f}%)\")\n",
    "print()\n",
    "\n",
    "#Correlation Analysis\n",
    "print(\"Computing cross-model feature correlations\")\n",
    "\n",
    "def compute_correlation_within_models(features_dict):\n",
    "    all_features = []\n",
    "    boundaries = [0]\n",
    "    reordered_indices = {}\n",
    "\n",
    "    for model in ['AnatCL', 'BrainIAC', 'CNN', 'FreeSurfer']:\n",
    "        if model not in features_dict:\n",
    "            continue\n",
    "\n",
    "        feats = features_dict[model]\n",
    "        feats = StandardScaler().fit_transform(feats)\n",
    "\n",
    "        valid = ~np.isnan(feats).any(axis=0) & (np.std(feats, axis=0) > 1e-10)\n",
    "        feats = feats[:, valid]\n",
    "\n",
    "        if feats.shape[1] == 0:\n",
    "            continue\n",
    "\n",
    "        print(f\"  {model}: {feats.shape[1]} features\")\n",
    "\n",
    "        corr = np.corrcoef(feats.T)\n",
    "        corr = np.nan_to_num((corr + corr.T) / 2, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        np.fill_diagonal(corr, 1.0)\n",
    "\n",
    "        dist = np.abs(1 - np.abs(corr))\n",
    "        np.fill_diagonal(dist, 0)\n",
    "\n",
    "        condensed = np.nan_to_num(\n",
    "            squareform((dist + dist.T) / 2, checks=False),\n",
    "            nan=0.0, posinf=1.0, neginf=0.0\n",
    "        )\n",
    "\n",
    "        reorder_idx = dendrogram(linkage(condensed, method='ward'), no_plot=True)['leaves']\n",
    "        all_features.append(feats[:, reorder_idx])\n",
    "        boundaries.append(boundaries[-1] + feats.shape[1])\n",
    "        reordered_indices[model] = reorder_idx\n",
    "\n",
    "    combined = np.hstack(all_features)\n",
    "\n",
    "    corr = np.corrcoef(combined.T)\n",
    "    corr = np.nan_to_num((corr + corr.T) / 2, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "\n",
    "    print(f\"\\nTotal features: {combined.shape[1]}\")\n",
    "    \n",
    "    return corr, boundaries, ['AnatCL', 'BrainIAC', 'CNN', 'FreeSurfer'], reordered_indices\n",
    "    \n",
    "corr_matrix, boundaries, model_names, reorder_indices = compute_correlation_within_models(features_dict)\n",
    "pd.DataFrame(corr_matrix).to_csv('hbn_feature_correlation_matrix.csv', index=False)\n",
    "print(f\"Correlation matrix: {corr_matrix.shape}\")\n",
    "print(f\"Model boundaries: {boundaries}\\n\")\n",
    "\n",
    "print(\"ANALYZING FREESURFER CORRELATION CLUSTERS\")\n",
    "\n",
    "# Extract FreeSurfer block from correlation matrix\n",
    "fs_start_idx = boundaries[3]  # FreeSurfer is 4th model (index 3)\n",
    "fs_end_idx = boundaries[4]\n",
    "fs_corr_block = corr_matrix[fs_start_idx:fs_end_idx, fs_start_idx:fs_end_idx]\n",
    "\n",
    "print(f\"\\nFreeSurfer correlation block: {fs_corr_block.shape}\")\n",
    "\n",
    "# The features are already reordered by hierarchical clustering\n",
    "# Now identify the two main clusters\n",
    "reordered_fs_indices = reorder_indices['FreeSurfer']\n",
    "\n",
    "# Map back to original feature indices and names\n",
    "region_names = fs_region_info['region_names']\n",
    "hemisphere = fs_region_info['hemisphere']\n",
    "\n",
    "# Create feature labels (first 400 = Surface Area, last 400 = Thickness)\n",
    "fs_feature_labels = []\n",
    "for i in range(400):\n",
    "    fs_feature_labels.append({\n",
    "        'original_index': i,\n",
    "        'region_name': region_names[i],\n",
    "        'hemisphere': hemisphere[i],\n",
    "        'feature_type': 'Surface_Area'\n",
    "    })\n",
    "for i in range(400):\n",
    "    fs_feature_labels.append({\n",
    "        'original_index': i + 400,\n",
    "        'region_name': region_names[i],\n",
    "        'hemisphere': hemisphere[i],\n",
    "        'feature_type': 'Thickness'\n",
    "    })\n",
    "\n",
    "# Apply the reordering to get clustered feature labels\n",
    "clustered_features = [fs_feature_labels[i] for i in reordered_fs_indices]\n",
    "\n",
    "# Perform hierarchical clustering on FreeSurfer block to identify 2 clusters\n",
    "dist_fs = np.abs(1 - np.abs(fs_corr_block))\n",
    "np.fill_diagonal(dist_fs, 0)\n",
    "condensed_fs = squareform(dist_fs, checks=False)\n",
    "linkage_fs = linkage(condensed_fs, method='ward')\n",
    "\n",
    "# Cut tree to get 2 clusters\n",
    "cluster_assignments = fcluster(linkage_fs, 2, criterion='maxclust')\n",
    "\n",
    "print(f\"Identified {len(np.unique(cluster_assignments))} clusters\")\n",
    "print(f\"Cluster 1: {np.sum(cluster_assignments == 1)} features\")\n",
    "print(f\"Cluster 2: {np.sum(cluster_assignments == 2)} features\")\n",
    "\n",
    "# Create DataFrame with cluster assignments\n",
    "cluster_df = pd.DataFrame(clustered_features)\n",
    "cluster_df['cluster'] = cluster_assignments\n",
    "cluster_df['reordered_index'] = range(len(clustered_features))\n",
    "\n",
    "# Analyze cluster composition\n",
    "print(\"\\nCluster Composition:\")\n",
    "for cluster_id in [1, 2]:\n",
    "    cluster_data = cluster_df[cluster_df['cluster'] == cluster_id]\n",
    "    n_sa = np.sum(cluster_data['feature_type'] == 'Surface_Area')\n",
    "    n_thick = np.sum(cluster_data['feature_type'] == 'Thickness')\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    print(f\"Surface Area: {n_sa} features ({100*n_sa/len(cluster_data):.1f}%)\")\n",
    "    print(f\"Thickness: {n_thick} features ({100*n_thick/len(cluster_data):.1f}%)\")\n",
    "\n",
    "# Save detailed cluster information\n",
    "cluster_df_sorted = cluster_df.sort_values(['cluster', 'feature_type', 'region_name'])\n",
    "cluster_df_sorted.to_csv('freesurfer_correlation_clusters.csv', index=False)\n",
    "print(\"\\nSaved: freesurfer_correlation_clusters.csv\")\n",
    "\n",
    "# Create separate CSVs for each cluster\n",
    "for cluster_id in [1, 2]:\n",
    "    cluster_data = cluster_df[cluster_df['cluster'] == cluster_id].copy()\n",
    "    cluster_data = cluster_data.sort_values(['feature_type', 'region_name'])\n",
    "    cluster_data.to_csv(f'freesurfer_cluster{cluster_id}_features.csv', index=False)\n",
    "    print(f\"Saved: freesurfer_cluster{cluster_id}_features.csv\")\n",
    "\n",
    "# Create summary of cluster characteristics\n",
    "cluster_summary = []\n",
    "for cluster_id in [1, 2]:\n",
    "    cluster_data = cluster_df[cluster_df['cluster'] == cluster_id]\n",
    "    for feat_type in ['Surface_Area', 'Thickness']:\n",
    "        type_data = cluster_data[cluster_data['feature_type'] == feat_type]\n",
    "        cluster_summary.append({\n",
    "            'Cluster': cluster_id,\n",
    "            'Feature_Type': feat_type,\n",
    "            'Count': len(type_data),\n",
    "            'Percentage': 100 * len(type_data) / len(cluster_data),\n",
    "            'Brain_Regions': ', '.join(type_data['region_name'].head(10).tolist()) + '...'\n",
    "        })\n",
    "\n",
    "pd.DataFrame(cluster_summary).to_csv('freesurfer_cluster_summary.csv', index=False)\n",
    "print(\"Saved: freesurfer_cluster_summary.csv\\n\")\n",
    "\n",
    "# Plot correlation matrix WITH PDF OUTPUT\n",
    "print(f\"Plotting FULL cross-model correlation matrix ({corr_matrix.shape[0]}x{corr_matrix.shape[1]} features)...\")\n",
    "\n",
    "try:\n",
    "    fig = plt.figure(figsize=(30, 28))\n",
    "    print(\"Figure created, generating heatmap\")\n",
    "    \n",
    "    ax = sns.heatmap(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1, center=0, \n",
    "                     square=True, linewidths=0, cbar_kws={\"shrink\": 0.3},\n",
    "                     xticklabels=False, yticklabels=False, rasterized=True)\n",
    "    print(\"Heatmap generated, adding boundaries\")\n",
    "    \n",
    "    for b in boundaries[1:-1]:\n",
    "        plt.axhline(y=b, color='black', linewidth=4)\n",
    "        plt.axvline(x=b, color='black', linewidth=4)\n",
    "    \n",
    "    for i, (pos, name) in enumerate(zip([(boundaries[i] + boundaries[i+1]) / 2 \n",
    "                                          for i in range(len(boundaries)-1)], model_names)):\n",
    "        plt.text(pos, -20, name, ha='center', fontsize=20, weight='bold')\n",
    "        plt.text(-20, pos, name, ha='center', va='center', fontsize=20, weight='bold', rotation=90)\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"Saving PDF...\")\n",
    "    plt.savefig('hbn_cross_model_correlation.pdf', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved: hbn_cross_model_correlation.pdf\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {e}\\n\")\n",
    "\n",
    "#Model Training Functions\n",
    "\n",
    "def run_classification(X_dict, y, test_size=0.1, task_name=\"Classification\"):\n",
    "    print(f\"Running {task_name}\")\n",
    "    results = {}\n",
    "    cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)), \n",
    "                                        stratify=y, random_state=SEED)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    alphas = [0.01, 0.05, 0.1, 0.15, 0.25, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "    print(\"DummyClassifier\")\n",
    "    dummy_val_auc = []\n",
    "    dummy_val_acc = []\n",
    "    \n",
    "    for train_rel, val_rel in skf.split(cv_idx, y[cv_idx]):\n",
    "        train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "        dummy = DummyClassifier(strategy='stratified', random_state=SEED)\n",
    "        dummy.fit(np.zeros((len(train_idx), 1)), y[train_idx])\n",
    "        \n",
    "        pred = dummy.predict(np.zeros((len(val_idx), 1)))\n",
    "        pred_proba = dummy.predict_proba(np.zeros((len(val_idx), 1)))[:, 1]\n",
    "        \n",
    "        dummy_val_acc.append(balanced_accuracy_score(y[val_idx], pred))\n",
    "        dummy_val_auc.append(roc_auc_score(y[val_idx], pred_proba))\n",
    "    \n",
    "    dummy = DummyClassifier(strategy='stratified', random_state=SEED)\n",
    "    dummy.fit(np.zeros((len(cv_idx), 1)), y[cv_idx])\n",
    "    dummy_test_pred = dummy.predict(np.zeros((len(test_idx), 1)))\n",
    "    dummy_test_proba = dummy.predict_proba(np.zeros((len(test_idx), 1)))[:, 1]\n",
    "    \n",
    "    results['DummyClassifier'] = {\n",
    "        'best_alpha': 1.0,\n",
    "        'test_balanced_accuracy': balanced_accuracy_score(y[test_idx], dummy_test_pred),\n",
    "        'test_auc': roc_auc_score(y[test_idx], dummy_test_proba),\n",
    "        'cv_auc_mean': np.mean(dummy_val_auc),\n",
    "        'cv_auc_std': np.std(dummy_val_auc),\n",
    "        'cv_acc_mean': np.mean(dummy_val_acc),\n",
    "        'cv_acc_std': np.std(dummy_val_acc),\n",
    "        'fold_results': {'val_auc': dummy_val_auc, 'val_acc': dummy_val_acc},\n",
    "        'test_idx': test_idx\n",
    "    }\n",
    "    print(f\"DummyClassifier: AUC={results['DummyClassifier']['test_auc']:.4f}, Bal_Acc={results['DummyClassifier']['test_balanced_accuracy']:.4f}\")\n",
    "\n",
    "    for model, X in X_dict.items():\n",
    "        val_auc = {a: [] for a in alphas}\n",
    "        val_acc = {a: [] for a in alphas}\n",
    "        train_auc = {a: [] for a in alphas}\n",
    "        train_acc = {a: [] for a in alphas}\n",
    "\n",
    "        for train_rel, val_rel in skf.split(cv_idx, y[cv_idx]):\n",
    "            train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "\n",
    "            for alpha in alphas:\n",
    "                n = int(alpha * len(train_idx))\n",
    "                try:\n",
    "                    tr_idx, _ = train_test_split(train_idx, train_size=n, stratify=y[train_idx], random_state=SEED)\n",
    "                except:\n",
    "                    tr_idx = np.random.choice(train_idx, n, replace=False)\n",
    "\n",
    "                if len(np.unique(y[tr_idx])) < 2:\n",
    "                    continue\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                rf = RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_split=5,\n",
    "                                           random_state=SEED, n_jobs=1, \n",
    "                                           max_features='sqrt', class_weight='balanced')\n",
    "                rf.fit(scaler.fit_transform(X[tr_idx]), y[tr_idx])\n",
    "\n",
    "                pred_proba = rf.predict_proba(scaler.transform(X[val_idx]))[:, 1]\n",
    "                val_acc[alpha].append(balanced_accuracy_score(y[val_idx], rf.predict(scaler.transform(X[val_idx]))))\n",
    "                val_auc[alpha].append(roc_auc_score(y[val_idx], pred_proba))\n",
    "                \n",
    "                train_pred_proba = rf.predict_proba(scaler.transform(X[tr_idx]))[:, 1]\n",
    "                train_acc[alpha].append(balanced_accuracy_score(y[tr_idx], rf.predict(scaler.transform(X[tr_idx]))))\n",
    "                train_auc[alpha].append(roc_auc_score(y[tr_idx], train_pred_proba))\n",
    "\n",
    "        avg_auc = {a: np.mean(val_auc[a]) for a in alphas if val_auc[a]}\n",
    "        best_alpha = max(avg_auc, key=avg_auc.get)\n",
    "\n",
    "        train_val_gaps = {}\n",
    "        for alpha in alphas:\n",
    "            if train_auc[alpha] and val_auc[alpha]:\n",
    "                train_val_gaps[alpha] = np.mean(train_auc[alpha]) - np.mean(val_auc[alpha])\n",
    "\n",
    "        try:\n",
    "            final_tr, _ = train_test_split(cv_idx, train_size=int(best_alpha * len(cv_idx)), \n",
    "                                          stratify=y[cv_idx], random_state=SEED)\n",
    "        except:\n",
    "            final_tr = cv_idx\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        rf = RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_split=5,\n",
    "                                    random_state=SEED, n_jobs=1, \n",
    "                                    max_features='sqrt', class_weight='balanced')\n",
    "        rf.fit(scaler.fit_transform(X[final_tr]), y[final_tr])\n",
    "        test_pred_proba = rf.predict_proba(scaler.transform(X[test_idx]))[:, 1]\n",
    "\n",
    "        train_val_gap = train_val_gaps[best_alpha]\n",
    "        is_overfitting = train_val_gap > 0.25\n",
    "\n",
    "        results[model] = {\n",
    "            'best_alpha': best_alpha,\n",
    "            'test_balanced_accuracy': balanced_accuracy_score(y[test_idx], rf.predict(scaler.transform(X[test_idx]))),\n",
    "            'test_auc': roc_auc_score(y[test_idx], test_pred_proba),\n",
    "            'cv_auc_mean': np.mean(val_auc[best_alpha]),\n",
    "            'cv_auc_std': np.std(val_auc[best_alpha]),\n",
    "            'cv_acc_mean': np.mean(val_acc[best_alpha]),\n",
    "            'cv_acc_std': np.std(val_acc[best_alpha]),\n",
    "            'train_auc_mean': np.mean(train_auc[best_alpha]),\n",
    "            'train_val_gap': train_val_gap,\n",
    "            'train_val_gaps_all_alphas': train_val_gaps,\n",
    "            'is_overfitting': is_overfitting,\n",
    "            'fold_results': {\n",
    "                'val_auc': val_auc[best_alpha],\n",
    "                'val_acc': val_acc[best_alpha],\n",
    "                'train_auc': train_auc[best_alpha]\n",
    "            },\n",
    "            'cv_results': {\n",
    "                'alphas': alphas, \n",
    "                'acc': {a: np.mean(val_acc[a]) for a in alphas if val_acc[a]},\n",
    "                'auc': avg_auc,\n",
    "                'acc_std': {a: np.std(val_acc[a]) for a in alphas if val_acc[a]},\n",
    "                'auc_std': {a: np.std(val_auc[a]) for a in alphas if val_auc[a]},\n",
    "                'train_auc': {a: np.mean(train_auc[a]) for a in alphas if train_auc[a]},\n",
    "                'train_acc': {a: np.mean(train_acc[a]) for a in alphas if train_acc[a]}\n",
    "            },\n",
    "            'trained_model': rf,\n",
    "            'scaler': scaler,\n",
    "            'test_idx': test_idx\n",
    "        }\n",
    "        print(f\"{model}: AUC={results[model]['test_auc']:.4f}, Bal_Acc={results[model]['test_balanced_accuracy']:.4f}\" + \n",
    "              (f\" [OVERFITTING: train-val gap={train_val_gap:.3f}]\" if is_overfitting else \"\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_regression(X_dict, y, test_size=0.1, task_name=\"Regression\"):\n",
    "    print(f\"Running {task_name}\")\n",
    "    results = {}\n",
    "    cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)), random_state=SEED)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    alphas = [0.01, 0.05, 0.1, 0.15, 0.25, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "    print(\"DummyRegressor\")\n",
    "    dummy_val_r2 = []\n",
    "    dummy_val_mae = []\n",
    "    \n",
    "    for train_rel, val_rel in kf.split(cv_idx):\n",
    "        train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "        dummy = DummyRegressor(strategy='mean')\n",
    "        dummy.fit(np.zeros((len(train_idx), 1)), y[train_idx])\n",
    "        \n",
    "        pred = dummy.predict(np.zeros((len(val_idx), 1)))\n",
    "        dummy_val_r2.append(r2_score(y[val_idx], pred))\n",
    "        dummy_val_mae.append(mean_absolute_error(y[val_idx], pred))\n",
    "    \n",
    "    dummy = DummyRegressor(strategy='mean')\n",
    "    dummy.fit(np.zeros((len(cv_idx), 1)), y[cv_idx])\n",
    "    dummy_test_pred = dummy.predict(np.zeros((len(test_idx), 1)))\n",
    "    \n",
    "    results['DummyRegressor'] = {\n",
    "        'best_alpha': 1.0,\n",
    "        'test_r2': r2_score(y[test_idx], dummy_test_pred),\n",
    "        'test_mae': mean_absolute_error(y[test_idx], dummy_test_pred),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y[test_idx], dummy_test_pred)),\n",
    "        'cv_r2_mean': np.mean(dummy_val_r2),\n",
    "        'cv_r2_std': np.std(dummy_val_r2),\n",
    "        'cv_mae_mean': np.mean(dummy_val_mae),\n",
    "        'cv_mae_std': np.std(dummy_val_mae),\n",
    "        'fold_results': {'val_r2': dummy_val_r2, 'val_mae': dummy_val_mae},\n",
    "        'test_idx': test_idx\n",
    "    }\n",
    "    print(f\"DummyRegressor: R2={results['DummyRegressor']['test_r2']:.4f}, MAE={results['DummyRegressor']['test_mae']:.2f}\")\n",
    "\n",
    "    for model, X in X_dict.items():\n",
    "        val_r2 = {a: [] for a in alphas}\n",
    "        val_mae = {a: [] for a in alphas}\n",
    "        train_r2 = {a: [] for a in alphas}\n",
    "        train_mae = {a: [] for a in alphas}\n",
    "\n",
    "        for train_rel, val_rel in kf.split(cv_idx):\n",
    "            train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "\n",
    "            for alpha in alphas:\n",
    "                n = int(alpha * len(train_idx))\n",
    "                tr_idx, _ = train_test_split(train_idx, train_size=n, random_state=SEED) \\\n",
    "                           if n < len(train_idx) else (train_idx, None)\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                rf = RandomForestRegressor(n_estimators=200, max_depth=6, min_samples_split=5, \n",
    "                                          random_state=SEED, n_jobs=1)\n",
    "                rf.fit(scaler.fit_transform(X[tr_idx]), y[tr_idx])\n",
    "\n",
    "                pred = rf.predict(scaler.transform(X[val_idx]))\n",
    "                val_r2[alpha].append(r2_score(y[val_idx], pred))\n",
    "                val_mae[alpha].append(mean_absolute_error(y[val_idx], pred))\n",
    "                \n",
    "                train_pred = rf.predict(scaler.transform(X[tr_idx]))\n",
    "                train_r2[alpha].append(r2_score(y[tr_idx], train_pred))\n",
    "                train_mae[alpha].append(mean_absolute_error(y[tr_idx], train_pred))\n",
    "\n",
    "        avg_r2 = {a: np.mean(val_r2[a]) for a in alphas if val_r2[a]}\n",
    "        best_alpha = max(avg_r2, key=avg_r2.get)\n",
    "\n",
    "        train_val_gaps = {}\n",
    "        for alpha in alphas:\n",
    "            if train_r2[alpha] and val_r2[alpha]:\n",
    "                train_val_gaps[alpha] = np.mean(train_r2[alpha]) - np.mean(val_r2[alpha])\n",
    "\n",
    "        n = int(best_alpha * len(cv_idx))\n",
    "        final_tr, _ = train_test_split(cv_idx, train_size=n, random_state=SEED) \\\n",
    "                     if n < len(cv_idx) else (cv_idx, None)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        rf = RandomForestRegressor(n_estimators=200, max_depth=6, min_samples_split=5, random_state=SEED, n_jobs=1)\n",
    "        rf.fit(scaler.fit_transform(X[final_tr]), y[final_tr])\n",
    "        test_pred = rf.predict(scaler.transform(X[test_idx]))\n",
    "\n",
    "        train_val_gap = train_val_gaps[best_alpha]\n",
    "        is_overfitting = train_val_gap > 0.35\n",
    "\n",
    "        results[model] = {\n",
    "            'best_alpha': best_alpha,\n",
    "            'test_r2': r2_score(y[test_idx], test_pred),\n",
    "            'test_mae': mean_absolute_error(y[test_idx], test_pred),\n",
    "            'test_rmse': np.sqrt(mean_squared_error(y[test_idx], test_pred)),\n",
    "            'predictions': test_pred,\n",
    "            'actual': y[test_idx],\n",
    "            'cv_r2_mean': np.mean(val_r2[best_alpha]),\n",
    "            'cv_r2_std': np.std(val_r2[best_alpha]),\n",
    "            'cv_mae_mean': np.mean(val_mae[best_alpha]),\n",
    "            'cv_mae_std': np.std(val_mae[best_alpha]),\n",
    "            'train_r2_mean': np.mean(train_r2[best_alpha]),\n",
    "            'train_val_gap': train_val_gap,\n",
    "            'train_val_gaps_all_alphas': train_val_gaps,\n",
    "            'is_overfitting': is_overfitting,\n",
    "            'fold_results': {\n",
    "                'val_r2': val_r2[best_alpha],\n",
    "                'val_mae': val_mae[best_alpha],\n",
    "                'train_r2': train_r2[best_alpha]\n",
    "            },\n",
    "            'cv_results': {\n",
    "                'alphas': alphas, \n",
    "                'r2': avg_r2, \n",
    "                'mae': {a: np.mean(val_mae[a]) for a in alphas if val_mae[a]},\n",
    "                'r2_std': {a: np.std(val_r2[a]) for a in alphas if val_r2[a]},\n",
    "                'mae_std': {a: np.std(val_mae[a]) for a in alphas if val_mae[a]},\n",
    "                'train_r2': {a: np.mean(train_r2[a]) for a in alphas if train_r2[a]},\n",
    "                'train_mae': {a: np.mean(train_mae[a]) for a in alphas if train_mae[a]}\n",
    "            },\n",
    "            'trained_model': rf,\n",
    "            'scaler': scaler,\n",
    "            'test_idx': test_idx\n",
    "        }\n",
    "        print(f\"{model}: R2={results[model]['test_r2']:.4f}, MAE={results[model]['test_mae']:.2f}\" +\n",
    "              (f\" [OVERFITTING: train-val gap={train_val_gap:.3f}]\" if is_overfitting else \"\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Sex Classification\")\n",
    "sex_results = run_classification(features_dict, sex_labels, test_size=0.1, task_name=\"Sex classification\")\n",
    "print()\n",
    "\n",
    "# NEW: SPLIT AGE PREDICTION BY SEX\n",
    "\n",
    "print(\"AGE PREDICTION - SPLIT BY SEX\")\n",
    "\n",
    "#Male subjects (sex_labels == 0)\n",
    "male_indices = np.where(sex_labels == 0)[0]\n",
    "male_features_dict = {k: v[male_indices] for k, v in features_dict.items()}\n",
    "male_age_labels = age_labels[male_indices]\n",
    "\n",
    "print(f\"\\nMale subjects: N={len(male_indices)}\")\n",
    "print(f\"Male age: mean={male_age_labels.mean():.1f}, std={male_age_labels.std():.2f}, range={male_age_labels.min():.1f}-{male_age_labels.max():.1f}\")\n",
    "\n",
    "male_age_results = run_regression(male_features_dict, male_age_labels, test_size=0.1, task_name=\"Male age prediction\")\n",
    "print()\n",
    "\n",
    "#Female subjects (sex_labels == 1)\n",
    "female_indices = np.where(sex_labels == 1)[0]\n",
    "female_features_dict = {k: v[female_indices] for k, v in features_dict.items()}\n",
    "female_age_labels = age_labels[female_indices]\n",
    "\n",
    "print(f\"\\nFemale subjects: N={len(female_indices)}\")\n",
    "print(f\"Female age: mean={female_age_labels.mean():.1f}, std={female_age_labels.std():.2f}, range={female_age_labels.min():.1f}-{female_age_labels.max():.1f}\")\n",
    "\n",
    "female_age_results = run_regression(female_features_dict, female_age_labels, test_size=0.1, task_name=\"Female age prediction\")\n",
    "print()\n",
    "\n",
    "print(\"Overfitting Analysis\")\n",
    "overfitting_report = []\n",
    "for model in features_dict.keys():\n",
    "    sex_overfit = sex_results[model]['is_overfitting']\n",
    "    male_age_overfit = male_age_results[model]['is_overfitting']\n",
    "    female_age_overfit = female_age_results[model]['is_overfitting']\n",
    "    \n",
    "    overfitting_report.append({\n",
    "        'Model': model,\n",
    "        'Sex_Overfitting': sex_overfit,\n",
    "        'Sex_Train_Val_Gap': sex_results[model]['train_val_gap'],\n",
    "        'Male_Age_Overfitting': male_age_overfit,\n",
    "        'Male_Age_Train_Val_Gap': male_age_results[model]['train_val_gap'],\n",
    "        'Female_Age_Overfitting': female_age_overfit,\n",
    "        'Female_Age_Train_Val_Gap': female_age_results[model]['train_val_gap']\n",
    "    })\n",
    "    \n",
    "    if sex_overfit or male_age_overfit or female_age_overfit:\n",
    "        print(f\"{model}:\")\n",
    "        if sex_overfit:\n",
    "            print(f\"Sex classification: Train-Val AUC gap = {sex_results[model]['train_val_gap']:.3f}\")\n",
    "        if male_age_overfit:\n",
    "            print(f\"Male age prediction: Train-Val R gap = {male_age_results[model]['train_val_gap']:.3f}\")\n",
    "        if female_age_overfit:\n",
    "            print(f\"Female age prediction: Train-Val R gap = {female_age_results[model]['train_val_gap']:.3f}\")\n",
    "\n",
    "if not any(r['Sex_Overfitting'] or r['Male_Age_Overfitting'] or r['Female_Age_Overfitting'] for r in overfitting_report):\n",
    "    print(\"No significant overfitting detected\")\n",
    "\n",
    "pd.DataFrame(overfitting_report).to_csv('hbn_overfitting_analysis.csv', index=False)\n",
    "print(\"\\nSaved: hbn_overfitting_analysis.csv\\n\")\n",
    "\n",
    "# Save train-val gaps for all alphas\n",
    "print(\"Saving train-val gaps for all alphas...\")\n",
    "gap_rows = []\n",
    "for model in features_dict.keys():\n",
    "    model_alphas = sex_results[model]['cv_results']['alphas']\n",
    "    for alpha in model_alphas:\n",
    "        sex_gap = sex_results[model]['train_val_gaps_all_alphas'].get(alpha, np.nan)\n",
    "        male_age_gap = male_age_results[model]['train_val_gaps_all_alphas'].get(alpha, np.nan)\n",
    "        female_age_gap = female_age_results[model]['train_val_gaps_all_alphas'].get(alpha, np.nan)\n",
    "        gap_rows.append({\n",
    "            'Model': model,\n",
    "            'Alpha': alpha,\n",
    "            'Sex_Train_Val_Gap': sex_gap,\n",
    "            'Male_Age_Train_Val_Gap': male_age_gap,\n",
    "            'Female_Age_Train_Val_Gap': female_age_gap\n",
    "        })\n",
    "\n",
    "pd.DataFrame(gap_rows).to_csv('hbn_train_val_gaps_all_alphas.csv', index=False)\n",
    "print(\"Saved: hbn_train_val_gaps_all_alphas.csv\\n\")\n",
    "\n",
    "print(\"Results\")\n",
    "\n",
    "all_models = ['DummyClassifier'] + list(features_dict.keys())\n",
    "\n",
    "pd.DataFrame([{'Model': m, 'Best_Alpha': sex_results[m]['best_alpha'], \n",
    "               'Test_Balanced_Accuracy': sex_results[m]['test_balanced_accuracy'], \n",
    "               'Test_AUC': sex_results[m]['test_auc'],\n",
    "               'CV_AUC_Mean': sex_results[m]['cv_auc_mean'],\n",
    "               'CV_AUC_Std': sex_results[m]['cv_auc_std']} \n",
    "              for m in all_models]).to_csv('hbn_sex_classification_summary.csv', index=False)\n",
    "\n",
    "all_models_reg = ['DummyRegressor'] + list(features_dict.keys())\n",
    "\n",
    "pd.DataFrame([{'Model': m, 'Best_Alpha': male_age_results[m]['best_alpha'], \n",
    "               'Test_R2': male_age_results[m]['test_r2'], \n",
    "               'Test_MAE': male_age_results[m]['test_mae'], \n",
    "               'Test_RMSE': male_age_results[m]['test_rmse'],\n",
    "               'CV_R2_Mean': male_age_results[m]['cv_r2_mean'],\n",
    "               'CV_R2_Std': male_age_results[m]['cv_r2_std'],\n",
    "               'CV_MAE_Mean': male_age_results[m]['cv_mae_mean'],\n",
    "               'CV_MAE_Std': male_age_results[m]['cv_mae_std']} \n",
    "              for m in all_models_reg]).to_csv('hbn_male_age_prediction_summary.csv', index=False)\n",
    "\n",
    "pd.DataFrame([{'Model': m, 'Best_Alpha': female_age_results[m]['best_alpha'], \n",
    "               'Test_R2': female_age_results[m]['test_r2'], \n",
    "               'Test_MAE': female_age_results[m]['test_mae'], \n",
    "               'Test_RMSE': female_age_results[m]['test_rmse'],\n",
    "               'CV_R2_Mean': female_age_results[m]['cv_r2_mean'],\n",
    "               'CV_R2_Std': female_age_results[m]['cv_r2_std'],\n",
    "               'CV_MAE_Mean': female_age_results[m]['cv_mae_mean'],\n",
    "               'CV_MAE_Std': female_age_results[m]['cv_mae_std']} \n",
    "              for m in all_models_reg]).to_csv('hbn_female_age_prediction_summary.csv', index=False)\n",
    "\n",
    "all_results = [\n",
    "    {'Model': m, \n",
    "     'Sex_AUC': sex_results[m]['test_auc'], \n",
    "     'Sex_Bal_Acc': sex_results[m]['test_balanced_accuracy'],\n",
    "     'Male_Age_R2': male_age_results[m]['test_r2'], \n",
    "     'Male_Age_MAE': male_age_results[m]['test_mae'], \n",
    "     'Male_Age_RMSE': male_age_results[m]['test_rmse'],\n",
    "     'Female_Age_R2': female_age_results[m]['test_r2'], \n",
    "     'Female_Age_MAE': female_age_results[m]['test_mae'], \n",
    "     'Female_Age_RMSE': female_age_results[m]['test_rmse']} \n",
    "    for m in list(features_dict.keys())\n",
    "]\n",
    "pd.DataFrame(all_results).to_csv('hbn_detailed_comparison.csv', index=False)\n",
    "\n",
    "rankings = []\n",
    "for task in ['Sex_AUC', 'Male_Age_R2', 'Female_Age_R2']:\n",
    "    for rank, item in enumerate(sorted(all_results, key=lambda x: x[task], reverse=True), 1):\n",
    "        rankings.append({\n",
    "            'Task': task.replace('_', ' '), \n",
    "            'Rank': rank, \n",
    "            'Model': item['Model'], \n",
    "            'Score': item[task]\n",
    "        })\n",
    "pd.DataFrame(rankings).to_csv('hbn_model_rankings.csv', index=False)\n",
    "\n",
    "print(\"Saved: hbn_sex_classification_summary.csv\")\n",
    "print(\"Saved: hbn_male_age_prediction_summary.csv\")\n",
    "print(\"Saved: hbn_female_age_prediction_summary.csv\")\n",
    "print(\"Saved: hbn_detailed_comparison.csv\")\n",
    "print(\"Saved: hbn_model_rankings.csv\\n\")\n",
    "\n",
    "#Visualization\n",
    "\n",
    "print(\"Learning Curves\")\n",
    "\n",
    "models = list(features_dict.keys())\n",
    "\n",
    "model_styles = {\n",
    "    'AnatCL': {'color': '#9B59B6', 'marker': '^', 'linestyle': '-', 'label_prefix': 'FM: '},\n",
    "    'BrainIAC': {'color': '#E74C3C', 'marker': '^', 'linestyle': '-', 'label_prefix': 'FM: '},\n",
    "    'CNN': {'color': '#3498DB', 'marker': 'o', 'linestyle': '--', 'label_prefix': ''},\n",
    "    'FreeSurfer': {'color': '#E67E22', 'marker': 's', 'linestyle': '-', 'label_prefix': ''}\n",
    "}\n",
    "\n",
    "n_cv_sex = len(sex_labels) - int(0.1 * len(sex_labels))\n",
    "n_cv_male = len(male_age_labels) - int(0.1 * len(male_age_labels))\n",
    "n_cv_female = len(female_age_labels) - int(0.1 * len(female_age_labels))\n",
    "\n",
    "alphas = [0.01, 0.05, 0.1, 0.15, 0.25, 0.4, 0.6, 0.8, 1.0]\n",
    "training_sizes_sex = [int(alpha * n_cv_sex * 0.8) for alpha in alphas]\n",
    "training_sizes_male = [int(alpha * n_cv_male * 0.8) for alpha in alphas]\n",
    "training_sizes_female = [int(alpha * n_cv_female * 0.8) for alpha in alphas]\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 6))\n",
    "\n",
    "# Sex classification\n",
    "for model in models:\n",
    "    style = model_styles[model]\n",
    "    acc_means = []\n",
    "    acc_stds = []\n",
    "    valid_sizes = []\n",
    "    \n",
    "    for i, alpha in enumerate(alphas):\n",
    "        if alpha in sex_results[model]['cv_results']['acc']:\n",
    "            acc_means.append(sex_results[model]['cv_results']['acc'][alpha])\n",
    "            acc_stds.append(sex_results[model]['cv_results']['acc_std'][alpha])\n",
    "            valid_sizes.append(training_sizes_sex[i])\n",
    "    \n",
    "    acc_means = np.array(acc_means)\n",
    "    acc_stds = np.array(acc_stds)\n",
    "    valid_sizes = np.array(valid_sizes)\n",
    "    \n",
    "    ax1.plot(valid_sizes, acc_means, marker=style['marker'], linestyle=style['linestyle'],\n",
    "             linewidth=3, markersize=10, label=style['label_prefix'] + model, \n",
    "             color=style['color'])\n",
    "    ax1.fill_between(valid_sizes, acc_means - acc_stds, acc_means + acc_stds, \n",
    "                      alpha=0.2, color=style['color'])\n",
    "\n",
    "dummy_acc_cv = sex_results['DummyClassifier']['cv_acc_mean']\n",
    "ax1.axhline(y=dummy_acc_cv, color='gray', linestyle=':', linewidth=2, \n",
    "            label=f'Baseline ({dummy_acc_cv:.3f})', alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Number of Training Subjects', fontsize=12, weight='bold')\n",
    "ax1.set_ylabel('Balanced Accuracy', fontsize=12, weight='bold')\n",
    "ax1.set_title('Sex Classification', fontsize=14, weight='bold')\n",
    "ax1.legend(fontsize=9, loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Male age prediction\n",
    "for model in models:\n",
    "    style = model_styles[model]\n",
    "    mae_means = []\n",
    "    mae_stds = []\n",
    "    valid_sizes = []\n",
    "    \n",
    "    for i, alpha in enumerate(alphas):\n",
    "        if alpha in male_age_results[model]['cv_results']['mae']:\n",
    "            mae_means.append(male_age_results[model]['cv_results']['mae'][alpha])\n",
    "            mae_stds.append(male_age_results[model]['cv_results']['mae_std'][alpha])\n",
    "            valid_sizes.append(training_sizes_male[i])\n",
    "    \n",
    "    mae_means = np.array(mae_means)\n",
    "    mae_stds = np.array(mae_stds)\n",
    "    valid_sizes = np.array(valid_sizes)\n",
    "    \n",
    "    ax2.plot(valid_sizes, mae_means, marker=style['marker'], linestyle=style['linestyle'],\n",
    "             linewidth=3, markersize=10, label=style['label_prefix'] + model,\n",
    "             color=style['color'])\n",
    "    ax2.fill_between(valid_sizes, mae_means - mae_stds, mae_means + mae_stds,\n",
    "                      alpha=0.2, color=style['color'])\n",
    "\n",
    "dummy_mae_cv_male = male_age_results['DummyRegressor']['cv_mae_mean']\n",
    "ax2.axhline(y=dummy_mae_cv_male, color='gray', linestyle=':', linewidth=2, \n",
    "            label=f'Baseline ({dummy_mae_cv_male:.2f})', alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('Number of Training Subjects', fontsize=12, weight='bold')\n",
    "ax2.set_ylabel('MAE (years)', fontsize=12, weight='bold')\n",
    "ax2.set_title('Male Age Prediction', fontsize=14, weight='bold')\n",
    "ax2.legend(fontsize=9, loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Female age prediction\n",
    "for model in models:\n",
    "    style = model_styles[model]\n",
    "    mae_means = []\n",
    "    mae_stds = []\n",
    "    valid_sizes = []\n",
    "    \n",
    "    for i, alpha in enumerate(alphas):\n",
    "        if alpha in female_age_results[model]['cv_results']['mae']:\n",
    "            mae_means.append(female_age_results[model]['cv_results']['mae'][alpha])\n",
    "            mae_stds.append(female_age_results[model]['cv_results']['mae_std'][alpha])\n",
    "            valid_sizes.append(training_sizes_female[i])\n",
    "    \n",
    "    mae_means = np.array(mae_means)\n",
    "    mae_stds = np.array(mae_stds)\n",
    "    valid_sizes = np.array(valid_sizes)\n",
    "    \n",
    "    ax3.plot(valid_sizes, mae_means, marker=style['marker'], linestyle=style['linestyle'],\n",
    "             linewidth=3, markersize=10, label=style['label_prefix'] + model,\n",
    "             color=style['color'])\n",
    "    ax3.fill_between(valid_sizes, mae_means - mae_stds, mae_means + mae_stds,\n",
    "                      alpha=0.2, color=style['color'])\n",
    "\n",
    "dummy_mae_cv_female = female_age_results['DummyRegressor']['cv_mae_mean']\n",
    "ax3.axhline(y=dummy_mae_cv_female, color='gray', linestyle=':', linewidth=2, \n",
    "            label=f'Baseline ({dummy_mae_cv_female:.2f})', alpha=0.7)\n",
    "\n",
    "ax3.set_xlabel('Number of Training Subjects', fontsize=12, weight='bold')\n",
    "ax3.set_ylabel('MAE (years)', fontsize=12, weight='bold')\n",
    "ax3.set_title('Female Age Prediction', fontsize=14, weight='bold')\n",
    "ax3.legend(fontsize=9, loc='upper right')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hbn_cv_learning_curves.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: hbn_cv_learning_curves.pdf\\n\")\n",
    "\n",
    "print(f\"\\nDataset: {len(common_subjects)} subjects\")\n",
    "print(f\"Age range: {age_labels.min():.1f} - {age_labels.max():.1f} years\")\n",
    "print(f\"Sex: {np.sum(sex_labels)} Female, {len(sex_labels) - np.sum(sex_labels)} Male\")\n",
    "print(f\"Males: {len(male_indices)} subjects\")\n",
    "print(f\"Females: {len(female_indices)} subjects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43ab5bec-c747-45c1-82e5-adc641bfd024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/arelbaha/links/projects/rrg-glatard/arelbaha/combined_multimodel_comparison.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/arelbaha/links/projects/rrg-glatard/arelbaha/combined_multimodel_comparison.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import nibabel as nib\n",
    "from anatcl import AnatCL\n",
    "from monai.networks.nets import ViT\n",
    "import torch.nn.functional as F\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.ndimage as ndi\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.set_num_threads(1)\n",
    "torch.set_num_interop_threads(1)\n",
    "\n",
    "device = \"cpu\"\n",
    "DROPOUT_RATE = 0.3\n",
    "ANATCL_ENCODER_PATH = \"/home/arelbaha/.venvs/jupyter_py3/bin\"\n",
    "BRAINIAC_CKPT = \"/home/arelbaha/.venvs/jupyter_py3/bin/BrainIAC.ckpt\"\n",
    "\n",
    "class CAT12VBMDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform):\n",
    "        self.data, self.labels, self.transform = data, labels, transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.transform(nib.load(self.data[idx]).get_fdata()).unsqueeze(0)\n",
    "        return img, torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "class BrainIACDataset(Dataset):\n",
    "    def __init__(self, paths, labels, transform):\n",
    "        self.paths, self.labels, self.transform = paths, labels, transform\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.paths[idx]).get_fdata().astype(np.float32)\n",
    "        if img.shape != (96, 96, 96):\n",
    "            img = F.interpolate(torch.from_numpy(img[None, None]), size=(96, 96, 96), \n",
    "                              mode='trilinear', align_corners=False).squeeze().numpy()\n",
    "        img_tensor = torch.from_numpy(img)\n",
    "        img_normalized = self.transform(img_tensor)\n",
    "        return img_normalized[None], torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "class CNNDataset(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.paths[idx]).get_fdata().astype(np.float32)\n",
    "        if img.shape != (92, 110, 92):\n",
    "            img = ndi.zoom(img, [92/img.shape[0], 110/img.shape[1], 92/img.shape[2]], order=1)\n",
    "        img = (img - img.mean()) / (img.std() + 1e-6)\n",
    "        return torch.from_numpy(img[None]).float()\n",
    "\n",
    "class CNN3D(nn.Module):\n",
    "    def __init__(self, dropout_rate=DROPOUT_RATE):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 64, 3, padding=1)\n",
    "        self.bn1, self.pool1, self.drop1 = nn.BatchNorm3d(64), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv2 = nn.Conv3d(64, 64, 3, padding=1)\n",
    "        self.bn2, self.pool2, self.drop2 = nn.BatchNorm3d(64), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv3 = nn.Conv3d(64, 128, 3, padding=1)\n",
    "        self.bn3, self.pool3, self.drop3 = nn.BatchNorm3d(128), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv4 = nn.Conv3d(128, 256, 3, padding=1)\n",
    "        self.bn4, self.pool4, self.drop4 = nn.BatchNorm3d(256), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc1, self.drop5 = nn.Linear(256, 512), nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop1(self.bn1(self.pool1(F.relu(self.conv1(x)))))\n",
    "        x = self.drop2(self.bn2(self.pool2(F.relu(self.conv2(x)))))\n",
    "        x = self.drop3(self.bn3(self.pool3(F.relu(self.conv3(x)))))\n",
    "        x = self.drop4(self.bn4(self.pool4(F.relu(self.conv4(x)))))\n",
    "        return self.drop5(F.relu(self.fc1(self.global_pool(x).view(x.size(0), -1))))\n",
    "\n",
    "def load_brainiac_vit(ckpt_path, device):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    vit = ViT(in_channels=1, img_size=(96, 96, 96), patch_size=(16, 16, 16),\n",
    "              hidden_size=768, mlp_dim=3072, num_layers=12, num_heads=12, classification=False)\n",
    "    pe_key = \"backbone.patch_embedding.position_embeddings\"\n",
    "    if pe_key in ckpt.get(\"state_dict\", ckpt):\n",
    "        vit.patch_embedding.position_embeddings = nn.Parameter(ckpt[\"state_dict\"][pe_key].clone())\n",
    "    backbone_state = {k.replace(\"backbone.\", \"\"): v for k, v in ckpt.get(\"state_dict\", ckpt).items() \n",
    "                     if k.startswith(\"backbone.\")}\n",
    "    vit.load_state_dict(backbone_state, strict=False)\n",
    "    return vit.to(device).eval()\n",
    "\n",
    "def run_classification(X_dict, y, test_size=0.1, task_name=\"Classification\"):\n",
    "    print(f\"Running {task_name}\")\n",
    "    results = {}\n",
    "    cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)), \n",
    "                                        stratify=y, random_state=SEED)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    alphas = [0.01, 0.05, 0.1, 0.15, 0.25, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "    dummy_val_auc, dummy_val_acc = [], []\n",
    "    for train_rel, val_rel in skf.split(cv_idx, y[cv_idx]):\n",
    "        train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "        dummy = DummyClassifier(strategy='stratified', random_state=SEED)\n",
    "        dummy.fit(np.zeros((len(train_idx), 1)), y[train_idx])\n",
    "        pred = dummy.predict(np.zeros((len(val_idx), 1)))\n",
    "        pred_proba = dummy.predict_proba(np.zeros((len(val_idx), 1)))[:, 1]\n",
    "        dummy_val_acc.append(balanced_accuracy_score(y[val_idx], pred))\n",
    "        dummy_val_auc.append(roc_auc_score(y[val_idx], pred_proba))\n",
    "    \n",
    "    dummy = DummyClassifier(strategy='stratified', random_state=SEED)\n",
    "    dummy.fit(np.zeros((len(cv_idx), 1)), y[cv_idx])\n",
    "    dummy_test_pred = dummy.predict(np.zeros((len(test_idx), 1)))\n",
    "    dummy_test_proba = dummy.predict_proba(np.zeros((len(test_idx), 1)))[:, 1]\n",
    "    \n",
    "    results['DummyClassifier'] = {\n",
    "        'best_alpha': 1.0,\n",
    "        'test_balanced_accuracy': balanced_accuracy_score(y[test_idx], dummy_test_pred),\n",
    "        'test_auc': roc_auc_score(y[test_idx], dummy_test_proba),\n",
    "        'cv_auc_mean': np.mean(dummy_val_auc), 'cv_auc_std': np.std(dummy_val_auc),\n",
    "        'cv_acc_mean': np.mean(dummy_val_acc), 'cv_acc_std': np.std(dummy_val_acc),\n",
    "        'fold_results': {'val_auc': dummy_val_auc, 'val_acc': dummy_val_acc}, 'test_idx': test_idx\n",
    "    }\n",
    "    print(f\"  DummyClassifier: AUC={results['DummyClassifier']['test_auc']:.4f}\")\n",
    "\n",
    "    for model, X in X_dict.items():\n",
    "        val_auc = {a: [] for a in alphas}\n",
    "        val_acc = {a: [] for a in alphas}\n",
    "        train_auc = {a: [] for a in alphas}\n",
    "        train_acc = {a: [] for a in alphas}\n",
    "\n",
    "        for train_rel, val_rel in skf.split(cv_idx, y[cv_idx]):\n",
    "            train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "            for alpha in alphas:\n",
    "                n = int(alpha * len(train_idx))\n",
    "                try:\n",
    "                    tr_idx, _ = train_test_split(train_idx, train_size=n, stratify=y[train_idx], random_state=SEED)\n",
    "                except:\n",
    "                    tr_idx = np.random.choice(train_idx, n, replace=False)\n",
    "                if len(np.unique(y[tr_idx])) < 2:\n",
    "                    continue\n",
    "                scaler = StandardScaler()\n",
    "                rf = RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_split=5,\n",
    "                                           random_state=SEED, n_jobs=1, max_features='sqrt', class_weight='balanced')\n",
    "                rf.fit(scaler.fit_transform(X[tr_idx]), y[tr_idx])\n",
    "                pred_proba = rf.predict_proba(scaler.transform(X[val_idx]))[:, 1]\n",
    "                val_acc[alpha].append(balanced_accuracy_score(y[val_idx], rf.predict(scaler.transform(X[val_idx]))))\n",
    "                val_auc[alpha].append(roc_auc_score(y[val_idx], pred_proba))\n",
    "                train_pred_proba = rf.predict_proba(scaler.transform(X[tr_idx]))[:, 1]\n",
    "                train_acc[alpha].append(balanced_accuracy_score(y[tr_idx], rf.predict(scaler.transform(X[tr_idx]))))\n",
    "                train_auc[alpha].append(roc_auc_score(y[tr_idx], train_pred_proba))\n",
    "\n",
    "        avg_auc = {a: np.mean(val_auc[a]) for a in alphas if val_auc[a]}\n",
    "        best_alpha = max(avg_auc, key=avg_auc.get)\n",
    "        train_val_gaps = {a: np.mean(train_auc[a]) - np.mean(val_auc[a]) for a in alphas if train_auc[a] and val_auc[a]}\n",
    "\n",
    "        try:\n",
    "            final_tr, _ = train_test_split(cv_idx, train_size=int(best_alpha * len(cv_idx)), stratify=y[cv_idx], random_state=SEED)\n",
    "        except:\n",
    "            final_tr = cv_idx\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        rf = RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_split=5,\n",
    "                                    random_state=SEED, n_jobs=1, max_features='sqrt', class_weight='balanced')\n",
    "        rf.fit(scaler.fit_transform(X[final_tr]), y[final_tr])\n",
    "        test_pred_proba = rf.predict_proba(scaler.transform(X[test_idx]))[:, 1]\n",
    "        train_val_gap = train_val_gaps[best_alpha]\n",
    "\n",
    "        results[model] = {\n",
    "            'best_alpha': best_alpha,\n",
    "            'test_balanced_accuracy': balanced_accuracy_score(y[test_idx], rf.predict(scaler.transform(X[test_idx]))),\n",
    "            'test_auc': roc_auc_score(y[test_idx], test_pred_proba),\n",
    "            'cv_auc_mean': np.mean(val_auc[best_alpha]), 'cv_auc_std': np.std(val_auc[best_alpha]),\n",
    "            'cv_acc_mean': np.mean(val_acc[best_alpha]), 'cv_acc_std': np.std(val_acc[best_alpha]),\n",
    "            'train_auc_mean': np.mean(train_auc[best_alpha]), 'train_val_gap': train_val_gap,\n",
    "            'train_val_gaps_all_alphas': train_val_gaps, 'is_overfitting': train_val_gap > 0.25,\n",
    "            'fold_results': {'val_auc': val_auc[best_alpha], 'val_acc': val_acc[best_alpha], 'train_auc': train_auc[best_alpha]},\n",
    "            'cv_results': {\n",
    "                'alphas': alphas, \n",
    "                'acc': {a: np.mean(val_acc[a]) for a in alphas if val_acc[a]},\n",
    "                'auc': avg_auc,\n",
    "                'acc_std': {a: np.std(val_acc[a]) for a in alphas if val_acc[a]},\n",
    "                'auc_std': {a: np.std(val_auc[a]) for a in alphas if val_auc[a]},\n",
    "            },\n",
    "            'trained_model': rf, 'scaler': scaler, 'test_idx': test_idx\n",
    "        }\n",
    "        print(f\"  {model}: AUC={results[model]['test_auc']:.4f}, Bal_Acc={results[model]['test_balanced_accuracy']:.4f}\")\n",
    "    return results\n",
    "\n",
    "def run_regression(X_dict, y, test_size=0.1, task_name=\"Regression\"):\n",
    "    print(f\"Running {task_name}\")\n",
    "    results = {}\n",
    "    cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)), random_state=SEED)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    alphas = [0.01, 0.05, 0.1, 0.15, 0.25, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "    dummy_val_r2, dummy_val_mae = [], []\n",
    "    for train_rel, val_rel in kf.split(cv_idx):\n",
    "        train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "        dummy = DummyRegressor(strategy='mean')\n",
    "        dummy.fit(np.zeros((len(train_idx), 1)), y[train_idx])\n",
    "        pred = dummy.predict(np.zeros((len(val_idx), 1)))\n",
    "        dummy_val_r2.append(r2_score(y[val_idx], pred))\n",
    "        dummy_val_mae.append(mean_absolute_error(y[val_idx], pred))\n",
    "    \n",
    "    dummy = DummyRegressor(strategy='mean')\n",
    "    dummy.fit(np.zeros((len(cv_idx), 1)), y[cv_idx])\n",
    "    dummy_test_pred = dummy.predict(np.zeros((len(test_idx), 1)))\n",
    "    \n",
    "    results['DummyRegressor'] = {\n",
    "        'best_alpha': 1.0,\n",
    "        'test_r2': r2_score(y[test_idx], dummy_test_pred),\n",
    "        'test_mae': mean_absolute_error(y[test_idx], dummy_test_pred),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y[test_idx], dummy_test_pred)),\n",
    "        'cv_r2_mean': np.mean(dummy_val_r2), 'cv_r2_std': np.std(dummy_val_r2),\n",
    "        'cv_mae_mean': np.mean(dummy_val_mae), 'cv_mae_std': np.std(dummy_val_mae),\n",
    "        'fold_results': {'val_r2': dummy_val_r2, 'val_mae': dummy_val_mae}, 'test_idx': test_idx\n",
    "    }\n",
    "    print(f\"  DummyRegressor: R2={results['DummyRegressor']['test_r2']:.4f}, MAE={results['DummyRegressor']['test_mae']:.2f}\")\n",
    "\n",
    "    for model, X in X_dict.items():\n",
    "        val_r2 = {a: [] for a in alphas}\n",
    "        val_mae = {a: [] for a in alphas}\n",
    "        train_r2 = {a: [] for a in alphas}\n",
    "        train_mae = {a: [] for a in alphas}\n",
    "\n",
    "        for train_rel, val_rel in kf.split(cv_idx):\n",
    "            train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "            for alpha in alphas:\n",
    "                n = int(alpha * len(train_idx))\n",
    "                tr_idx, _ = train_test_split(train_idx, train_size=n, random_state=SEED) if n < len(train_idx) else (train_idx, None)\n",
    "                scaler = StandardScaler()\n",
    "                rf = RandomForestRegressor(n_estimators=200, max_depth=6, min_samples_split=5, random_state=SEED, n_jobs=1)\n",
    "                rf.fit(scaler.fit_transform(X[tr_idx]), y[tr_idx])\n",
    "                pred = rf.predict(scaler.transform(X[val_idx]))\n",
    "                val_r2[alpha].append(r2_score(y[val_idx], pred))\n",
    "                val_mae[alpha].append(mean_absolute_error(y[val_idx], pred))\n",
    "                train_pred = rf.predict(scaler.transform(X[tr_idx]))\n",
    "                train_r2[alpha].append(r2_score(y[tr_idx], train_pred))\n",
    "                train_mae[alpha].append(mean_absolute_error(y[tr_idx], train_pred))\n",
    "\n",
    "        avg_r2 = {a: np.mean(val_r2[a]) for a in alphas if val_r2[a]}\n",
    "        best_alpha = max(avg_r2, key=avg_r2.get)\n",
    "        train_val_gaps = {a: np.mean(train_r2[a]) - np.mean(val_r2[a]) for a in alphas if train_r2[a] and val_r2[a]}\n",
    "\n",
    "        n = int(best_alpha * len(cv_idx))\n",
    "        final_tr, _ = train_test_split(cv_idx, train_size=n, random_state=SEED) if n < len(cv_idx) else (cv_idx, None)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        rf = RandomForestRegressor(n_estimators=200, max_depth=6, min_samples_split=5, random_state=SEED, n_jobs=1)\n",
    "        rf.fit(scaler.fit_transform(X[final_tr]), y[final_tr])\n",
    "        test_pred = rf.predict(scaler.transform(X[test_idx]))\n",
    "        train_val_gap = train_val_gaps[best_alpha]\n",
    "\n",
    "        results[model] = {\n",
    "            'best_alpha': best_alpha,\n",
    "            'test_r2': r2_score(y[test_idx], test_pred),\n",
    "            'test_mae': mean_absolute_error(y[test_idx], test_pred),\n",
    "            'test_rmse': np.sqrt(mean_squared_error(y[test_idx], test_pred)),\n",
    "            'predictions': test_pred, 'actual': y[test_idx],\n",
    "            'cv_r2_mean': np.mean(val_r2[best_alpha]), 'cv_r2_std': np.std(val_r2[best_alpha]),\n",
    "            'cv_mae_mean': np.mean(val_mae[best_alpha]), 'cv_mae_std': np.std(val_mae[best_alpha]),\n",
    "            'train_r2_mean': np.mean(train_r2[best_alpha]), 'train_val_gap': train_val_gap,\n",
    "            'train_val_gaps_all_alphas': train_val_gaps, 'is_overfitting': train_val_gap > 0.35,\n",
    "            'fold_results': {'val_r2': val_r2[best_alpha], 'val_mae': val_mae[best_alpha], 'train_r2': train_r2[best_alpha]},\n",
    "            'cv_results': {\n",
    "                'alphas': alphas,\n",
    "                'r2': avg_r2,\n",
    "                'mae': {a: np.mean(val_mae[a]) for a in alphas if val_mae[a]},\n",
    "                'r2_std': {a: np.std(val_r2[a]) for a in alphas if val_r2[a]},\n",
    "                'mae_std': {a: np.std(val_mae[a]) for a in alphas if val_mae[a]},\n",
    "            },\n",
    "            'trained_model': rf, 'scaler': scaler, 'test_idx': test_idx\n",
    "        }\n",
    "        print(f\"  {model}: R2={results[model]['test_r2']:.4f}, MAE={results[model]['test_mae']:.2f}\")\n",
    "    return results\n",
    "\n",
    "def compute_correlation_with_cluster_labels(features_dict, fs_feature_info, dataset_name):\n",
    "    all_features = []\n",
    "    boundaries = [0]\n",
    "    reordered_indices = {}\n",
    "\n",
    "    for model in ['AnatCL', 'BrainIAC', 'CNN', 'FreeSurfer']:\n",
    "        if model not in features_dict:\n",
    "            continue\n",
    "        feats = StandardScaler().fit_transform(features_dict[model])\n",
    "        valid = ~np.isnan(feats).any(axis=0) & (np.std(feats, axis=0) > 1e-10)\n",
    "        feats = feats[:, valid]\n",
    "        if feats.shape[1] == 0:\n",
    "            continue\n",
    "        print(f\"  {model}: {feats.shape[1]} features\")\n",
    "        \n",
    "        corr = np.corrcoef(feats.T)\n",
    "        corr = np.nan_to_num((corr + corr.T) / 2, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        np.fill_diagonal(corr, 1.0)\n",
    "        dist = np.abs(1 - np.abs(corr))\n",
    "        np.fill_diagonal(dist, 0)\n",
    "        condensed = np.nan_to_num(squareform((dist + dist.T) / 2, checks=False), nan=0.0, posinf=1.0, neginf=0.0)\n",
    "        reorder_idx = dendrogram(linkage(condensed, method='ward'), no_plot=True)['leaves']\n",
    "        all_features.append(feats[:, reorder_idx])\n",
    "        boundaries.append(boundaries[-1] + feats.shape[1])\n",
    "        reordered_indices[model] = reorder_idx\n",
    "\n",
    "    combined = np.hstack(all_features)\n",
    "    corr = np.nan_to_num((np.corrcoef(combined.T) + np.corrcoef(combined.T).T) / 2, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "\n",
    "    reordered_fs_indices = reordered_indices['FreeSurfer']\n",
    "    \n",
    "    cluster_df = pd.DataFrame([\n",
    "        {**fs_feature_info[idx], 'original_index': idx, 'reordered_position': pos}\n",
    "        for pos, idx in enumerate(reordered_fs_indices)\n",
    "    ])\n",
    "    \n",
    "    n_sa = np.sum(cluster_df['feature_type'] == 'Surface_Area')\n",
    "    n_thick = np.sum(cluster_df['feature_type'] == 'Thickness')\n",
    "    total = len(cluster_df)\n",
    "    \n",
    "    print(f\"\\n{dataset_name} FreeSurfer ({total} features reordered by correlation):\")\n",
    "    print(f\"  Surface Area: {n_sa} ({100*n_sa/total:.1f}%)\")\n",
    "    print(f\"  Thickness: {n_thick} ({100*n_thick/total:.1f}%)\")\n",
    "    \n",
    "    return corr, boundaries, ['AnatCL', 'BrainIAC', 'CNN', 'FreeSurfer'], reordered_indices, cluster_df\n",
    "\n",
    "def extract_features(cat12_paths, brainiac_paths, age_labels, device):\n",
    "    anatcl_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda x: torch.from_numpy(x.copy()).float()),\n",
    "        transforms.Normalize(mean=0.0, std=1.0)\n",
    "    ])\n",
    "    \n",
    "    print(\"Extracting AnatCL features...\")\n",
    "    all_fold_features = []\n",
    "    for fold_idx in range(5):\n",
    "        path = os.path.join(ANATCL_ENCODER_PATH, f\"fold{fold_idx}.pth\")\n",
    "        encoder = AnatCL(descriptor=\"global\", fold=fold_idx, pretrained=False).to(device).eval()\n",
    "        encoder.backbone.load_state_dict(torch.load(path, map_location=device, weights_only=False)['model'])\n",
    "        for p in encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        dl = DataLoader(CAT12VBMDataset(cat12_paths, age_labels, anatcl_transform), batch_size=32, num_workers=0)\n",
    "        with torch.no_grad():\n",
    "            fold_features = torch.cat([encoder(vol.to(device)).cpu() for vol, _ in dl]).numpy()\n",
    "        all_fold_features.append(fold_features)\n",
    "        del encoder\n",
    "    anatcl_features = np.mean(all_fold_features, axis=0)\n",
    "    print(f\"  AnatCL: {anatcl_features.shape}\")\n",
    "    \n",
    "    print(\"Extracting BrainIAC features...\")\n",
    "    brainiac_vit = load_brainiac_vit(BRAINIAC_CKPT, device)\n",
    "    brainiac_transform = lambda x: (x - x.mean()) / (x.std() + 1e-6)\n",
    "    dl = DataLoader(BrainIACDataset(brainiac_paths, age_labels, brainiac_transform), batch_size=16, num_workers=0)\n",
    "    brainiac_features = []\n",
    "    with torch.no_grad():\n",
    "        for x, _ in dl:\n",
    "            out = brainiac_vit(x.to(device))\n",
    "            cls_token = out[0][:, 0] if isinstance(out, tuple) else out[:, 0]\n",
    "            brainiac_features.append(cls_token.cpu().numpy())\n",
    "    brainiac_features = np.vstack(brainiac_features)\n",
    "    print(f\"  BrainIAC: {brainiac_features.shape}\")\n",
    "    del brainiac_vit\n",
    "    \n",
    "    print(\"Extracting CNN features...\")\n",
    "    cnn_model = CNN3D().to(device).eval()\n",
    "    dl = DataLoader(CNNDataset(brainiac_paths), batch_size=8, num_workers=0)\n",
    "    with torch.no_grad():\n",
    "        cnn_features = np.vstack([cnn_model(x.to(device)).cpu().numpy() for x in dl])\n",
    "    print(f\"  CNN: {cnn_features.shape}\")\n",
    "    del cnn_model\n",
    "    \n",
    "    return anatcl_features, brainiac_features, cnn_features\n",
    "\n",
    "def plot_correlation_with_labels(corr_matrix, boundaries, model_names, cluster_df, dataset_name):\n",
    "    fig, ax = plt.subplots(figsize=(32, 30))\n",
    "    \n",
    "    im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1, aspect='equal', interpolation='nearest')\n",
    "    plt.colorbar(im, ax=ax, shrink=0.3)\n",
    "    \n",
    "    for b in boundaries[1:-1]:\n",
    "        ax.axhline(y=b-0.5, color='black', linewidth=4)\n",
    "        ax.axvline(x=b-0.5, color='black', linewidth=4)\n",
    "    \n",
    "    fs_start = boundaries[3]\n",
    "    fs_end = boundaries[4]\n",
    "    bar_width = 8\n",
    "    \n",
    "    feature_types = cluster_df['feature_type'].values\n",
    "    colors = {'Surface_Area': '#2ECC71', 'Thickness': '#3498DB'}\n",
    "    \n",
    "    for i, ft in enumerate(feature_types):\n",
    "        color = colors.get(ft, '#95A5A6')\n",
    "        rect_right = plt.Rectangle((fs_end + 3, fs_start + i - 0.5), bar_width, 1,\n",
    "                                    facecolor=color, edgecolor='none')\n",
    "        ax.add_patch(rect_right)\n",
    "        rect_left = plt.Rectangle((-bar_width - 5, fs_start + i - 0.5), bar_width, 1,\n",
    "                                   facecolor=color, edgecolor='none')\n",
    "        ax.add_patch(rect_left)\n",
    "    \n",
    "    for i in range(len(boundaries) - 1):\n",
    "        pos = (boundaries[i] + boundaries[i+1]) / 2\n",
    "        label = model_names[i]\n",
    "        ax.text(pos, -40, label, ha='center', fontsize=20, weight='bold')\n",
    "        ax.text(-50, pos, label, ha='center', va='center', fontsize=20, weight='bold', rotation=90)\n",
    "    \n",
    "    ax.set_xlim(-bar_width - 60, fs_end + bar_width + 50)\n",
    "    ax.set_ylim(fs_end + 20, -60)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    \n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#2ECC71', edgecolor='black', label='Surface Area'),\n",
    "        Patch(facecolor='#3498DB', edgecolor='black', label='Thickness')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{dataset_name.lower()}_cross_model_correlation.pdf', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {dataset_name.lower()}_cross_model_correlation.pdf\")\n",
    "\n",
    "\n",
    "print(\"PPMI DATASET\")\n",
    "PPMI_CAT12_BASE_DIR = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data/inputs\"\n",
    "PPMI_DATA_DIR = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data\"\n",
    "PPMI_LABELS_PATH = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data/processed_cohort_with_mri.csv\"\n",
    "PPMI_BRAINIAC_MAPPING_CSV = os.path.join(PPMI_DATA_DIR, \"processed_files_mapping.csv\")\n",
    "\n",
    "print(\"Loading PPMI demographics...\")\n",
    "ppmi_labels_df = pd.read_csv(PPMI_LABELS_PATH)\n",
    "ppmi_id_sex_dict, ppmi_id_parkinson_dict, ppmi_id_age_dict = {}, {}, {}\n",
    "\n",
    "for _, row in ppmi_labels_df.iterrows():\n",
    "    patno = str(int(row['PATNO']))\n",
    "    sex = row['Sex'].strip().upper()\n",
    "    ppmi_id_sex_dict[patno] = 1 if sex == 'F' else 0\n",
    "    group = row['Group'].strip()\n",
    "    ppmi_id_parkinson_dict[patno] = 1 if group == 'PD' else 0\n",
    "    ppmi_id_age_dict[patno] = row['Age']\n",
    "\n",
    "print(f\"  {len(ppmi_id_sex_dict)} subjects\")\n",
    "\n",
    "def extract_patno_from_path(filepath):\n",
    "    for part in filepath.split(os.sep):\n",
    "        if part.startswith('sub-'):\n",
    "            return part[4:]\n",
    "    return None\n",
    "\n",
    "print(\"Finding PPMI CAT12 files...\")\n",
    "ppmi_cat12_files = glob.glob(os.path.join(PPMI_CAT12_BASE_DIR, \"**\", \"*s6mwp1*.nii*\"), recursive=True)\n",
    "ppmi_cat12_data = {}\n",
    "for f in ppmi_cat12_files:\n",
    "    if not os.path.isfile(f):\n",
    "        continue\n",
    "    patno = extract_patno_from_path(f)\n",
    "    if patno and patno in ppmi_id_sex_dict:\n",
    "        ppmi_cat12_data[patno] = f\n",
    "print(f\"  Found {len(ppmi_cat12_data)} CAT12 files\")\n",
    "\n",
    "print(\"Finding PPMI BrainIAC files...\")\n",
    "ppmi_brainiac_df = pd.read_csv(PPMI_BRAINIAC_MAPPING_CSV).dropna(subset=[\"processed_file\", \"Age\", \"subject_id\"])\n",
    "ppmi_brainiac_data = {}\n",
    "for _, row in ppmi_brainiac_df.iterrows():\n",
    "    patno = str(row['subject_id'])\n",
    "    if patno in ppmi_id_sex_dict and os.path.exists(row['processed_file']):\n",
    "        ppmi_brainiac_data[patno] = row['processed_file']\n",
    "print(f\"  Found {len(ppmi_brainiac_data)} BrainIAC files\")\n",
    "\n",
    "print(\"Loading PPMI FreeSurfer features...\")\n",
    "ppmi_fs_cth_df = pd.read_csv(os.path.join(PPMI_CAT12_BASE_DIR, \"FS7_APARC_CTH_23Oct2025.csv\"))\n",
    "ppmi_fs_sa_df = pd.read_csv(os.path.join(PPMI_CAT12_BASE_DIR, \"FS7_APARC_SA_23Oct2025.csv\"))\n",
    "ppmi_fs_cth_df = ppmi_fs_cth_df[ppmi_fs_cth_df['EVENT_ID'] == 'BL'].copy()\n",
    "ppmi_fs_sa_df = ppmi_fs_sa_df[ppmi_fs_sa_df['EVENT_ID'] == 'BL'].copy()\n",
    "ppmi_fs_cth_df['PATNO'] = ppmi_fs_cth_df['PATNO'].astype(str)\n",
    "ppmi_fs_sa_df['PATNO'] = ppmi_fs_sa_df['PATNO'].astype(str)\n",
    "ppmi_cth_features = [c for c in ppmi_fs_cth_df.columns if c not in ['PATNO', 'EVENT_ID']]\n",
    "ppmi_sa_features = [c for c in ppmi_fs_sa_df.columns if c not in ['PATNO', 'EVENT_ID']]\n",
    "\n",
    "ppmi_fs_feature_info = []\n",
    "for feat in ppmi_cth_features:\n",
    "    ppmi_fs_feature_info.append({'feature_name': feat, 'feature_type': 'Thickness'})\n",
    "for feat in ppmi_sa_features:\n",
    "    ppmi_fs_feature_info.append({'feature_name': feat, 'feature_type': 'Surface_Area'})\n",
    "\n",
    "ppmi_common_subjects = sorted(list(set(ppmi_cat12_data.keys()) & set(ppmi_brainiac_data.keys())))\n",
    "ppmi_fs_data = {}\n",
    "for patno in ppmi_common_subjects:\n",
    "    cth_row = ppmi_fs_cth_df[ppmi_fs_cth_df['PATNO'] == patno]\n",
    "    sa_row = ppmi_fs_sa_df[ppmi_fs_sa_df['PATNO'] == patno]\n",
    "    if len(cth_row) > 0 and len(sa_row) > 0:\n",
    "        combined = np.concatenate([cth_row[ppmi_cth_features].values.flatten(), sa_row[ppmi_sa_features].values.flatten()])\n",
    "        if not np.any(np.isnan(combined)):\n",
    "            ppmi_fs_data[patno] = combined\n",
    "\n",
    "ppmi_common_subjects = sorted(list(set(ppmi_cat12_data.keys()) & set(ppmi_brainiac_data.keys()) & set(ppmi_fs_data.keys())))\n",
    "print(f\"  Common subjects: {len(ppmi_common_subjects)}\")\n",
    "\n",
    "ppmi_cat12_paths = [ppmi_cat12_data[p] for p in ppmi_common_subjects]\n",
    "ppmi_brainiac_paths = [ppmi_brainiac_data[p] for p in ppmi_common_subjects]\n",
    "ppmi_fs_features = np.array([ppmi_fs_data[p] for p in ppmi_common_subjects])\n",
    "ppmi_sex_labels = np.array([ppmi_id_sex_dict[p] for p in ppmi_common_subjects])\n",
    "ppmi_parkinson_labels = np.array([ppmi_id_parkinson_dict[p] for p in ppmi_common_subjects])\n",
    "ppmi_age_labels = np.array([ppmi_id_age_dict[p] for p in ppmi_common_subjects])\n",
    "\n",
    "ppmi_anatcl_features, ppmi_brainiac_features, ppmi_cnn_features = extract_features(\n",
    "    ppmi_cat12_paths, ppmi_brainiac_paths, ppmi_age_labels, device)\n",
    "\n",
    "ppmi_features_dict = {\n",
    "    'AnatCL': ppmi_anatcl_features,\n",
    "    'BrainIAC': ppmi_brainiac_features,\n",
    "    'CNN': ppmi_cnn_features,\n",
    "    'FreeSurfer': ppmi_fs_features\n",
    "}\n",
    "\n",
    "print(\"\\nComputing PPMI correlation matrix...\")\n",
    "ppmi_corr_matrix, ppmi_boundaries, ppmi_model_names, ppmi_reorder_indices, ppmi_cluster_df = \\\n",
    "    compute_correlation_with_cluster_labels(ppmi_features_dict, ppmi_fs_feature_info, \"PPMI\")\n",
    "\n",
    "print(\"\\nPPMI Sex Classification\")\n",
    "ppmi_sex_results = run_classification(ppmi_features_dict, ppmi_sex_labels, task_name=\"PPMI Sex\")\n",
    "\n",
    "print(\"\\nPPMI Parkinson Classification\")\n",
    "ppmi_parkinson_results = run_classification(ppmi_features_dict, ppmi_parkinson_labels, task_name=\"PPMI Parkinson\")\n",
    "\n",
    "ppmi_male_indices = np.where(ppmi_sex_labels == 0)[0]\n",
    "ppmi_male_features_dict = {k: v[ppmi_male_indices] for k, v in ppmi_features_dict.items()}\n",
    "ppmi_male_age_labels = ppmi_age_labels[ppmi_male_indices]\n",
    "print(f\"\\nPPMI Male subjects: {len(ppmi_male_indices)}\")\n",
    "ppmi_male_age_results = run_regression(ppmi_male_features_dict, ppmi_male_age_labels, task_name=\"PPMI Male Age\")\n",
    "\n",
    "ppmi_female_indices = np.where(ppmi_sex_labels == 1)[0]\n",
    "ppmi_female_features_dict = {k: v[ppmi_female_indices] for k, v in ppmi_features_dict.items()}\n",
    "ppmi_female_age_labels = ppmi_age_labels[ppmi_female_indices]\n",
    "print(f\"\\nPPMI Female subjects: {len(ppmi_female_indices)}\")\n",
    "ppmi_female_age_results = run_regression(ppmi_female_features_dict, ppmi_female_age_labels, task_name=\"PPMI Female Age\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nHBN DATASET\")\n",
    "HBN_BIDS = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/HBN_BIDS\"\n",
    "HBN_BIDS_LOWER = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/hbn_bids\"\n",
    "HBN_BRAINIAC_OUT = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/brainiac_p_outputs\"\n",
    "HBN_FREESURFER_DIR = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/HBN_FreeSurfer/freesurfer\"\n",
    "HBN_DEMO_FILE = os.path.join(HBN_BIDS, \"final_preprocessed_subjects_with_demographics.tsv\")\n",
    "HBN_PARCELLATION = \"Schaefer2018_400Parcels_17Networks_order\"\n",
    "\n",
    "print(\"Loading HBN demographics...\")\n",
    "hbn_demo_df = pd.read_csv(HBN_DEMO_FILE, sep='\\t')\n",
    "hbn_demo_df['participant_id'] = hbn_demo_df['participant_id'].astype(str)\n",
    "hbn_id_sex_dict, hbn_id_age_dict = {}, {}\n",
    "\n",
    "for _, row in hbn_demo_df.iterrows():\n",
    "    subject_id = row['participant_id']\n",
    "    sex = row['sex'].strip()\n",
    "    hbn_id_sex_dict[subject_id] = 1 if sex == 'Female' else 0\n",
    "    hbn_id_age_dict[subject_id] = row['age']\n",
    "\n",
    "print(f\"  {len(hbn_id_sex_dict)} subjects\")\n",
    "\n",
    "print(\"Finding HBN CAT12 files...\")\n",
    "hbn_cat12_data = {}\n",
    "for subject_id in hbn_id_sex_dict.keys():\n",
    "    pattern = os.path.join(HBN_BIDS, f\"sub-{subject_id}\", \"ses-*\", \"anat\", \"mri\", \"s6mwp1sub*.nii\")\n",
    "    files = glob.glob(pattern)\n",
    "    if files:\n",
    "        hbn_cat12_data[subject_id] = files[0]\n",
    "for subject_id in hbn_id_sex_dict.keys():\n",
    "    if subject_id not in hbn_cat12_data:\n",
    "        pattern = os.path.join(HBN_BIDS_LOWER, f\"sub-{subject_id}\", \"ses-*\", \"anat\", \"mri\", \"s6mwp1sub*.nii\")\n",
    "        files = glob.glob(pattern)\n",
    "        if files:\n",
    "            hbn_cat12_data[subject_id] = files[0]\n",
    "print(f\"  Found {len(hbn_cat12_data)} CAT12 files\")\n",
    "\n",
    "print(\"Finding HBN BrainIAC files...\")\n",
    "hbn_brainiac_data = {}\n",
    "batch_dirs = glob.glob(os.path.join(HBN_BRAINIAC_OUT, \"batch_*\"))\n",
    "for batch_dir in sorted(batch_dirs):\n",
    "    files = glob.glob(os.path.join(batch_dir, \"sub-*_0000.nii.gz\"))\n",
    "    for f in files:\n",
    "        basename = os.path.basename(f)\n",
    "        subject_id = basename.split('_')[0].replace('sub-', '')\n",
    "        if subject_id in hbn_id_sex_dict and subject_id not in hbn_brainiac_data:\n",
    "            hbn_brainiac_data[subject_id] = f\n",
    "print(f\"  Found {len(hbn_brainiac_data)} BrainIAC files\")\n",
    "\n",
    "print(\"Loading HBN FreeSurfer features...\")\n",
    "hbn_fs_data = {}\n",
    "hbn_fs_region_info = {}\n",
    "hbn_fs_row_counts = []\n",
    "hbn_fs_region_sets = {}\n",
    "hbn_fs_extra_regions = {}\n",
    "\n",
    "for subject_id in hbn_id_sex_dict.keys():\n",
    "    subject_dir = os.path.join(HBN_FREESURFER_DIR, f\"sub-{subject_id}\")\n",
    "    stats_file = os.path.join(subject_dir, f\"sub-{subject_id}_regionsurfacestats.tsv\")\n",
    "    if os.path.exists(stats_file):\n",
    "        try:\n",
    "            df = pd.read_csv(stats_file, sep='\\t')\n",
    "            filtered_df = df[df[\"atlas\"] == HBN_PARCELLATION]\n",
    "            if not filtered_df.empty:\n",
    "                filtered_df = filtered_df.sort_values(\"StructName\")\n",
    "                hbn_fs_row_counts.append(len(filtered_df))\n",
    "                if len(filtered_df) > 400:\n",
    "                    extra = filtered_df['StructName'].values[400:].tolist()\n",
    "                    hbn_fs_extra_regions[subject_id] = extra\n",
    "                if \"SurfArea\" in filtered_df.columns and \"ThickAvg\" in filtered_df.columns:\n",
    "                    surf_area = filtered_df[\"SurfArea\"].values[:400]\n",
    "                    thick_avg = filtered_df[\"ThickAvg\"].values[:400]\n",
    "                    combined = np.concatenate([surf_area, thick_avg])\n",
    "                    if not np.any(np.isnan(combined)) and len(surf_area) == 400 and len(thick_avg) == 400:\n",
    "                        hbn_fs_data[subject_id] = combined\n",
    "                        region_names_400 = tuple(filtered_df['StructName'].values[:400].tolist())\n",
    "                        hbn_fs_region_sets[subject_id] = region_names_400\n",
    "                        if not hbn_fs_region_info:\n",
    "                            hbn_fs_region_info = {\n",
    "                                'region_names': list(region_names_400),\n",
    "                                'hemisphere': filtered_df['hemisphere'].values[:400].tolist(),\n",
    "                            }\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if hbn_fs_row_counts:\n",
    "    unique_counts = np.unique(hbn_fs_row_counts)\n",
    "    if len(unique_counts) > 1:\n",
    "        print(f\"  WARNING: Inconsistent row counts found: {unique_counts}\")\n",
    "        for cnt in unique_counts:\n",
    "            print(f\"    {cnt} rows: {np.sum(np.array(hbn_fs_row_counts) == cnt)} subjects\")\n",
    "\n",
    "if hbn_fs_extra_regions:\n",
    "    print(f\"  Subjects with >400 rows: {len(hbn_fs_extra_regions)}\")\n",
    "    all_extras = [r for regions in hbn_fs_extra_regions.values() for r in regions]\n",
    "    unique_extras = set(all_extras)\n",
    "    print(f\"  Truncated regions (appear after row 400): {unique_extras}\")\n",
    "\n",
    "if hbn_fs_region_sets:\n",
    "    unique_region_sets = set(hbn_fs_region_sets.values())\n",
    "    if len(unique_region_sets) == 1:\n",
    "        print(f\"  VERIFIED: All {len(hbn_fs_region_sets)} subjects have identical 400 regions\")\n",
    "    else:\n",
    "        print(f\"  WARNING: Found {len(unique_region_sets)} different region sets!\")\n",
    "        reference_set = list(unique_region_sets)[0]\n",
    "        for subject_id, regions in hbn_fs_region_sets.items():\n",
    "            if regions != reference_set:\n",
    "                diff = set(reference_set) ^ set(regions)\n",
    "                print(f\"    Subject {subject_id} differs by: {diff}\")\n",
    "                break\n",
    "\n",
    "print(f\"  Found {len(hbn_fs_data)} FreeSurfer subjects with exactly 800 features\")\n",
    "\n",
    "hbn_fs_feature_info = []\n",
    "if hbn_fs_region_info:\n",
    "    for i in range(400):\n",
    "        hbn_fs_feature_info.append({\n",
    "            'feature_name': hbn_fs_region_info['region_names'][i],\n",
    "            'feature_type': 'Surface_Area',\n",
    "            'hemisphere': hbn_fs_region_info['hemisphere'][i]\n",
    "        })\n",
    "    for i in range(400):\n",
    "        hbn_fs_feature_info.append({\n",
    "            'feature_name': hbn_fs_region_info['region_names'][i],\n",
    "            'feature_type': 'Thickness',\n",
    "            'hemisphere': hbn_fs_region_info['hemisphere'][i]\n",
    "        })\n",
    "\n",
    "# Load HBN aparc parcellation (68 regions)\n",
    "print(\"\\nLoading HBN FreeSurfer aparc features...\")\n",
    "HBN_APARC_PARCELLATION = \"aparc\"\n",
    "hbn_aparc_fs_data = {}\n",
    "hbn_aparc_region_info = {}\n",
    "\n",
    "for subject_id in hbn_id_sex_dict.keys():\n",
    "    subject_dir = os.path.join(HBN_FREESURFER_DIR, f\"sub-{subject_id}\")\n",
    "    stats_file = os.path.join(subject_dir, f\"sub-{subject_id}_regionsurfacestats.tsv\")\n",
    "    if os.path.exists(stats_file):\n",
    "        try:\n",
    "            df = pd.read_csv(stats_file, sep='\\t')\n",
    "            filtered_df = df[df[\"atlas\"] == HBN_APARC_PARCELLATION]\n",
    "            if not filtered_df.empty:\n",
    "                filtered_df = filtered_df.sort_values(\"StructName\")\n",
    "                if \"SurfArea\" in filtered_df.columns and \"ThickAvg\" in filtered_df.columns:\n",
    "                    surf_area = filtered_df[\"SurfArea\"].values\n",
    "                    thick_avg = filtered_df[\"ThickAvg\"].values\n",
    "                    combined = np.concatenate([surf_area, thick_avg])\n",
    "                    if not np.any(np.isnan(combined)) and len(surf_area) == 68 and len(thick_avg) == 68:\n",
    "                        hbn_aparc_fs_data[subject_id] = combined\n",
    "                        if not hbn_aparc_region_info:\n",
    "                            hbn_aparc_region_info = {\n",
    "                                'region_names': filtered_df['StructName'].values.tolist(),\n",
    "                                'hemisphere': filtered_df['hemisphere'].values.tolist(),\n",
    "                            }\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(f\"  Found {len(hbn_aparc_fs_data)} FreeSurfer aparc subjects with 136 features (68 SA + 68 Th)\")\n",
    "\n",
    "hbn_aparc_fs_feature_info = []\n",
    "if hbn_aparc_region_info:\n",
    "    for i in range(68):\n",
    "        hbn_aparc_fs_feature_info.append({\n",
    "            'feature_name': hbn_aparc_region_info['region_names'][i],\n",
    "            'feature_type': 'Surface_Area',\n",
    "            'hemisphere': hbn_aparc_region_info['hemisphere'][i]\n",
    "        })\n",
    "    for i in range(68):\n",
    "        hbn_aparc_fs_feature_info.append({\n",
    "            'feature_name': hbn_aparc_region_info['region_names'][i],\n",
    "            'feature_type': 'Thickness',\n",
    "            'hemisphere': hbn_aparc_region_info['hemisphere'][i]\n",
    "        })\n",
    "\n",
    "hbn_common_subjects = sorted(list(set(hbn_cat12_data.keys()) & set(hbn_brainiac_data.keys()) & set(hbn_fs_data.keys()) & set(hbn_aparc_fs_data.keys())))\n",
    "print(f\"  Common subjects: {len(hbn_common_subjects)}\")\n",
    "\n",
    "hbn_cat12_paths = [hbn_cat12_data[s] for s in hbn_common_subjects]\n",
    "hbn_brainiac_paths = [hbn_brainiac_data[s] for s in hbn_common_subjects]\n",
    "hbn_fs_features = np.array([hbn_fs_data[s] for s in hbn_common_subjects])\n",
    "hbn_sex_labels = np.array([hbn_id_sex_dict[s] for s in hbn_common_subjects])\n",
    "hbn_age_labels = np.array([hbn_id_age_dict[s] for s in hbn_common_subjects])\n",
    "\n",
    "hbn_anatcl_features, hbn_brainiac_features, hbn_cnn_features = extract_features(\n",
    "    hbn_cat12_paths, hbn_brainiac_paths, hbn_age_labels, device)\n",
    "\n",
    "hbn_features_dict = {\n",
    "    'AnatCL': hbn_anatcl_features,\n",
    "    'BrainIAC': hbn_brainiac_features,\n",
    "    'CNN': hbn_cnn_features,\n",
    "    'FreeSurfer': hbn_fs_features\n",
    "}\n",
    "\n",
    "# Create aparc features dict\n",
    "hbn_aparc_fs_features = np.array([hbn_aparc_fs_data[s] for s in hbn_common_subjects])\n",
    "hbn_aparc_features_dict = {\n",
    "    'AnatCL': hbn_anatcl_features,\n",
    "    'BrainIAC': hbn_brainiac_features,\n",
    "    'CNN': hbn_cnn_features,\n",
    "    'FreeSurfer': hbn_aparc_fs_features\n",
    "}\n",
    "\n",
    "print(\"\\nComputing HBN Schaefer correlation matrix...\")\n",
    "hbn_corr_matrix, hbn_boundaries, hbn_model_names, hbn_reorder_indices, hbn_cluster_df = \\\n",
    "    compute_correlation_with_cluster_labels(hbn_features_dict, hbn_fs_feature_info, \"HBN_Schaefer\")\n",
    "\n",
    "print(\"\\nComputing HBN aparc correlation matrix...\")\n",
    "hbn_aparc_corr_matrix, hbn_aparc_boundaries, hbn_aparc_model_names, hbn_aparc_reorder_indices, hbn_aparc_cluster_df = \\\n",
    "    compute_correlation_with_cluster_labels(hbn_aparc_features_dict, hbn_aparc_fs_feature_info, \"HBN_aparc\")\n",
    "\n",
    "print(\"\\nHBN Sex Classification\")\n",
    "hbn_sex_results = run_classification(hbn_features_dict, hbn_sex_labels, task_name=\"HBN Sex\")\n",
    "\n",
    "hbn_male_indices = np.where(hbn_sex_labels == 0)[0]\n",
    "hbn_male_features_dict = {k: v[hbn_male_indices] for k, v in hbn_features_dict.items()}\n",
    "hbn_male_age_labels = hbn_age_labels[hbn_male_indices]\n",
    "print(f\"\\nHBN Male subjects: {len(hbn_male_indices)}\")\n",
    "hbn_male_age_results = run_regression(hbn_male_features_dict, hbn_male_age_labels, task_name=\"HBN Male Age\")\n",
    "\n",
    "hbn_female_indices = np.where(hbn_sex_labels == 1)[0]\n",
    "hbn_female_features_dict = {k: v[hbn_female_indices] for k, v in hbn_features_dict.items()}\n",
    "hbn_female_age_labels = hbn_age_labels[hbn_female_indices]\n",
    "print(f\"\\nHBN Female subjects: {len(hbn_female_indices)}\")\n",
    "hbn_female_age_results = run_regression(hbn_female_features_dict, hbn_female_age_labels, task_name=\"HBN Female Age\")\n",
    "\n",
    "# Run HBN aparc experiments\n",
    "print(\"\\n\\nHBN APARC EXPERIMENTS\")\n",
    "print(\"\\nHBN aparc Sex Classification\")\n",
    "hbn_aparc_sex_results = run_classification(hbn_aparc_features_dict, hbn_sex_labels, task_name=\"HBN aparc Sex\")\n",
    "\n",
    "hbn_aparc_male_features_dict = {k: v[hbn_male_indices] for k, v in hbn_aparc_features_dict.items()}\n",
    "print(f\"\\nHBN aparc Male subjects: {len(hbn_male_indices)}\")\n",
    "hbn_aparc_male_age_results = run_regression(hbn_aparc_male_features_dict, hbn_male_age_labels, task_name=\"HBN aparc Male Age\")\n",
    "\n",
    "hbn_aparc_female_features_dict = {k: v[hbn_female_indices] for k, v in hbn_aparc_features_dict.items()}\n",
    "print(f\"\\nHBN aparc Female subjects: {len(hbn_female_indices)}\")\n",
    "hbn_aparc_female_age_results = run_regression(hbn_aparc_female_features_dict, hbn_female_age_labels, task_name=\"HBN aparc Female Age\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nGENERATING COMBINED FIGURE\")\n",
    "models = ['AnatCL', 'BrainIAC', 'CNN', 'FreeSurfer']\n",
    "model_styles = {\n",
    "    'AnatCL': {'color': '#9B59B6', 'marker': '^', 'linestyle': '-', 'label_prefix': 'FM: '},\n",
    "    'BrainIAC': {'color': '#E74C3C', 'marker': '^', 'linestyle': '-', 'label_prefix': 'FM: '},\n",
    "    'CNN': {'color': '#3498DB', 'marker': 'o', 'linestyle': '--', 'label_prefix': ''},\n",
    "    'FreeSurfer': {'color': '#E67E22', 'marker': 's', 'linestyle': '-', 'label_prefix': ''}\n",
    "}\n",
    "alphas = [0.01, 0.05, 0.1, 0.15, 0.25, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "ppmi_n_cv_sex = len(ppmi_sex_labels) - int(0.1 * len(ppmi_sex_labels))\n",
    "ppmi_n_cv_parkinson = len(ppmi_parkinson_labels) - int(0.1 * len(ppmi_parkinson_labels))\n",
    "ppmi_n_cv_male = len(ppmi_male_age_labels) - int(0.1 * len(ppmi_male_age_labels))\n",
    "ppmi_n_cv_female = len(ppmi_female_age_labels) - int(0.1 * len(ppmi_female_age_labels))\n",
    "\n",
    "hbn_n_cv_sex = len(hbn_sex_labels) - int(0.1 * len(hbn_sex_labels))\n",
    "hbn_n_cv_male = len(hbn_male_age_labels) - int(0.1 * len(hbn_male_age_labels))\n",
    "hbn_n_cv_female = len(hbn_female_age_labels) - int(0.1 * len(hbn_female_age_labels))\n",
    "\n",
    "ppmi_sizes_sex = [int(a * ppmi_n_cv_sex * 0.8) for a in alphas]\n",
    "ppmi_sizes_parkinson = [int(a * ppmi_n_cv_parkinson * 0.8) for a in alphas]\n",
    "ppmi_sizes_male = [int(a * ppmi_n_cv_male * 0.8) for a in alphas]\n",
    "ppmi_sizes_female = [int(a * ppmi_n_cv_female * 0.8) for a in alphas]\n",
    "\n",
    "hbn_sizes_sex = [int(a * hbn_n_cv_sex * 0.8) for a in alphas]\n",
    "hbn_sizes_male = [int(a * hbn_n_cv_male * 0.8) for a in alphas]\n",
    "hbn_sizes_female = [int(a * hbn_n_cv_female * 0.8) for a in alphas]\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(20, 24))\n",
    "\n",
    "fig.text(0.28, 0.95, 'PPMI', ha='center', fontsize=18, weight='bold')\n",
    "fig.text(0.73, 0.95, 'HBN', ha='center', fontsize=18, weight='bold')\n",
    "\n",
    "def plot_classification(ax, results, training_sizes, ylabel=None, show_legend=False):\n",
    "    for model in models:\n",
    "        style = model_styles[model]\n",
    "        acc_means, acc_stds, valid_sizes = [], [], []\n",
    "        for i, alpha in enumerate(alphas):\n",
    "            if alpha in results[model]['cv_results']['acc']:\n",
    "                acc_means.append(results[model]['cv_results']['acc'][alpha])\n",
    "                acc_stds.append(results[model]['cv_results']['acc_std'][alpha])\n",
    "                valid_sizes.append(training_sizes[i])\n",
    "        acc_means, acc_stds, valid_sizes = np.array(acc_means), np.array(acc_stds), np.array(valid_sizes)\n",
    "        ax.plot(valid_sizes, acc_means, marker=style['marker'], linestyle=style['linestyle'],\n",
    "                linewidth=3, markersize=10, label=style['label_prefix'] + model, color=style['color'])\n",
    "        ax.fill_between(valid_sizes, acc_means - acc_stds, acc_means + acc_stds, alpha=0.2, color=style['color'])\n",
    "    \n",
    "    dummy_acc = results['DummyClassifier']['cv_acc_mean']\n",
    "    ax.axhline(y=dummy_acc, color='gray', linestyle=':', linewidth=2, label=f'Baseline ({dummy_acc:.3f})', alpha=0.7)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel, fontsize=12, weight='bold')\n",
    "    ax.set_xlabel('Number of Training Subjects', fontsize=12, weight='bold')\n",
    "    if show_legend:\n",
    "        ax.legend(fontsize=9, loc='lower right')\n",
    "\n",
    "def plot_regression(ax, results, training_sizes, ylabel=None, show_legend=False, ylim=None):\n",
    "    for model in models:\n",
    "        style = model_styles[model]\n",
    "        mae_means, mae_stds, valid_sizes = [], [], []\n",
    "        for i, alpha in enumerate(alphas):\n",
    "            if alpha in results[model]['cv_results']['mae']:\n",
    "                mae_means.append(results[model]['cv_results']['mae'][alpha])\n",
    "                mae_stds.append(results[model]['cv_results']['mae_std'][alpha])\n",
    "                valid_sizes.append(training_sizes[i])\n",
    "        mae_means, mae_stds, valid_sizes = np.array(mae_means), np.array(mae_stds), np.array(valid_sizes)\n",
    "        ax.plot(valid_sizes, mae_means, marker=style['marker'], linestyle=style['linestyle'],\n",
    "                linewidth=3, markersize=10, label=style['label_prefix'] + model, color=style['color'])\n",
    "        ax.fill_between(valid_sizes, mae_means - mae_stds, mae_means + mae_stds, alpha=0.2, color=style['color'])\n",
    "    \n",
    "    dummy_mae = results['DummyRegressor']['cv_mae_mean']\n",
    "    ax.axhline(y=dummy_mae, color='gray', linestyle=':', linewidth=2, label=f'Baseline ({dummy_mae:.2f})', alpha=0.7)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if ylim:\n",
    "        ax.set_ylim(ylim)\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel, fontsize=12, weight='bold')\n",
    "    ax.set_xlabel('Number of Training Subjects', fontsize=12, weight='bold')\n",
    "    if show_legend:\n",
    "        ax.legend(fontsize=9, loc='upper right')\n",
    "\n",
    "plot_classification(axes[0, 0], ppmi_sex_results, ppmi_sizes_sex, ylabel='Balanced Accuracy', show_legend=True)\n",
    "plot_classification(axes[0, 1], hbn_sex_results, hbn_sizes_sex, show_legend=True)\n",
    "axes[0, 0].text(-0.15, 0.5, 'Sex Classification', transform=axes[0, 0].transAxes, fontsize=14, weight='bold', \n",
    "                va='center', ha='center', rotation=90)\n",
    "\n",
    "# Compute shared y-axis limits for all MAE plots\n",
    "all_mae_values = []\n",
    "for results in [ppmi_male_age_results, ppmi_female_age_results, hbn_male_age_results, hbn_female_age_results]:\n",
    "    all_mae_values.append(results['DummyRegressor']['cv_mae_mean'])\n",
    "    for model in models:\n",
    "        for alpha in alphas:\n",
    "            if alpha in results[model]['cv_results']['mae']:\n",
    "                mae = results[model]['cv_results']['mae'][alpha]\n",
    "                mae_std = results[model]['cv_results']['mae_std'][alpha]\n",
    "                all_mae_values.extend([mae - mae_std, mae + mae_std])\n",
    "mae_ylim = (0, max(all_mae_values) * 1.05)\n",
    "\n",
    "plot_regression(axes[1, 0], ppmi_male_age_results, ppmi_sizes_male, ylabel='MAE (years)', show_legend=True, ylim=mae_ylim)\n",
    "plot_regression(axes[1, 1], hbn_male_age_results, hbn_sizes_male, ylim=mae_ylim, show_legend=True)\n",
    "axes[1, 0].text(-0.15, 0.5, 'Male Age Prediction', transform=axes[1, 0].transAxes, fontsize=14, weight='bold',\n",
    "                va='center', ha='center', rotation=90)\n",
    "\n",
    "plot_regression(axes[2, 0], ppmi_female_age_results, ppmi_sizes_female, ylabel='MAE (years)', ylim=mae_ylim, show_legend=True)\n",
    "plot_regression(axes[2, 1], hbn_female_age_results, hbn_sizes_female, ylim=mae_ylim, show_legend=True)\n",
    "axes[2, 0].text(-0.15, 0.5, 'Female Age Prediction', transform=axes[2, 0].transAxes, fontsize=14, weight='bold',\n",
    "                va='center', ha='center', rotation=90)\n",
    "\n",
    "plot_classification(axes[3, 0], ppmi_parkinson_results, ppmi_sizes_parkinson, ylabel='Balanced Accuracy', show_legend=True)\n",
    "axes[3, 1].set_xticks([])\n",
    "axes[3, 1].set_yticks([])\n",
    "for spine in axes[3, 1].spines.values():\n",
    "    spine.set_visible(False)\n",
    "axes[3, 0].text(-0.15, 0.5, 'Parkinson Classification', transform=axes[3, 0].transAxes, fontsize=14, weight='bold',\n",
    "                va='center', ha='center', rotation=90)\n",
    "\n",
    "plt.tight_layout(rect=[0.05, 0, 1, 0.94])\n",
    "plt.savefig('combined_learning_curves.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: combined_learning_curves.pdf\")\n",
    "\n",
    "# Generate HBN aparc learning curves (separate PDF)\n",
    "print(\"\\nGenerating HBN aparc learning curves...\")\n",
    "fig_aparc, axes_aparc = plt.subplots(3, 1, figsize=(10, 18))\n",
    "\n",
    "fig_aparc.suptitle('HBN aparc Parcellation (68 regions)', fontsize=16, weight='bold')\n",
    "\n",
    "plot_classification(axes_aparc[0], hbn_aparc_sex_results, hbn_sizes_sex, ylabel='Balanced Accuracy', show_legend=True)\n",
    "axes_aparc[0].set_title('Sex Classification', fontsize=14, weight='bold')\n",
    "\n",
    "# Compute shared y-axis limits for aparc MAE plots\n",
    "aparc_mae_values = []\n",
    "for results in [hbn_aparc_male_age_results, hbn_aparc_female_age_results]:\n",
    "    aparc_mae_values.append(results['DummyRegressor']['cv_mae_mean'])\n",
    "    for model in models:\n",
    "        for alpha in alphas:\n",
    "            if alpha in results[model]['cv_results']['mae']:\n",
    "                mae = results[model]['cv_results']['mae'][alpha]\n",
    "                mae_std = results[model]['cv_results']['mae_std'][alpha]\n",
    "                aparc_mae_values.extend([mae - mae_std, mae + mae_std])\n",
    "aparc_mae_ylim = (0, max(aparc_mae_values) * 1.05)\n",
    "\n",
    "plot_regression(axes_aparc[1], hbn_aparc_male_age_results, hbn_sizes_male, ylabel='MAE (years)', show_legend=True, ylim=aparc_mae_ylim)\n",
    "axes_aparc[1].set_title('Male Age Prediction', fontsize=14, weight='bold')\n",
    "\n",
    "plot_regression(axes_aparc[2], hbn_aparc_female_age_results, hbn_sizes_female, ylabel='MAE (years)', show_legend=True, ylim=aparc_mae_ylim)\n",
    "axes_aparc[2].set_title('Female Age Prediction', fontsize=14, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hbn_aparc_learning_curves.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: hbn_aparc_learning_curves.pdf\")\n",
    "\n",
    "print(\"\\nGenerating correlation matrix plots...\")\n",
    "plot_correlation_with_labels(ppmi_corr_matrix, ppmi_boundaries, ppmi_model_names, ppmi_cluster_df, \"PPMI\")\n",
    "plot_correlation_with_labels(hbn_corr_matrix, hbn_boundaries, hbn_model_names, hbn_cluster_df, \"HBN_Schaefer\")\n",
    "plot_correlation_with_labels(hbn_aparc_corr_matrix, hbn_aparc_boundaries, hbn_aparc_model_names, hbn_aparc_cluster_df, \"HBN_aparc\")\n",
    "\n",
    "ppmi_cluster_df.to_csv('ppmi_freesurfer_features_reordered.csv', index=False)\n",
    "hbn_cluster_df.to_csv('hbn_schaefer_freesurfer_features_reordered.csv', index=False)\n",
    "hbn_aparc_cluster_df.to_csv('hbn_aparc_freesurfer_features_reordered.csv', index=False)\n",
    "print(\"Saved: ppmi_freesurfer_features_reordered.csv\")\n",
    "print(\"Saved: hbn_schaefer_freesurfer_features_reordered.csv\")\n",
    "print(\"Saved: hbn_aparc_freesurfer_features_reordered.csv\")\n",
    "\n",
    "print(f\"\\nDONE - PPMI: {len(ppmi_common_subjects)} subjects, HBN: {len(hbn_common_subjects)} subjects\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
