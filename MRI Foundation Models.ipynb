{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b6b822-71f3-49ce-aeed-22c8b4febb7f",
   "metadata": {},
   "source": [
    "# MRI Foundation Models: Pre-Trained Models' Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653b3cf-1728-4a6d-b2aa-f01ea0a5dadb",
   "metadata": {},
   "source": [
    "## SPM/CAT12 Preprocessing | AnatCL | HBN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c2d56-25ab-4b65-a537-a8f2288012f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/links/projects/rrg-glatard/arelbaha/HBN_BIDS/new_cat12_preprocessing.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import boutiques\n",
    "from boutiques import bosh\n",
    "from boutiques.descriptor2func import function\n",
    "\n",
    "boutiques_descriptor = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/descriptors/cat12_prepro.json\"\n",
    "base_dir = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/HBN_BIDS\"\n",
    "output_dir = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/preprocessed_output\"\n",
    "\n",
    "bosh([\"exec\", \n",
    "      \"prepare\", \n",
    "      boutiques_descriptor, \n",
    "      \"--imagepath\", \n",
    "      \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif\"\n",
    "])\n",
    "\n",
    "cat12 = boutiques.descriptor2func.function(boutiques_descriptor)\n",
    "\n",
    "t1_nii_files = glob.glob(os.path.join(base_dir, \"sub-*\", \"ses-*\", \"anat\", \"sub-*_T1w.nii\"))\n",
    "print(f\"Found {len(t1_nii_files)} T1w files.\")\n",
    "\n",
    "task_id = int(os.environ[\"SLURM_ARRAY_TASK_ID\"])\n",
    "\n",
    "if task_id >= len(t1_nii_files) or task_id < 0:\n",
    "    print(f\"SLURM_ARRAY_TASK_ID={task_id} out of range\")\n",
    "    exit(1)\n",
    "\n",
    "input_file = t1_nii_files[task_id]\n",
    "\n",
    "path_parts = input_file.split(os.sep)\n",
    "subject_id = next((part for part in path_parts if part.startswith(\"sub-\")), None)\n",
    "\n",
    "if subject_id is None:\n",
    "        print(f\"No subject ID in path: {input_file}\")\n",
    "        exit(1)\n",
    "\n",
    "filename = os.path.basename(input_file)\n",
    "\n",
    "subject_output_dir = os.path.join(output_dir, subject_id)\n",
    "os.makedirs(subject_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Processing={task_id} -> file: {filename} (subject: {subject_id})\")\n",
    "\n",
    "result = cat12('--imagepath=/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif', \n",
    "                   input_file=input_file, \n",
    "                   output_dir=subject_output_dir)\n",
    "\n",
    "result_dict = vars(result)\n",
    "print(\"Available result keys:\", result_dict.keys())\n",
    "print(\"\\nExit code:\", result_dict.get(\"exit_code\"))\n",
    "print(\"\\nStdout:\\n\", result_dict.get (\"stdout\" ))\n",
    "print(\"\\nStderr:\\n\", result_dict.get (\"stderr\"))\n",
    "print(\"\\nOutput files:\\n\", result_dict.get(\"output_files\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d9624-6f3c-41b0-9fdc-54d3878e50ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch --array=500-520\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=CAT12_preproc\n",
    "#SBATCH --account=rrg-glatard\n",
    "#SBATCH --mem=12G\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --output=CAT12_preproc_%A_%a.out\n",
    "#SBATCH --error=CAT12_preproc_%A_%a.err\n",
    "#SBATCH --time=1:0:0\n",
    "\n",
    "source ~/.venvs/jupyter_py3/bin/activate\n",
    "\n",
    "module load apptainer\n",
    "\n",
    "cd ~/links/projects/rrg-glatard/arelbaha/HBN_BIDS #Location of preprocessing data and script\n",
    "\n",
    "echo \"Running task ID: $SLURM_ARRAY_TASK_ID\"\n",
    "\n",
    "python new_cat12_preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56370de7-6f6c-47d8-bf17-ad17b7dbfabb",
   "metadata": {},
   "source": [
    "## SPM/CAT12 Preprocessing | AnatCL | PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dcbe07-4e28-43bc-b009-ae1d394712a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/links/projects/def-glatard/arelbaha/data/inputs/ppmicat12_preprocessing.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import boutiques\n",
    "from boutiques import bosh\n",
    "from boutiques.descriptor2func import function\n",
    "\n",
    "#Downloading Container Part + Paths\n",
    "boutiques_descriptor = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/descriptors/cat12_prepro.json\"\n",
    "base_dir = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data/inputs\"\n",
    "output_dir = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/preprocessed_output\"\n",
    "\n",
    "bosh([\"exec\", \"prepare\", boutiques_descriptor, \"--imagepath\", \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif\"])\n",
    "cat12 = boutiques.descriptor2func.function(boutiques_descriptor)\n",
    "\n",
    "#Task ID Extraction\n",
    "data = glob.glob(os.path.join(base_dir, \"sub-*\", \"ses-*\", \"anat\", \"*.nii\"))\n",
    "print(f\"Found {len(data)}\")\n",
    "\n",
    "task_id = int(os.environ[\"SLURM_ARRAY_TASK_ID\"])\n",
    "\n",
    "if task_id >= len(data) or task_id < 0:\n",
    "    print(f\"SLURM_ARRAY_TASK_ID={task_id} out of range\")\n",
    "    exit(1)\n",
    "\n",
    "input_file = data[task_id]\n",
    "\n",
    "path_parts = input_file.split(os.sep)\n",
    "subject_id = next((part for part in path_parts if part.startswith(\"sub-\")), None)\n",
    "\n",
    "if subject_id is None:\n",
    "        print(f\"Could not find subject ID in path: {input_file}\")\n",
    "        exit(1)\n",
    "\n",
    "filename = os.path.basename(input_file)\n",
    "\n",
    "subject_output_dir = os.path.join(output_dir, subject_id)\n",
    "os.makedirs(subject_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Processing SLURM_ARRAY_TASK_ID ={task_id} -> file: {filename} (subject: {subject_id})\")\n",
    "\n",
    "result = cat12('--imagepath=/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif', \n",
    "                   input_file=input_file, \n",
    "                   output_dir=subject_output_dir)\n",
    "\n",
    "result_dict = vars(result)\n",
    "print(\"Available result keys:\", result_dict.keys())\n",
    "print(\"\\nExit code:\", result_dict.get(\"exit_code\"))\n",
    "print(\"\\nStdout:\\n\", result_dict.get (\"stdout\" ))\n",
    "print(\"\\nStderr:\\n\", result_dict.get (\"stderr\"))\n",
    "print(\"\\nOutput files:\\n\", result_dict.get(\"output_files\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472cc4a-c309-45a3-bc27-2ff39eed37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch --array=1040-1040\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=CAT12_parkinson_preproc\n",
    "#SBATCH --account=rrg-glatard\n",
    "#SBATCH --mem=12G\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --output=parkinson_preproc_%A_%a.out\n",
    "#SBATCH --error=parkinson_preproc_%A_%a.err\n",
    "#SBATCH --time=1:0:0\n",
    "\n",
    "source ~/.venvs/jupyter_py3/bin/activate\n",
    "\n",
    "module load apptainer\n",
    "\n",
    "cd ~/links/projects/def-glatard/arelbaha/data/inputs #Location of preprocessing data and script\n",
    "\n",
    "echo \"Running task ID: $SLURM_ARRAY_TASK_ID\"\n",
    "\n",
    "python classification_parkinson.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015677a-ccfc-42e3-8a47-ee62150d4954",
   "metadata": {},
   "source": [
    "### HD-BET Preprocessing | BrainIAC, CNN, SwinBrain | PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c39d8-15dd-469b-913a-c243854f1583",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch --array=23,24\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-glatard\n",
    "#SBATCH --time=20:00:00\n",
    "#SBATCH --mem=16G\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --job-name=brainiac_batch\n",
    "#SBATCH --output=logs/prep_batch_%a.out\n",
    "#SBATCH --error=logs/prep_batch_%a.err\n",
    "\n",
    "export BASE_DIR=\"/home/arelbaha/links/projects/def-glatard/arelbaha/data\"\n",
    "export RAW_DIR=\"${BASE_DIR}/raw_files_brainiac\"\n",
    "export OUTPUT_DIR=\"${BASE_DIR}/processed_outputs\"\n",
    "\n",
    "# Load environment\n",
    "module load python/3.11\n",
    "module load opencv\n",
    "source /home/arelbaha/.venvs/brainiac_env/bin/activate\n",
    "\n",
    "BATCH_DIR=\"${RAW_DIR}/batch_${SLURM_ARRAY_TASK_ID}\"\n",
    "BATCH_OUTPUT=\"${OUTPUT_DIR}/batch_${SLURM_ARRAY_TASK_ID}\"\n",
    "\n",
    "mkdir -p \"$BATCH_OUTPUT\"\n",
    "\n",
    "echo \"Processing batch ${SLURM_ARRAY_TASK_ID} from ${BATCH_DIR}\"\n",
    "\n",
    "python /home/arelbaha/.venvs/brainiac_env/lib/python3.11/site-packages/BrainIAC/src/preprocessing/mri_preprocess_3d_simple.py \\\n",
    "    --temp_img /home/arelbaha/.venvs/brainiac_env/lib/python3.11/site-packages/BrainIAC/src/preprocessing/atlases/temp_head.nii.gz \\\n",
    "    --input_dir \"$BATCH_DIR\" \\\n",
    "    --output_dir \"$BATCH_OUTPUT\"\n",
    "\n",
    "echo \"Completed batch ${SLURM_ARRAY_TASK_ID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc4053-2a9c-4966-887a-aea3c11b52f1",
   "metadata": {},
   "source": [
    "## HD-BET Preprocessing | BrainIAC, CNN, SwinBrain | HBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfebf287-f626-4640-967a-b16c3dc1333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch --array=1-100\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-glatard\n",
    "#SBATCH --time=12:00:00\n",
    "#SBATCH --mem=16G\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --job-name=brainiac_batch\n",
    "#SBATCH --output=logs/prep_batch_%a.out\n",
    "#SBATCH --error=logs/prep_batch_%a.err\n",
    "\n",
    "export BASE_DIR=\"/home/arelbaha/links/projects/rrg-glatard/arelbaha\"\n",
    "export RAW_DIR=\"${BASE_DIR}/brainiac_p_files\"\n",
    "export OUTPUT_DIR=\"${BASE_DIR}/brainiac_p_outputs\"\n",
    "\n",
    "module load python/3.11\n",
    "module load opencv\n",
    "source /home/arelbaha/.venvs/brainiac_env/bin/activate\n",
    "\n",
    "#Batch\n",
    "BATCH_NUM=$(printf \"%03d\" ${SLURM_ARRAY_TASK_ID})\n",
    "BATCH_DIR=\"${RAW_DIR}/batch_${BATCH_NUM}\"\n",
    "BATCH_OUTPUT=\"${OUTPUT_DIR}/batch_${BATCH_NUM}\"\n",
    "\n",
    "mkdir -p \"$BATCH_OUTPUT\"\n",
    "echo \"Processing batch ${BATCH_NUM} from ${BATCH_DIR}\"\n",
    "\n",
    "#Preprocessing\n",
    "python /home/arelbaha/.venvs/brainiac_env/lib/python3.11/site-packages/BrainIAC/src/preprocessing/mri_preprocess_3d_simple.py \\\n",
    "    --temp_img /home/arelbaha/.venvs/brainiac_env/lib/python3.11/site-packages/BrainIAC/src/preprocessing/atlases/temp_head.nii.gz \\\n",
    "    --input_dir \"$BATCH_DIR\" \\\n",
    "    --output_dir \"$BATCH_OUTPUT\"\n",
    "\n",
    "echo \"Completed batch ${BATCH_NUM}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb41b4-95b0-4a46-9c0b-74079bdb4e0b",
   "metadata": {},
   "source": [
    "## Multi-Task Evaluation of Models | HBN and PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e6939-08ed-4fcb-91fb-b3a7f506fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/arelbaha/links/projects/rrg-glatard/arelbaha/new_combined_multimodel_comparison_w_permutation.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import nibabel as nib\n",
    "from anatcl import AnatCL\n",
    "from monai.networks.nets import ViT, SwinUNETR\n",
    "import torch.nn.functional as F\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import permutation_test\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.ndimage as ndi\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "SEEDS = [0, 1, 2, 3, 42]\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.set_num_threads(1)\n",
    "torch.set_num_interop_threads(1)\n",
    "\n",
    "device = \"cpu\"\n",
    "DROPOUT_RATE = 0.3\n",
    "ANATCL_ENCODER_PATH = \"/home/arelbaha/.venvs/jupyter_py3/bin\"\n",
    "BRAINIAC_CKPT = \"/home/arelbaha/.venvs/jupyter_py3/bin/BrainIAC.ckpt\"\n",
    "SWINBRAIN_CKPT = \"/home/arelbaha/.venvs/jupyter_py3/bin/Brain_Swin_UNETR.pth\"\n",
    "\n",
    "class CAT12VBMDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform):\n",
    "        self.data, self.labels, self.transform = data, labels, transform\n",
    "        self.target_shape = (121, 128, 121)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.data[idx]).get_fdata().astype(np.float32)\n",
    "        if img.shape != self.target_shape:\n",
    "            img = F.interpolate(torch.from_numpy(img[None, None]), size=self.target_shape,\n",
    "                               mode='trilinear', align_corners=False).squeeze().numpy()\n",
    "        img = self.transform(img).unsqueeze(0)\n",
    "        return img, torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "class BrainIACDataset(Dataset):\n",
    "    def __init__(self, paths, labels, transform):\n",
    "        self.paths, self.labels, self.transform = paths, labels, transform\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.paths[idx]).get_fdata().astype(np.float32)\n",
    "        if img.shape != (96, 96, 96):\n",
    "            img = F.interpolate(torch.from_numpy(img[None, None]), size=(96, 96, 96), \n",
    "                              mode='trilinear', align_corners=False).squeeze().numpy()\n",
    "        img_tensor = torch.from_numpy(img)\n",
    "        img_normalized = self.transform(img_tensor)\n",
    "        return img_normalized[None], torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "class SwinBrainDataset(Dataset):\n",
    "    def __init__(self, paths, labels):\n",
    "        self.paths, self.labels = paths, labels\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.paths[idx]).get_fdata().astype(np.float32)\n",
    "        img = F.interpolate(torch.from_numpy(img[None, None]), size=(128, 128, 64),\n",
    "                           mode='trilinear', align_corners=False).squeeze(0)\n",
    "        img = (img - img.mean()) / (img.std() + 1e-6)\n",
    "        img = img.repeat(3, 1, 1, 1)\n",
    "        return img, torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "class CNNDataset(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.paths[idx]).get_fdata().astype(np.float32)\n",
    "        if img.shape != (92, 110, 92):\n",
    "            img = ndi.zoom(img, [92/img.shape[0], 110/img.shape[1], 92/img.shape[2]], order=1)\n",
    "        img = (img - img.mean()) / (img.std() + 1e-6)\n",
    "        return torch.from_numpy(img[None]).float()\n",
    "\n",
    "class CNN3D(nn.Module):\n",
    "    def __init__(self, dropout_rate=DROPOUT_RATE):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 64, 3, padding=1)\n",
    "        self.bn1, self.pool1, self.drop1 = nn.BatchNorm3d(64), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv2 = nn.Conv3d(64, 64, 3, padding=1)\n",
    "        self.bn2, self.pool2, self.drop2 = nn.BatchNorm3d(64), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv3 = nn.Conv3d(64, 128, 3, padding=1)\n",
    "        self.bn3, self.pool3, self.drop3 = nn.BatchNorm3d(128), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv4 = nn.Conv3d(128, 256, 3, padding=1)\n",
    "        self.bn4, self.pool4, self.drop4 = nn.BatchNorm3d(256), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc1, self.drop5 = nn.Linear(256, 512), nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop1(self.bn1(self.pool1(F.relu(self.conv1(x)))))\n",
    "        x = self.drop2(self.bn2(self.pool2(F.relu(self.conv2(x)))))\n",
    "        x = self.drop3(self.bn3(self.pool3(F.relu(self.conv3(x)))))\n",
    "        x = self.drop4(self.bn4(self.pool4(F.relu(self.conv4(x)))))\n",
    "        return self.drop5(F.relu(self.fc1(self.global_pool(x).view(x.size(0), -1))))\n",
    "\n",
    "def load_brainiac_vit(ckpt_path, device):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    vit = ViT(in_channels=1, img_size=(96, 96, 96), patch_size=(16, 16, 16),\n",
    "              hidden_size=768, mlp_dim=3072, num_layers=12, num_heads=12, save_attn=True)\n",
    "    pe_key = \"backbone.patch_embedding.position_embeddings\"\n",
    "    if pe_key in ckpt.get(\"state_dict\", ckpt):\n",
    "        vit.patch_embedding.position_embeddings = nn.Parameter(ckpt[\"state_dict\"][pe_key].clone())\n",
    "    backbone_state = {k.replace(\"backbone.\", \"\"): v for k, v in ckpt.get(\"state_dict\", ckpt).items() \n",
    "                     if k.startswith(\"backbone.\")}\n",
    "    vit.load_state_dict(backbone_state, strict=False)\n",
    "    return vit.to(device).eval()\n",
    "\n",
    "def load_swinbrain(ckpt_path, device):\n",
    "    model = SwinUNETR(img_size=(128, 128, 64), in_channels=3, out_channels=3, spatial_dims=3,\n",
    "                      feature_size=24, drop_rate=0.0, attn_drop_rate=0.0)\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    clean_state = {k.replace(\"module.\", \"\"): v for k, v in ckpt.items()}\n",
    "    model.load_state_dict(clean_state, strict=True)\n",
    "    return model.to(device).eval()\n",
    "\n",
    "def run_classification(X_dict, y, test_size=0.1, task_name=\"Classification\"):\n",
    "    print(f\"\\n{task_name}\")\n",
    "    results = {}\n",
    "    alphas = [0.01, 0.05, 0.1, 0.15, 0.25, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "    dummy_cv_acc, dummy_cv_auc, dummy_test_acc, dummy_test_auc = [], [], [], []\n",
    "    for seed in SEEDS:\n",
    "        cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)), \n",
    "                                            stratify=y, random_state=seed)\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        for train_rel, val_rel in skf.split(cv_idx, y[cv_idx]):\n",
    "            train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "            dummy = DummyClassifier(strategy='stratified', random_state=seed)\n",
    "            dummy.fit(np.zeros((len(train_idx), 1)), y[train_idx])\n",
    "            dummy_cv_acc.append(balanced_accuracy_score(y[val_idx], dummy.predict(np.zeros((len(val_idx), 1)))))\n",
    "            dummy_cv_auc.append(roc_auc_score(y[val_idx], dummy.predict_proba(np.zeros((len(val_idx), 1)))[:, 1]))\n",
    "        \n",
    "        dummy = DummyClassifier(strategy='stratified', random_state=seed)\n",
    "        dummy.fit(np.zeros((len(cv_idx), 1)), y[cv_idx])\n",
    "        dummy_test_acc.append(balanced_accuracy_score(y[test_idx], dummy.predict(np.zeros((len(test_idx), 1)))))\n",
    "        dummy_test_auc.append(roc_auc_score(y[test_idx], dummy.predict_proba(np.zeros((len(test_idx), 1)))[:, 1]))\n",
    "    \n",
    "    results['DummyClassifier'] = {\n",
    "        'best_alpha': 1.0,\n",
    "        'test_acc_mean': np.mean(dummy_test_acc), 'test_acc_std': np.std(dummy_test_acc),\n",
    "        'test_auc_mean': np.mean(dummy_test_auc), 'test_auc_std': np.std(dummy_test_auc),\n",
    "        'cv_auc_mean': np.mean(dummy_cv_auc), 'cv_auc_std': np.std(dummy_cv_auc),\n",
    "        'cv_acc_mean': np.mean(dummy_cv_acc), 'cv_acc_std': np.std(dummy_cv_acc),\n",
    "        'fold_results': {'val_auc': dummy_cv_auc, 'val_acc': dummy_cv_acc}\n",
    "    }\n",
    "    print(f\"Dummy: AUC={np.mean(dummy_test_auc):.3f}±{np.std(dummy_test_auc):.3f}\")\n",
    "\n",
    "    for model, X in X_dict.items():\n",
    "        all_val_auc = {a: [] for a in alphas}\n",
    "        all_val_acc = {a: [] for a in alphas}\n",
    "        all_train_auc = {a: [] for a in alphas}\n",
    "        all_train_acc = {a: [] for a in alphas}\n",
    "        all_test_acc, all_test_auc = [], []\n",
    "        \n",
    "        for seed in SEEDS:\n",
    "            cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)), \n",
    "                                                stratify=y, random_state=seed)\n",
    "            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "            \n",
    "            for train_rel, val_rel in skf.split(cv_idx, y[cv_idx]):\n",
    "                train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "                for alpha in alphas:\n",
    "                    n = int(alpha * len(train_idx))\n",
    "                    if n >= len(train_idx):\n",
    "                        tr_idx = train_idx\n",
    "                    else:\n",
    "                        tr_idx, _ = train_test_split(train_idx, train_size=n, stratify=y[train_idx], random_state=seed)\n",
    "                    if len(np.unique(y[tr_idx])) < 2:\n",
    "                        continue\n",
    "\n",
    "                    scaler = StandardScaler()\n",
    "                    rf = RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_split=5,\n",
    "                                               random_state=seed, n_jobs=1, max_features='sqrt', class_weight='balanced')\n",
    "                    rf.fit(scaler.fit_transform(X[tr_idx]), y[tr_idx])\n",
    "                    val_pred_proba = rf.predict_proba(scaler.transform(X[val_idx]))[:, 1]\n",
    "                    all_val_acc[alpha].append(balanced_accuracy_score(y[val_idx], rf.predict(scaler.transform(X[val_idx]))))\n",
    "                    all_val_auc[alpha].append(roc_auc_score(y[val_idx], val_pred_proba))\n",
    "                    all_train_acc[alpha].append(balanced_accuracy_score(y[tr_idx], rf.predict(scaler.transform(X[tr_idx]))))\n",
    "                    all_train_auc[alpha].append(roc_auc_score(y[tr_idx], rf.predict_proba(scaler.transform(X[tr_idx]))[:, 1]))\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            rf = RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_split=5,\n",
    "                                        random_state=seed, n_jobs=1, max_features='sqrt', class_weight='balanced')\n",
    "            rf.fit(scaler.fit_transform(X[cv_idx]), y[cv_idx])\n",
    "            test_pred_proba = rf.predict_proba(scaler.transform(X[test_idx]))[:, 1]\n",
    "            all_test_acc.append(balanced_accuracy_score(y[test_idx], rf.predict(scaler.transform(X[test_idx]))))\n",
    "            all_test_auc.append(roc_auc_score(y[test_idx], test_pred_proba))\n",
    "\n",
    "        avg_auc = {a: np.mean(all_val_auc[a]) for a in alphas if all_val_auc[a]}\n",
    "        best_alpha = max(avg_auc, key=avg_auc.get)\n",
    "        train_val_gaps = {a: np.mean(all_train_auc[a]) - np.mean(all_val_auc[a]) for a in alphas if all_train_auc[a] and all_val_auc[a]}\n",
    "        \n",
    "        results[model] = {\n",
    "            'best_alpha': best_alpha,\n",
    "            'test_acc_mean': np.mean(all_test_acc), 'test_acc_std': np.std(all_test_acc),\n",
    "            'test_auc_mean': np.mean(all_test_auc), 'test_auc_std': np.std(all_test_auc),\n",
    "            'cv_auc_mean': np.mean(all_val_auc[best_alpha]), 'cv_auc_std': np.std(all_val_auc[best_alpha]),\n",
    "            'cv_acc_mean': np.mean(all_val_acc[best_alpha]), 'cv_acc_std': np.std(all_val_acc[best_alpha]),\n",
    "            'train_val_gap': train_val_gaps.get(best_alpha, 0),\n",
    "            'is_overfitting': train_val_gaps.get(best_alpha, 0) > 0.25,\n",
    "            'fold_results': {'val_auc': all_val_auc[best_alpha], 'val_acc': all_val_acc[best_alpha]},\n",
    "            'fold_results_alpha1': {'val_acc': all_val_acc[1.0], 'val_auc': all_val_auc[1.0]},\n",
    "            'cv_results': {\n",
    "                'alphas': alphas, \n",
    "                'acc': {a: np.mean(all_val_acc[a]) for a in alphas if all_val_acc[a]},\n",
    "                'auc': avg_auc,\n",
    "                'acc_std': {a: np.std(all_val_acc[a]) for a in alphas if all_val_acc[a]},\n",
    "                'auc_std': {a: np.std(all_val_auc[a]) for a in alphas if all_val_auc[a]},\n",
    "                'train_acc': {a: np.mean(all_train_acc[a]) for a in alphas if all_train_acc[a]},\n",
    "                'train_acc_std': {a: np.std(all_train_acc[a]) for a in alphas if all_train_acc[a]},\n",
    "                'train_auc': {a: np.mean(all_train_auc[a]) for a in alphas if all_train_auc[a]},\n",
    "                'train_auc_std': {a: np.std(all_train_auc[a]) for a in alphas if all_train_auc[a]},\n",
    "            }\n",
    "        }\n",
    "        print(f\"  {model}: AUC={np.mean(all_test_auc):.3f}±{np.std(all_test_auc):.3f}, Acc={np.mean(all_test_acc):.3f}±{np.std(all_test_acc):.3f}\")\n",
    "    return results\n",
    "\n",
    "def run_regression(X_dict, y, test_size=0.1, task_name=\"Regression\"):\n",
    "    print(f\"\\n{task_name}\")\n",
    "    results = {}\n",
    "    alphas = [0.01, 0.05, 0.1, 0.15, 0.25, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "    dummy_cv_r2, dummy_cv_mae, dummy_test_r2, dummy_test_mae = [], [], [], []\n",
    "    for seed in SEEDS:\n",
    "        cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)), random_state=seed)\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        for train_rel, val_rel in kf.split(cv_idx):\n",
    "            train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "            dummy = DummyRegressor(strategy='mean')\n",
    "            dummy.fit(np.zeros((len(train_idx), 1)), y[train_idx])\n",
    "            pred = dummy.predict(np.zeros((len(val_idx), 1)))\n",
    "            dummy_cv_r2.append(r2_score(y[val_idx], pred))\n",
    "            dummy_cv_mae.append(mean_absolute_error(y[val_idx], pred))\n",
    "        \n",
    "        dummy = DummyRegressor(strategy='mean')\n",
    "        dummy.fit(np.zeros((len(cv_idx), 1)), y[cv_idx])\n",
    "        test_pred = dummy.predict(np.zeros((len(test_idx), 1)))\n",
    "        dummy_test_r2.append(r2_score(y[test_idx], test_pred))\n",
    "        dummy_test_mae.append(mean_absolute_error(y[test_idx], test_pred))\n",
    "    \n",
    "    results['DummyRegressor'] = {\n",
    "        'best_alpha': 1.0,\n",
    "        'test_r2_mean': np.mean(dummy_test_r2), 'test_r2_std': np.std(dummy_test_r2),\n",
    "        'test_mae_mean': np.mean(dummy_test_mae), 'test_mae_std': np.std(dummy_test_mae),\n",
    "        'cv_r2_mean': np.mean(dummy_cv_r2), 'cv_r2_std': np.std(dummy_cv_r2),\n",
    "        'cv_mae_mean': np.mean(dummy_cv_mae), 'cv_mae_std': np.std(dummy_cv_mae),\n",
    "        'fold_results': {'val_r2': dummy_cv_r2, 'val_mae': dummy_cv_mae}\n",
    "    }\n",
    "    print(f\"  Dummy: MAE={np.mean(dummy_test_mae):.2f}±{np.std(dummy_test_mae):.2f}\")\n",
    "\n",
    "    for model, X in X_dict.items():\n",
    "        all_val_r2 = {a: [] for a in alphas}\n",
    "        all_val_mae = {a: [] for a in alphas}\n",
    "        all_train_r2 = {a: [] for a in alphas}\n",
    "        all_train_mae = {a: [] for a in alphas}\n",
    "        all_test_r2, all_test_mae = [], []\n",
    "\n",
    "        for seed in SEEDS:\n",
    "            cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)), random_state=seed)\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "            \n",
    "            for train_rel, val_rel in kf.split(cv_idx):\n",
    "                train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "                for alpha in alphas:\n",
    "                    n = int(alpha * len(train_idx))\n",
    "                    tr_idx, _ = train_test_split(train_idx, train_size=n, random_state=seed) if n < len(train_idx) else (train_idx, None)\n",
    "                    \n",
    "                    scaler = StandardScaler()\n",
    "                    rf = RandomForestRegressor(n_estimators=200, max_depth=6, min_samples_split=5, random_state=seed, n_jobs=1)\n",
    "                    rf.fit(scaler.fit_transform(X[tr_idx]), y[tr_idx])\n",
    "                    pred = rf.predict(scaler.transform(X[val_idx]))\n",
    "                    train_pred = rf.predict(scaler.transform(X[tr_idx]))\n",
    "                    all_val_r2[alpha].append(r2_score(y[val_idx], pred))\n",
    "                    all_val_mae[alpha].append(mean_absolute_error(y[val_idx], pred))\n",
    "                    all_train_r2[alpha].append(r2_score(y[tr_idx], train_pred))\n",
    "                    all_train_mae[alpha].append(mean_absolute_error(y[tr_idx], train_pred))\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            rf = RandomForestRegressor(n_estimators=200, max_depth=6, min_samples_split=5, random_state=seed, n_jobs=1)\n",
    "            rf.fit(scaler.fit_transform(X[cv_idx]), y[cv_idx])\n",
    "            test_pred = rf.predict(scaler.transform(X[test_idx]))\n",
    "            all_test_r2.append(r2_score(y[test_idx], test_pred))\n",
    "            all_test_mae.append(mean_absolute_error(y[test_idx], test_pred))\n",
    "\n",
    "        avg_r2 = {a: np.mean(all_val_r2[a]) for a in alphas if all_val_r2[a]}\n",
    "        best_alpha = max(avg_r2, key=avg_r2.get)\n",
    "        train_val_gaps = {a: np.mean(all_train_r2[a]) - np.mean(all_val_r2[a]) for a in alphas if all_train_r2[a] and all_val_r2[a]}\n",
    "        \n",
    "        results[model] = {\n",
    "            'best_alpha': best_alpha,\n",
    "            'test_r2_mean': np.mean(all_test_r2), 'test_r2_std': np.std(all_test_r2),\n",
    "            'test_mae_mean': np.mean(all_test_mae), 'test_mae_std': np.std(all_test_mae),\n",
    "            'cv_r2_mean': np.mean(all_val_r2[best_alpha]), 'cv_r2_std': np.std(all_val_r2[best_alpha]),\n",
    "            'cv_mae_mean': np.mean(all_val_mae[best_alpha]), 'cv_mae_std': np.std(all_val_mae[best_alpha]),\n",
    "            'train_val_gap': train_val_gaps.get(best_alpha, 0),\n",
    "            'is_overfitting': train_val_gaps.get(best_alpha, 0) > 0.35,\n",
    "            'fold_results': {'val_r2': all_val_r2[best_alpha], 'val_mae': all_val_mae[best_alpha]},\n",
    "            'fold_results_alpha1': {'val_mae': all_val_mae[1.0], 'val_r2': all_val_r2[1.0]},\n",
    "            'cv_results': {\n",
    "                'alphas': alphas,\n",
    "                'r2': avg_r2,\n",
    "                'mae': {a: np.mean(all_val_mae[a]) for a in alphas if all_val_mae[a]},\n",
    "                'r2_std': {a: np.std(all_val_r2[a]) for a in alphas if all_val_r2[a]},\n",
    "                'mae_std': {a: np.std(all_val_mae[a]) for a in alphas if all_val_mae[a]},\n",
    "                'train_r2': {a: np.mean(all_train_r2[a]) for a in alphas if all_train_r2[a]},\n",
    "                'train_r2_std': {a: np.std(all_train_r2[a]) for a in alphas if all_train_r2[a]},\n",
    "                'train_mae': {a: np.mean(all_train_mae[a]) for a in alphas if all_train_mae[a]},\n",
    "                'train_mae_std': {a: np.std(all_train_mae[a]) for a in alphas if all_train_mae[a]},\n",
    "            }\n",
    "        }\n",
    "        print(f\"  {model}: MAE={np.mean(all_test_mae):.2f}±{np.std(all_test_mae):.2f}, R2={np.mean(all_test_r2):.3f}±{np.std(all_test_r2):.3f}\")\n",
    "    return results\n",
    "\n",
    "def compute_correlation_with_cluster_labels(features_dict, fs_feature_infos, fs_keys, dataset_name):\n",
    "    all_features, boundaries, reordered_indices, model_names_used, cluster_dfs = [], [0], {}, [], {}\n",
    "    model_order = ['AnatCL', 'BrainIAC', 'SwinBrain', 'CNN'] + fs_keys\n",
    "    \n",
    "    for model in model_order:\n",
    "        if model not in features_dict:\n",
    "            continue\n",
    "        feats = StandardScaler().fit_transform(features_dict[model])\n",
    "        valid = ~np.isnan(feats).any(axis=0) & (np.std(feats, axis=0) > 1e-10)\n",
    "        feats = feats[:, valid]\n",
    "        if feats.shape[1] == 0:\n",
    "            continue\n",
    "        print(f\"  {model}: {feats.shape[1]} features\")\n",
    "        \n",
    "        corr = np.corrcoef(feats.T)\n",
    "        corr = np.nan_to_num((corr + corr.T) / 2, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        np.fill_diagonal(corr, 1.0)\n",
    "        dist = np.abs(1 - np.abs(corr))\n",
    "        np.fill_diagonal(dist, 0)\n",
    "        condensed = np.nan_to_num(squareform((dist + dist.T) / 2, checks=False), nan=0.0, posinf=1.0, neginf=0.0)\n",
    "        reorder_idx = dendrogram(linkage(condensed, method='ward'), no_plot=True)['leaves']\n",
    "        all_features.append(feats[:, reorder_idx])\n",
    "        boundaries.append(boundaries[-1] + feats.shape[1])\n",
    "        reordered_indices[model] = reorder_idx\n",
    "        model_names_used.append(model)\n",
    "\n",
    "    combined = np.hstack(all_features)\n",
    "    corr = np.nan_to_num((np.corrcoef(combined.T) + np.corrcoef(combined.T).T) / 2, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "\n",
    "    for fs_key in fs_keys:\n",
    "        if fs_key in reordered_indices:\n",
    "            fs_info = fs_feature_infos[fs_key]\n",
    "            cluster_df = pd.DataFrame([{**fs_info[idx], 'original_index': idx, 'reordered_position': pos}\n",
    "                                       for pos, idx in enumerate(reordered_indices[fs_key])])\n",
    "            cluster_dfs[fs_key] = cluster_df\n",
    "    \n",
    "    return corr, boundaries, model_names_used, reordered_indices, cluster_dfs\n",
    "\n",
    "def extract_features(cat12_paths, brainiac_paths, age_labels, device):\n",
    "    anatcl_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda x: torch.from_numpy(x.copy()).float()),\n",
    "        transforms.Normalize(mean=0.0, std=1.0)\n",
    "    ])\n",
    "    \n",
    "    print(\"Extracting AnatCL...\")\n",
    "    all_fold_features = []\n",
    "    for fold_idx in range(5):\n",
    "        path = os.path.join(ANATCL_ENCODER_PATH, f\"fold{fold_idx}.pth\")\n",
    "        encoder = AnatCL(descriptor=\"global\", fold=fold_idx, pretrained=False).to(device).eval()\n",
    "        encoder.backbone.load_state_dict(torch.load(path, map_location=device, weights_only=False)['model'])\n",
    "        for p in encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        dl = DataLoader(CAT12VBMDataset(cat12_paths, age_labels, anatcl_transform), batch_size=32, num_workers=0)\n",
    "        with torch.no_grad():\n",
    "            fold_features = torch.cat([encoder(vol.to(device)).cpu() for vol, _ in dl]).numpy()\n",
    "        all_fold_features.append(fold_features)\n",
    "        del encoder\n",
    "    anatcl_features = np.mean(all_fold_features, axis=0)\n",
    "    print(f\"  AnatCL: {anatcl_features.shape}\")\n",
    "    \n",
    "    print(\"Extracting BrainIAC...\")\n",
    "    brainiac_vit = load_brainiac_vit(BRAINIAC_CKPT, device)\n",
    "    brainiac_transform = lambda x: (x - x.mean()) / (x.std() + 1e-6)\n",
    "    dl = DataLoader(BrainIACDataset(brainiac_paths, age_labels, brainiac_transform), batch_size=16, num_workers=0)\n",
    "    brainiac_features = []\n",
    "    with torch.no_grad():\n",
    "        for x, _ in dl:\n",
    "            out = brainiac_vit(x.to(device))\n",
    "            cls_token = out[0][:, 0] if isinstance(out, tuple) else out[:, 0]\n",
    "            brainiac_features.append(cls_token.cpu().numpy())\n",
    "    brainiac_features = np.vstack(brainiac_features)\n",
    "    print(f\"  BrainIAC: {brainiac_features.shape}\")\n",
    "    del brainiac_vit\n",
    "    \n",
    "    print(\"Extracting SwinBrain...\")\n",
    "    swinbrain_model = load_swinbrain(SWINBRAIN_CKPT, device)\n",
    "    swinbrain_hook_output = [None]\n",
    "    def swinbrain_hook(module, input, output):\n",
    "        swinbrain_hook_output[0] = output\n",
    "    hook_handle = swinbrain_model.encoder10.register_forward_hook(swinbrain_hook)\n",
    "    pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "    dl = DataLoader(SwinBrainDataset(brainiac_paths, age_labels), batch_size=8, num_workers=0)\n",
    "    swinbrain_features_list = []\n",
    "    with torch.no_grad():\n",
    "        for x, _ in dl:\n",
    "            _ = swinbrain_model(x.to(device))\n",
    "            pooled = pool(swinbrain_hook_output[0])\n",
    "            swinbrain_features_list.append(pooled.view(pooled.size(0), -1).cpu().numpy())\n",
    "    hook_handle.remove()\n",
    "    swinbrain_features = np.vstack(swinbrain_features_list)\n",
    "    print(f\"  SwinBrain: {swinbrain_features.shape}\")\n",
    "    del swinbrain_model\n",
    "    \n",
    "    print(\"Extracting CNN...\")\n",
    "    cnn_model = CNN3D().to(device).eval()\n",
    "    dl = DataLoader(CNNDataset(brainiac_paths), batch_size=8, num_workers=0)\n",
    "    with torch.no_grad():\n",
    "        cnn_features = np.vstack([cnn_model(x.to(device)).cpu().numpy() for x in dl])\n",
    "    print(f\"  CNN: {cnn_features.shape}\")\n",
    "    del cnn_model\n",
    "    \n",
    "    return anatcl_features, brainiac_features, swinbrain_features, cnn_features\n",
    "\n",
    "def plot_correlation_with_labels(corr_matrix, boundaries, model_names, cluster_dfs, fs_keys, dataset_name):\n",
    "    fig, ax = plt.subplots(figsize=(32, 30))\n",
    "    im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1, aspect='equal', interpolation='nearest')\n",
    "    plt.colorbar(im, ax=ax, shrink=0.3)\n",
    "    \n",
    "    for b in boundaries[1:-1]:\n",
    "        ax.axhline(y=b-0.5, color='black', linewidth=4)\n",
    "        ax.axvline(x=b-0.5, color='black', linewidth=4)\n",
    "    \n",
    "    bar_width = 8\n",
    "    colors = {'Surface_Area': '#2ECC71', 'Thickness': '#3498DB'}\n",
    "    \n",
    "    for fs_key in fs_keys:\n",
    "        if fs_key not in cluster_dfs:\n",
    "            continue\n",
    "        cluster_df = cluster_dfs[fs_key]\n",
    "        fs_idx = model_names.index(fs_key)\n",
    "        fs_start = boundaries[fs_idx]\n",
    "        for i, ft in enumerate(cluster_df['feature_type'].values):\n",
    "            color = colors.get(ft, '#95A5A6')\n",
    "            ax.add_patch(plt.Rectangle((boundaries[-1] + 3, fs_start + i - 0.5), bar_width, 1, facecolor=color, edgecolor='none'))\n",
    "            ax.add_patch(plt.Rectangle((-bar_width - 5, fs_start + i - 0.5), bar_width, 1, facecolor=color, edgecolor='none'))\n",
    "    \n",
    "    display_names = {'FS_Schaefer': 'FS: Schaefer', 'FS_aparc': 'FS: aparc', 'FreeSurfer': 'FS'}\n",
    "    label_fontsize = 12 if dataset_name.lower() == 'hbn' else 18\n",
    "    \n",
    "    for i in range(len(boundaries) - 1):\n",
    "        pos = (boundaries[i] + boundaries[i+1]) / 2\n",
    "        label = display_names.get(model_names[i], model_names[i])\n",
    "        ax.text(pos, -30, label, ha='center', fontsize=label_fontsize, weight='bold')\n",
    "        ax.text(-30, pos, label, ha='right', va='center', fontsize=label_fontsize, weight='bold', rotation=90)\n",
    "    \n",
    "    ax.set_xlim(-bar_width - 60, boundaries[-1] + bar_width + 50)\n",
    "    ax.set_ylim(boundaries[-1] + 20, -60)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    ax.legend(handles=[Patch(facecolor='#2ECC71', edgecolor='black', label='Surface Area'),\n",
    "                       Patch(facecolor='#3498DB', edgecolor='black', label='Thickness')],\n",
    "              loc='upper left', fontsize=14, bbox_to_anchor=(1.02, 1.0), frameon=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{dataset_name.lower()}_cross_model_correlation.pdf', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {dataset_name.lower()}_cross_model_correlation.pdf\")\n",
    "\n",
    "\n",
    "# PPMI Dataset\n",
    "print(\"\\n=== PPMI DATASET ===\")\n",
    "PPMI_CAT12_BASE_DIR = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data/inputs\"\n",
    "PPMI_DATA_DIR = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data\"\n",
    "PPMI_LABELS_PATH = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data/processed_cohort_with_mri.csv\"\n",
    "PPMI_BRAINIAC_MAPPING_CSV = os.path.join(PPMI_DATA_DIR, \"processed_files_mapping.csv\")\n",
    "\n",
    "print(\"Loading PPMI demographics...\")\n",
    "ppmi_labels_df = pd.read_csv(PPMI_LABELS_PATH)\n",
    "ppmi_id_sex_dict, ppmi_id_parkinson_dict, ppmi_id_age_dict = {}, {}, {}\n",
    "for _, row in ppmi_labels_df.iterrows():\n",
    "    patno = str(int(row['PATNO']))\n",
    "    ppmi_id_sex_dict[patno] = 1 if row['Sex'].strip().upper() == 'F' else 0\n",
    "    ppmi_id_parkinson_dict[patno] = 1 if row['Group'].strip() == 'PD' else 0\n",
    "    ppmi_id_age_dict[patno] = row['Age']\n",
    "print(f\"  {len(ppmi_id_sex_dict)} subjects\")\n",
    "\n",
    "def extract_patno_from_path(filepath):\n",
    "    for part in filepath.split(os.sep):\n",
    "        if part.startswith('sub-'):\n",
    "            return part[4:]\n",
    "    return None\n",
    "\n",
    "print(\"Finding PPMI files...\")\n",
    "ppmi_cat12_files = glob.glob(os.path.join(PPMI_CAT12_BASE_DIR, \"**\", \"mwp1*.nii*\"), recursive=True)\n",
    "ppmi_cat12_data = {}\n",
    "for f in ppmi_cat12_files:\n",
    "    if os.path.isfile(f):\n",
    "        patno = extract_patno_from_path(f)\n",
    "        if patno and patno in ppmi_id_sex_dict:\n",
    "            ppmi_cat12_data[patno] = f\n",
    "\n",
    "ppmi_brainiac_df = pd.read_csv(PPMI_BRAINIAC_MAPPING_CSV).dropna(subset=[\"processed_file\", \"Age\", \"subject_id\"])\n",
    "ppmi_brainiac_data = {}\n",
    "for _, row in ppmi_brainiac_df.iterrows():\n",
    "    patno = str(row['subject_id'])\n",
    "    if patno in ppmi_id_sex_dict and os.path.exists(row['processed_file']):\n",
    "        ppmi_brainiac_data[patno] = row['processed_file']\n",
    "\n",
    "print(\"Loading PPMI FreeSurfer...\")\n",
    "ppmi_fs_cth_df = pd.read_csv(os.path.join(PPMI_CAT12_BASE_DIR, \"FS7_APARC_CTH_23Oct2025.csv\"))\n",
    "ppmi_fs_sa_df = pd.read_csv(os.path.join(PPMI_CAT12_BASE_DIR, \"FS7_APARC_SA_23Oct2025.csv\"))\n",
    "ppmi_fs_cth_df = ppmi_fs_cth_df[ppmi_fs_cth_df['EVENT_ID'] == 'BL'].copy()\n",
    "ppmi_fs_sa_df = ppmi_fs_sa_df[ppmi_fs_sa_df['EVENT_ID'] == 'BL'].copy()\n",
    "ppmi_fs_cth_df['PATNO'] = ppmi_fs_cth_df['PATNO'].astype(str)\n",
    "ppmi_fs_sa_df['PATNO'] = ppmi_fs_sa_df['PATNO'].astype(str)\n",
    "ppmi_cth_features = [c for c in ppmi_fs_cth_df.columns if c not in ['PATNO', 'EVENT_ID']]\n",
    "ppmi_sa_features = [c for c in ppmi_fs_sa_df.columns if c not in ['PATNO', 'EVENT_ID']]\n",
    "\n",
    "ppmi_fs_feature_info = []\n",
    "for feat in ppmi_cth_features:\n",
    "    ppmi_fs_feature_info.append({'feature_name': feat, 'feature_type': 'Thickness'})\n",
    "for feat in ppmi_sa_features:\n",
    "    ppmi_fs_feature_info.append({'feature_name': feat, 'feature_type': 'Surface_Area'})\n",
    "\n",
    "ppmi_common_subjects = sorted(list(set(ppmi_cat12_data.keys()) & set(ppmi_brainiac_data.keys())))\n",
    "ppmi_fs_data = {}\n",
    "for patno in ppmi_common_subjects:\n",
    "    cth_row = ppmi_fs_cth_df[ppmi_fs_cth_df['PATNO'] == patno]\n",
    "    sa_row = ppmi_fs_sa_df[ppmi_fs_sa_df['PATNO'] == patno]\n",
    "    if len(cth_row) > 0 and len(sa_row) > 0:\n",
    "        combined = np.concatenate([cth_row[ppmi_cth_features].values.flatten(), sa_row[ppmi_sa_features].values.flatten()])\n",
    "        if not np.any(np.isnan(combined)):\n",
    "            ppmi_fs_data[patno] = combined\n",
    "\n",
    "ppmi_common_subjects = sorted(list(set(ppmi_cat12_data.keys()) & set(ppmi_brainiac_data.keys()) & set(ppmi_fs_data.keys())))\n",
    "print(f\"  Common subjects: {len(ppmi_common_subjects)}\")\n",
    "\n",
    "ppmi_cat12_paths = [ppmi_cat12_data[p] for p in ppmi_common_subjects]\n",
    "ppmi_brainiac_paths = [ppmi_brainiac_data[p] for p in ppmi_common_subjects]\n",
    "ppmi_fs_features = np.array([ppmi_fs_data[p] for p in ppmi_common_subjects])\n",
    "ppmi_sex_labels = np.array([ppmi_id_sex_dict[p] for p in ppmi_common_subjects])\n",
    "ppmi_parkinson_labels = np.array([ppmi_id_parkinson_dict[p] for p in ppmi_common_subjects])\n",
    "ppmi_age_labels = np.array([ppmi_id_age_dict[p] for p in ppmi_common_subjects])\n",
    "\n",
    "ppmi_anatcl_features, ppmi_brainiac_features, ppmi_swinbrain_features, ppmi_cnn_features = extract_features(\n",
    "    ppmi_cat12_paths, ppmi_brainiac_paths, ppmi_age_labels, device)\n",
    "\n",
    "ppmi_features_dict = {\n",
    "    'AnatCL': ppmi_anatcl_features,\n",
    "    'BrainIAC': ppmi_brainiac_features,\n",
    "    'SwinBrain': ppmi_swinbrain_features,\n",
    "    'CNN': ppmi_cnn_features,\n",
    "    'FreeSurfer': ppmi_fs_features,\n",
    "    'AnatCL+FS': np.hstack([ppmi_anatcl_features, ppmi_fs_features]),\n",
    "    'BrainIAC+FS': np.hstack([ppmi_brainiac_features, ppmi_fs_features]),\n",
    "    'SwinBrain+FS': np.hstack([ppmi_swinbrain_features, ppmi_fs_features]),\n",
    "    'FM+FS': np.hstack([ppmi_anatcl_features, ppmi_brainiac_features, ppmi_swinbrain_features, ppmi_fs_features])\n",
    "}\n",
    "\n",
    "print(\"\\nComputing PPMI correlation...\")\n",
    "ppmi_corr_matrix, ppmi_boundaries, ppmi_model_names, ppmi_reorder_indices, ppmi_cluster_dfs = \\\n",
    "    compute_correlation_with_cluster_labels(ppmi_features_dict, {'FreeSurfer': ppmi_fs_feature_info}, ['FreeSurfer'], \"PPMI\")\n",
    "\n",
    "ppmi_sex_results = run_classification(ppmi_features_dict, ppmi_sex_labels, task_name=\"PPMI Sex Classification\")\n",
    "ppmi_parkinson_results = run_classification(ppmi_features_dict, ppmi_parkinson_labels, task_name=\"PPMI Parkinson Classification\")\n",
    "\n",
    "ppmi_male_indices = np.where(ppmi_sex_labels == 0)[0]\n",
    "ppmi_male_features_dict = {k: v[ppmi_male_indices] for k, v in ppmi_features_dict.items()}\n",
    "ppmi_male_age_labels = ppmi_age_labels[ppmi_male_indices]\n",
    "print(f\"\\nPPMI Male: {len(ppmi_male_indices)} subjects\")\n",
    "ppmi_male_age_results = run_regression(ppmi_male_features_dict, ppmi_male_age_labels, task_name=\"PPMI Male Age\")\n",
    "\n",
    "ppmi_female_indices = np.where(ppmi_sex_labels == 1)[0]\n",
    "ppmi_female_features_dict = {k: v[ppmi_female_indices] for k, v in ppmi_features_dict.items()}\n",
    "ppmi_female_age_labels = ppmi_age_labels[ppmi_female_indices]\n",
    "print(f\"\\nPPMI Female: {len(ppmi_female_indices)} subjects\")\n",
    "ppmi_female_age_results = run_regression(ppmi_female_features_dict, ppmi_female_age_labels, task_name=\"PPMI Female Age\")\n",
    "\n",
    "\n",
    "# HBN Dataset\n",
    "print(\"\\n=== HBN DATASET ===\")\n",
    "HBN_BIDS = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/HBN_BIDS\"\n",
    "HBN_BIDS_LOWER = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/hbn_bids\"\n",
    "HBN_BRAINIAC_OUT = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/brainiac_p_outputs\"\n",
    "HBN_FREESURFER_DIR = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/HBN_FreeSurfer/freesurfer\"\n",
    "HBN_DEMO_FILE = os.path.join(HBN_BIDS, \"final_preprocessed_subjects_with_demographics.tsv\")\n",
    "HBN_PARCELLATION = \"Schaefer2018_400Parcels_17Networks_order\"\n",
    "\n",
    "print(\"Loading HBN demographics...\")\n",
    "hbn_demo_df = pd.read_csv(HBN_DEMO_FILE, sep='\\t')\n",
    "hbn_demo_df['participant_id'] = hbn_demo_df['participant_id'].astype(str)\n",
    "hbn_id_sex_dict, hbn_id_age_dict = {}, {}\n",
    "for _, row in hbn_demo_df.iterrows():\n",
    "    hbn_id_sex_dict[row['participant_id']] = 1 if row['sex'].strip() == 'Female' else 0\n",
    "    hbn_id_age_dict[row['participant_id']] = row['age']\n",
    "print(f\"  {len(hbn_id_sex_dict)} subjects\")\n",
    "\n",
    "print(\"Finding HBN files...\")\n",
    "hbn_cat12_data = {}\n",
    "for subject_id in hbn_id_sex_dict.keys():\n",
    "    for base in [HBN_BIDS, HBN_BIDS_LOWER]:\n",
    "        pattern = os.path.join(base, f\"sub-{subject_id}\", \"ses-*\", \"anat\", \"mri\", \"mwp1sub*.nii\")\n",
    "        files = glob.glob(pattern)\n",
    "        if files:\n",
    "            hbn_cat12_data[subject_id] = files[0]\n",
    "            break\n",
    "\n",
    "hbn_brainiac_data = {}\n",
    "batch_dirs = glob.glob(os.path.join(HBN_BRAINIAC_OUT, \"batch_*\"))\n",
    "for batch_dir in sorted(batch_dirs):\n",
    "    for f in glob.glob(os.path.join(batch_dir, \"sub-*_0000.nii.gz\")):\n",
    "        subject_id = os.path.basename(f).split('_')[0].replace('sub-', '')\n",
    "        if subject_id in hbn_id_sex_dict and subject_id not in hbn_brainiac_data:\n",
    "            hbn_brainiac_data[subject_id] = f\n",
    "\n",
    "print(\"Loading HBN FreeSurfer Schaefer...\")\n",
    "hbn_fs_data, hbn_fs_region_info = {}, {}\n",
    "for subject_id in hbn_id_sex_dict.keys():\n",
    "    stats_file = os.path.join(HBN_FREESURFER_DIR, f\"sub-{subject_id}\", f\"sub-{subject_id}_regionsurfacestats.tsv\")\n",
    "    if os.path.exists(stats_file):\n",
    "        df = pd.read_csv(stats_file, sep='\\t')\n",
    "        filtered_df = df[df[\"atlas\"] == HBN_PARCELLATION].sort_values(\"StructName\")\n",
    "        if not filtered_df.empty and \"SurfArea\" in filtered_df.columns and \"ThickAvg\" in filtered_df.columns:\n",
    "            surf_area = filtered_df[\"SurfArea\"].values[:400]\n",
    "            thick_avg = filtered_df[\"ThickAvg\"].values[:400]\n",
    "            combined = np.concatenate([surf_area, thick_avg])\n",
    "            if not np.any(np.isnan(combined)) and len(surf_area) == 400 and len(thick_avg) == 400:\n",
    "                hbn_fs_data[subject_id] = combined\n",
    "                if not hbn_fs_region_info:\n",
    "                    hbn_fs_region_info = {'region_names': filtered_df['StructName'].values[:400].tolist(), 'hemisphere': filtered_df['hemisphere'].values[:400].tolist()}\n",
    "hbn_fs_feature_info = []\n",
    "if hbn_fs_region_info:\n",
    "    for i in range(400):\n",
    "        hbn_fs_feature_info.append({'feature_name': hbn_fs_region_info['region_names'][i], 'feature_type': 'Surface_Area', 'hemisphere': hbn_fs_region_info['hemisphere'][i]})\n",
    "    for i in range(400):\n",
    "        hbn_fs_feature_info.append({'feature_name': hbn_fs_region_info['region_names'][i], 'feature_type': 'Thickness', 'hemisphere': hbn_fs_region_info['hemisphere'][i]})\n",
    "\n",
    "print(\"Loading HBN FreeSurfer aparc...\")\n",
    "hbn_aparc_fs_data, hbn_aparc_region_info = {}, {}\n",
    "for subject_id in hbn_id_sex_dict.keys():\n",
    "    stats_file = os.path.join(HBN_FREESURFER_DIR, f\"sub-{subject_id}\", f\"sub-{subject_id}_regionsurfacestats.tsv\")\n",
    "    if os.path.exists(stats_file):\n",
    "        df = pd.read_csv(stats_file, sep='\\t')\n",
    "        filtered_df = df[df[\"atlas\"] == \"aparc\"].sort_values(\"StructName\")\n",
    "        if not filtered_df.empty and \"SurfArea\" in filtered_df.columns and \"ThickAvg\" in filtered_df.columns:\n",
    "            surf_area = filtered_df[\"SurfArea\"].values\n",
    "            thick_avg = filtered_df[\"ThickAvg\"].values\n",
    "            combined = np.concatenate([surf_area, thick_avg])\n",
    "            if not np.any(np.isnan(combined)) and len(surf_area) == 68 and len(thick_avg) == 68:\n",
    "                hbn_aparc_fs_data[subject_id] = combined\n",
    "                if not hbn_aparc_region_info:\n",
    "                    hbn_aparc_region_info = {'region_names': filtered_df['StructName'].values.tolist(), 'hemisphere': filtered_df['hemisphere'].values.tolist()}\n",
    "                    \n",
    "hbn_aparc_fs_feature_info = []\n",
    "if hbn_aparc_region_info:\n",
    "    for i in range(68):\n",
    "        hbn_aparc_fs_feature_info.append({'feature_name': hbn_aparc_region_info['region_names'][i], 'feature_type': 'Surface_Area', 'hemisphere': hbn_aparc_region_info['hemisphere'][i]})\n",
    "    for i in range(68):\n",
    "        hbn_aparc_fs_feature_info.append({'feature_name': hbn_aparc_region_info['region_names'][i], 'feature_type': 'Thickness', 'hemisphere': hbn_aparc_region_info['hemisphere'][i]})\n",
    "\n",
    "hbn_common_subjects = sorted(list(set(hbn_cat12_data.keys()) & set(hbn_brainiac_data.keys()) & set(hbn_fs_data.keys()) & set(hbn_aparc_fs_data.keys())))\n",
    "print(f\"  Common subjects: {len(hbn_common_subjects)}\")\n",
    "\n",
    "hbn_cat12_paths = [hbn_cat12_data[s] for s in hbn_common_subjects]\n",
    "hbn_brainiac_paths = [hbn_brainiac_data[s] for s in hbn_common_subjects]\n",
    "hbn_fs_features = np.array([hbn_fs_data[s] for s in hbn_common_subjects])\n",
    "hbn_aparc_fs_features = np.array([hbn_aparc_fs_data[s] for s in hbn_common_subjects])\n",
    "hbn_sex_labels = np.array([hbn_id_sex_dict[s] for s in hbn_common_subjects])\n",
    "hbn_age_labels = np.array([hbn_id_age_dict[s] for s in hbn_common_subjects])\n",
    "\n",
    "hbn_anatcl_features, hbn_brainiac_features, hbn_swinbrain_features, hbn_cnn_features = extract_features(\n",
    "    hbn_cat12_paths, hbn_brainiac_paths, hbn_age_labels, device)\n",
    "\n",
    "hbn_combined_features_dict = {\n",
    "    'AnatCL': hbn_anatcl_features,\n",
    "    'BrainIAC': hbn_brainiac_features,\n",
    "    'SwinBrain': hbn_swinbrain_features,\n",
    "    'CNN': hbn_cnn_features,\n",
    "    'FS_Schaefer': hbn_fs_features,\n",
    "    'FS_aparc': hbn_aparc_fs_features,\n",
    "    'AnatCL+Schaefer': np.hstack([hbn_anatcl_features, hbn_fs_features]),\n",
    "    'BrainIAC+Schaefer': np.hstack([hbn_brainiac_features, hbn_fs_features]),\n",
    "    'SwinBrain+Schaefer': np.hstack([hbn_swinbrain_features, hbn_fs_features]),\n",
    "    'AnatCL+aparc': np.hstack([hbn_anatcl_features, hbn_aparc_fs_features]),\n",
    "    'BrainIAC+aparc': np.hstack([hbn_brainiac_features, hbn_aparc_fs_features]),\n",
    "    'SwinBrain+aparc': np.hstack([hbn_swinbrain_features, hbn_aparc_fs_features]),\n",
    "    'FM+FS_Schaefer': np.hstack([hbn_anatcl_features, hbn_brainiac_features, hbn_swinbrain_features, hbn_fs_features]),\n",
    "    'FM+FS_aparc': np.hstack([hbn_anatcl_features, hbn_brainiac_features, hbn_swinbrain_features, hbn_aparc_fs_features])\n",
    "}\n",
    "\n",
    "print(\"\\nComputing HBN correlation...\")\n",
    "hbn_corr_dict = {k: v for k, v in hbn_combined_features_dict.items() if k in ['AnatCL', 'BrainIAC', 'SwinBrain', 'CNN', 'FS_Schaefer', 'FS_aparc']}\n",
    "hbn_corr_matrix, hbn_boundaries, hbn_model_names, hbn_reorder_indices, hbn_cluster_dfs = \\\n",
    "    compute_correlation_with_cluster_labels(hbn_corr_dict, {'FS_Schaefer': hbn_fs_feature_info, 'FS_aparc': hbn_aparc_fs_feature_info}, ['FS_Schaefer', 'FS_aparc'], \"HBN\")\n",
    "\n",
    "hbn_sex_results = run_classification(hbn_combined_features_dict, hbn_sex_labels, task_name=\"HBN Sex Classification\")\n",
    "\n",
    "hbn_male_indices = np.where(hbn_sex_labels == 0)[0]\n",
    "hbn_male_features_dict = {k: v[hbn_male_indices] for k, v in hbn_combined_features_dict.items()}\n",
    "hbn_male_age_labels = hbn_age_labels[hbn_male_indices]\n",
    "print(f\"\\nHBN Male: {len(hbn_male_indices)} subjects\")\n",
    "hbn_male_age_results = run_regression(hbn_male_features_dict, hbn_male_age_labels, task_name=\"HBN Male Age\")\n",
    "\n",
    "hbn_female_indices = np.where(hbn_sex_labels == 1)[0]\n",
    "hbn_female_features_dict = {k: v[hbn_female_indices] for k, v in hbn_combined_features_dict.items()}\n",
    "hbn_female_age_labels = hbn_age_labels[hbn_female_indices]\n",
    "print(f\"\\nHBN Female: {len(hbn_female_indices)} subjects\")\n",
    "hbn_female_age_results = run_regression(hbn_female_features_dict, hbn_female_age_labels, task_name=\"HBN Female Age\")\n",
    "\n",
    "\n",
    "# Figures\n",
    "ppmi_models = ['AnatCL', 'BrainIAC', 'SwinBrain', 'CNN', 'FreeSurfer', 'AnatCL+FS', 'BrainIAC+FS', 'SwinBrain+FS', 'FM+FS']\n",
    "hbn_models = ['AnatCL', 'BrainIAC', 'SwinBrain', 'CNN', 'FS_Schaefer', 'FS_aparc',\n",
    "              'AnatCL+Schaefer', 'BrainIAC+Schaefer', 'SwinBrain+Schaefer',\n",
    "              'AnatCL+aparc', 'BrainIAC+aparc', 'SwinBrain+aparc', 'FM+FS_Schaefer', 'FM+FS_aparc']\n",
    "\n",
    "model_styles = {\n",
    "    'AnatCL': {'color': '#9B59B6', 'marker': 'o', 'label': 'AnatCL'},\n",
    "    'BrainIAC': {'color': '#9B59B6', 'marker': 's', 'label': 'BrainIAC'},\n",
    "    'SwinBrain': {'color': '#9B59B6', 'marker': '^', 'label': 'SwinBrain'},\n",
    "    'CNN': {'color': '#3498DB', 'marker': 'D', 'label': 'CNN'},\n",
    "    'FreeSurfer': {'color': '#E74C3C', 'marker': 'o', 'label': 'FreeSurfer', 'linewidth': 3},\n",
    "    'FS_Schaefer': {'color': '#E74C3C', 'marker': 'o', 'label': 'FS: Schaefer', 'linewidth': 3},\n",
    "    'FS_aparc': {'color': '#E74C3C', 'marker': 's', 'label': 'FS: aparc', 'linewidth': 3},\n",
    "    'AnatCL+FS': {'color': '#27AE60', 'marker': 'o', 'label': 'AnatCL+FS'},\n",
    "    'BrainIAC+FS': {'color': '#27AE60', 'marker': 's', 'label': 'BrainIAC+FS'},\n",
    "    'SwinBrain+FS': {'color': '#27AE60', 'marker': '^', 'label': 'SwinBrain+FS'},\n",
    "    'AnatCL+Schaefer': {'color': '#27AE60', 'marker': 'o', 'label': 'AnatCL+Schaefer'},\n",
    "    'BrainIAC+Schaefer': {'color': '#27AE60', 'marker': 's', 'label': 'BrainIAC+Schaefer'},\n",
    "    'SwinBrain+Schaefer': {'color': '#27AE60', 'marker': '^', 'label': 'SwinBrain+Schaefer'},\n",
    "    'AnatCL+aparc': {'color': '#2ECC71', 'marker': 'o', 'label': 'AnatCL+aparc'},\n",
    "    'BrainIAC+aparc': {'color': '#2ECC71', 'marker': 's', 'label': 'BrainIAC+aparc'},\n",
    "    'SwinBrain+aparc': {'color': '#2ECC71', 'marker': '^', 'label': 'SwinBrain+aparc'},\n",
    "    'FM+FS': {'color': '#2C3E50', 'marker': '*', 'label': 'FM+FS', 'markersize': 10},\n",
    "    'FM+FS_Schaefer': {'color': '#2C3E50', 'marker': '*', 'label': 'FM+FS: Schaefer', 'markersize': 10},\n",
    "    'FM+FS_aparc': {'color': '#34495E', 'marker': '*', 'label': 'FM+FS: aparc', 'markersize': 10}\n",
    "}\n",
    "alphas = [0.01, 0.05, 0.1, 0.15, 0.25, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "ppmi_n_cv = len(ppmi_sex_labels) - int(0.1 * len(ppmi_sex_labels))\n",
    "hbn_n_cv = len(hbn_sex_labels) - int(0.1 * len(hbn_sex_labels))\n",
    "ppmi_sizes = [int(a * ppmi_n_cv * 0.8) for a in alphas]\n",
    "hbn_sizes = [int(a * hbn_n_cv * 0.8) for a in alphas]\n",
    "\n",
    "def mean_statistic(sample, axis):\n",
    "    return np.mean(sample, axis=axis)\n",
    "\n",
    "def get_significance_stars(p_corr):\n",
    "    \"\"\"Return stars based on corrected p-value: *** < 0.001, ** < 0.01, * < 0.05\"\"\"\n",
    "    if p_corr < 0.001:\n",
    "        return ' ***'\n",
    "    elif p_corr < 0.01:\n",
    "        return ' **'\n",
    "    elif p_corr < 0.05:\n",
    "        return ' *'\n",
    "    return ''\n",
    "\n",
    "def run_permutation_test_classification(results, fs_key, models_to_test, n_resamples=9999):\n",
    "    perm_results = {}\n",
    "    if fs_key not in results:\n",
    "        return perm_results\n",
    "    \n",
    "    fs_folds = np.array(results[fs_key]['fold_results_alpha1']['val_acc'])\n",
    "    fs_mean = np.mean(fs_folds)\n",
    "    \n",
    "    models_beating_fs = [m for m in models_to_test if m != fs_key and m in results and 'Dummy' not in m \n",
    "                         and np.mean(results[m]['fold_results_alpha1']['val_acc']) > fs_mean]\n",
    "    n_tests = len(models_beating_fs)\n",
    "    if n_tests == 0:\n",
    "        return perm_results\n",
    "    \n",
    "    for model in models_beating_fs:\n",
    "        model_folds = np.array(results[model]['fold_results_alpha1']['val_acc'])\n",
    "        diff = model_folds - fs_folds\n",
    "        res = permutation_test((diff,), mean_statistic, permutation_type='samples', n_resamples=n_resamples, vectorized=True, axis=0, alternative='greater')\n",
    "        p_corr = min(res.pvalue * n_tests, 1.0)\n",
    "        perm_results[model] = {'p_raw': res.pvalue, 'p_corr': p_corr, 'n_tests': n_tests, 'diff': np.mean(diff), 'significant': p_corr < 0.05}\n",
    "        print(f\"    {model}: diff={np.mean(diff):.4f}, p_raw={res.pvalue:.4f}, p_corr={p_corr:.4f}\")\n",
    "    return perm_results\n",
    "\n",
    "def run_permutation_test_regression(results, fs_key, models_to_test, n_resamples=9999):\n",
    "    perm_results = {}\n",
    "    if fs_key not in results:\n",
    "        return perm_results\n",
    "    \n",
    "    fs_folds = np.array(results[fs_key]['fold_results_alpha1']['val_mae'])\n",
    "    fs_mean = np.mean(fs_folds)\n",
    "    \n",
    "    models_beating_fs = [m for m in models_to_test if m != fs_key and m in results and 'Dummy' not in m \n",
    "                         and np.mean(results[m]['fold_results_alpha1']['val_mae']) < fs_mean]\n",
    "    n_tests = len(models_beating_fs)\n",
    "    if n_tests == 0:\n",
    "        return perm_results\n",
    "    \n",
    "    for model in models_beating_fs:\n",
    "        model_folds = np.array(results[model]['fold_results_alpha1']['val_mae'])\n",
    "        diff = model_folds - fs_folds\n",
    "        res = permutation_test((diff,), mean_statistic, permutation_type='samples', n_resamples=n_resamples, vectorized=True, axis=0, alternative='less')\n",
    "        p_corr = min(res.pvalue * n_tests, 1.0)\n",
    "        perm_results[model] = {'p_raw': res.pvalue, 'p_corr': p_corr, 'n_tests': n_tests, 'diff': np.mean(diff), 'significant': p_corr < 0.05}\n",
    "        print(f\"    {model}: diff={np.mean(diff):.4f}, p_raw={res.pvalue:.4f}, p_corr={p_corr:.4f}\")\n",
    "    return perm_results\n",
    "\n",
    "def combine_hbn_perm_strict(perm_schaefer, perm_aparc):\n",
    "    combined = {}\n",
    "    all_models = set(perm_schaefer.keys()) | set(perm_aparc.keys())\n",
    "    \n",
    "    for model in all_models:\n",
    "        in_schaefer = model in perm_schaefer\n",
    "        in_aparc = model in perm_aparc\n",
    "        \n",
    "        if in_schaefer and in_aparc:\n",
    "            sig_schaefer = perm_schaefer[model]['significant']\n",
    "            sig_aparc = perm_aparc[model]['significant']\n",
    "            \n",
    "            if sig_schaefer and sig_aparc:\n",
    "                p_corr = max(perm_schaefer[model]['p_corr'], perm_aparc[model]['p_corr'])\n",
    "                combined[model] = {'p_corr': p_corr, 'significant': True}\n",
    "            else:\n",
    "                combined[model] = {'significant': False}\n",
    "        else:\n",
    "            combined[model] = {'significant': False}\n",
    "    \n",
    "    return combined\n",
    "\n",
    "print(\"\\nPERMUTATION TESTS (Bonferroni corrected)\")\n",
    "print(\"\\nPPMI Sex:\")\n",
    "ppmi_sex_perm = run_permutation_test_classification(ppmi_sex_results, 'FreeSurfer', ppmi_models)\n",
    "print(\"\\nPPMI Male Age:\")\n",
    "ppmi_male_age_perm = run_permutation_test_regression(ppmi_male_age_results, 'FreeSurfer', ppmi_models)\n",
    "print(\"\\nPPMI Female Age:\")\n",
    "ppmi_female_age_perm = run_permutation_test_regression(ppmi_female_age_results, 'FreeSurfer', ppmi_models)\n",
    "\n",
    "print(\"\\nHBN Sex (vs Schaefer):\")\n",
    "hbn_sex_perm_schaefer = run_permutation_test_classification(hbn_sex_results, 'FS_Schaefer', hbn_models)\n",
    "print(\"\\nHBN Sex (vs aparc):\")\n",
    "hbn_sex_perm_aparc = run_permutation_test_classification(hbn_sex_results, 'FS_aparc', hbn_models)\n",
    "print(\"\\nHBN Male Age (vs Schaefer):\")\n",
    "hbn_male_perm_schaefer = run_permutation_test_regression(hbn_male_age_results, 'FS_Schaefer', hbn_models)\n",
    "print(\"\\nHBN Male Age (vs aparc):\")\n",
    "hbn_male_perm_aparc = run_permutation_test_regression(hbn_male_age_results, 'FS_aparc', hbn_models)\n",
    "print(\"\\nHBN Female Age (vs Schaefer):\")\n",
    "hbn_female_perm_schaefer = run_permutation_test_regression(hbn_female_age_results, 'FS_Schaefer', hbn_models)\n",
    "print(\"\\nHBN Female Age (vs aparc):\")\n",
    "hbn_female_perm_aparc = run_permutation_test_regression(hbn_female_age_results, 'FS_aparc', hbn_models)\n",
    "\n",
    "\n",
    "\n",
    "# Combined learning curves with global legends\n",
    "fig = plt.figure(figsize=(20, 28))\n",
    "gs = fig.add_gridspec(5, 2, height_ratios=[1, 1, 1, 1, 0.15], hspace=0.3, wspace=0.25)\n",
    "\n",
    "axes = [[fig.add_subplot(gs[i, j]) for j in range(2)] for i in range(4)]\n",
    "\n",
    "axes[0][0].set_title('PPMI', fontsize=18, weight='bold', pad=20)\n",
    "axes[0][1].set_title('HBN', fontsize=18, weight='bold', pad=20)\n",
    "\n",
    "hbn_sex_perm_strict = combine_hbn_perm_strict(hbn_sex_perm_schaefer, hbn_sex_perm_aparc)\n",
    "hbn_male_perm_strict = combine_hbn_perm_strict(hbn_male_perm_schaefer, hbn_male_perm_aparc)\n",
    "hbn_female_perm_strict = combine_hbn_perm_strict(hbn_female_perm_schaefer, hbn_female_perm_aparc)\n",
    "\n",
    "mae_ylim = (0, 12)\n",
    "\n",
    "# Row 0: Sex Classification\n",
    "plot_classification(axes[0][0], ppmi_sex_results, ppmi_sizes, ppmi_models, ppmi_sex_perm, ylabel='Balanced Accuracy')\n",
    "plot_classification(axes[0][1], hbn_sex_results, hbn_sizes, hbn_models, hbn_sex_perm_strict)\n",
    "axes[0][0].text(-0.15, 0.5, 'Sex Classification', transform=axes[0][0].transAxes, fontsize=14, weight='bold', va='center', ha='center', rotation=90)\n",
    "\n",
    "# Row 1: Male Age\n",
    "plot_regression(axes[1][0], ppmi_male_age_results, ppmi_sizes, ppmi_models, ppmi_male_age_perm, ylabel='MAE (years)', ylim=mae_ylim)\n",
    "plot_regression(axes[1][1], hbn_male_age_results, hbn_sizes, hbn_models, hbn_male_perm_strict, ylim=mae_ylim)\n",
    "axes[1][0].text(-0.15, 0.5, 'Male Age Prediction', transform=axes[1][0].transAxes, fontsize=14, weight='bold', va='center', ha='center', rotation=90)\n",
    "\n",
    "# Row 2: Female Age\n",
    "plot_regression(axes[2][0], ppmi_female_age_results, ppmi_sizes, ppmi_models, ppmi_female_age_perm, ylabel='MAE (years)', ylim=mae_ylim)\n",
    "plot_regression(axes[2][1], hbn_female_age_results, hbn_sizes, hbn_models, hbn_female_perm_strict, ylim=mae_ylim)\n",
    "axes[2][0].text(-0.15, 0.5, 'Female Age Prediction', transform=axes[2][0].transAxes, fontsize=14, weight='bold', va='center', ha='center', rotation=90)\n",
    "\n",
    "# Row 3: Parkinson (PPMI only)\n",
    "plot_classification(axes[3][0], ppmi_parkinson_results, ppmi_sizes, ppmi_models, None, ylabel='Balanced Accuracy')\n",
    "axes[3][1].axis('off')\n",
    "axes[3][0].text(-0.15, 0.5, 'Parkinson Classification', transform=axes[3][0].transAxes, fontsize=14, weight='bold', va='center', ha='center', rotation=90)\n",
    "\n",
    "# Add subplot labels (a-g)\n",
    "subplot_labels = ['(a)', '(b)', '(c)', '(d)', '(e)', '(f)', '(g)']\n",
    "label_idx = 0\n",
    "for row in range(4):\n",
    "    for col in range(2):\n",
    "        if row == 3 and col == 1:\n",
    "            continue\n",
    "        axes[row][col].text(-0.08, 1.05, subplot_labels[label_idx], transform=axes[row][col].transAxes, \n",
    "                           fontsize=14, weight='bold', va='top', ha='left')\n",
    "        label_idx += 1\n",
    "\n",
    "# Create global legends at bottom\n",
    "legend_ax_ppmi = fig.add_subplot(gs[4, 0])\n",
    "legend_ax_hbn = fig.add_subplot(gs[4, 1])\n",
    "legend_ax_ppmi.axis('off')\n",
    "legend_ax_hbn.axis('off')\n",
    "\n",
    "# PPMI legend handles\n",
    "ppmi_handles = []\n",
    "for model in ppmi_models:\n",
    "    style = model_styles[model]\n",
    "    lw = style.get('linewidth', 1.5)\n",
    "    ms = style.get('markersize', 4)\n",
    "    line = Line2D([0], [0], marker=style['marker'], markersize=ms, linewidth=lw, \n",
    "                  label=style['label'], color=style['color'], markerfacecolor=style['color'])\n",
    "    ppmi_handles.append(line)\n",
    "ppmi_handles.append(Line2D([0], [0], color='gray', linestyle=':', linewidth=2, label='Baseline'))\n",
    "\n",
    "legend_ax_ppmi.legend(handles=ppmi_handles, loc='center', ncol=5, fontsize=8, frameon=True)\n",
    "\n",
    "# HBN legend handles\n",
    "hbn_handles = []\n",
    "for model in hbn_models:\n",
    "    style = model_styles[model]\n",
    "    lw = style.get('linewidth', 1.5)\n",
    "    ms = style.get('markersize', 4)\n",
    "    line = Line2D([0], [0], marker=style['marker'], markersize=ms, linewidth=lw, \n",
    "                  label=style['label'], color=style['color'], markerfacecolor=style['color'])\n",
    "    hbn_handles.append(line)\n",
    "hbn_handles.append(Line2D([0], [0], color='gray', linestyle=':', linewidth=2, label='Baseline'))\n",
    "\n",
    "legend_ax_hbn.legend(handles=hbn_handles, loc='center', ncol=5, fontsize=7, frameon=True)\n",
    "\n",
    "plt.savefig('combined_learning_curves.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: combined_learning_curves.pdf\")\n",
    "\n",
    "# Correlation matrices\n",
    "plot_correlation_with_labels(ppmi_corr_matrix, ppmi_boundaries, ppmi_model_names, ppmi_cluster_dfs, ['FreeSurfer'], \"ppmi\")\n",
    "plot_correlation_with_labels(hbn_corr_matrix, hbn_boundaries, hbn_model_names, hbn_cluster_dfs, ['FS_Schaefer', 'FS_aparc'], \"hbn\")\n",
    "\n",
    "# Save CSVs\n",
    "ppmi_cluster_dfs['FreeSurfer'].to_csv('ppmi_freesurfer_features_reordered.csv', index=False)\n",
    "hbn_cluster_dfs['FS_Schaefer'].to_csv('hbn_schaefer_freesurfer_features_reordered.csv', index=False)\n",
    "hbn_cluster_dfs['FS_aparc'].to_csv('hbn_aparc_freesurfer_features_reordered.csv', index=False)\n",
    "\n",
    "# Export CV results to CSV\n",
    "\n",
    "def export_cv_results_classification(results, training_sizes, dataset_name, task_name, models_list):\n",
    "    rows = []\n",
    "    for model in models_list:\n",
    "        if model not in results or 'Dummy' in model:\n",
    "            continue\n",
    "        cv_res = results[model]['cv_results']\n",
    "        for i, alpha in enumerate(alphas):\n",
    "            if alpha in cv_res['acc']:\n",
    "                rows.append({\n",
    "                    'dataset': dataset_name,\n",
    "                    'task': task_name,\n",
    "                    'model': model,\n",
    "                    'alpha': alpha,\n",
    "                    'n_train': training_sizes[i],\n",
    "                    'val_acc_mean': cv_res['acc'][alpha],\n",
    "                    'val_acc_std': cv_res['acc_std'][alpha],\n",
    "                    'val_auc_mean': cv_res['auc'].get(alpha, np.nan),\n",
    "                    'val_auc_std': cv_res['auc_std'].get(alpha, np.nan),\n",
    "                    'train_acc_mean': cv_res.get('train_acc', {}).get(alpha, np.nan),\n",
    "                    'train_acc_std': cv_res.get('train_acc_std', {}).get(alpha, np.nan),\n",
    "                    'train_auc_mean': cv_res.get('train_auc', {}).get(alpha, np.nan),\n",
    "                    'train_auc_std': cv_res.get('train_auc_std', {}).get(alpha, np.nan),\n",
    "                    'test_acc_mean': results[model]['test_acc_mean'],\n",
    "                    'test_acc_std': results[model]['test_acc_std'],\n",
    "                    'test_auc_mean': results[model]['test_auc_mean'],\n",
    "                    'test_auc_std': results[model]['test_auc_std'],\n",
    "                })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def export_cv_results_regression(results, training_sizes, dataset_name, task_name, models_list):\n",
    "    rows = []\n",
    "    for model in models_list:\n",
    "        if model not in results or 'Dummy' in model:\n",
    "            continue\n",
    "        cv_res = results[model]['cv_results']\n",
    "        for i, alpha in enumerate(alphas):\n",
    "            if alpha in cv_res['mae']:\n",
    "                rows.append({\n",
    "                    'dataset': dataset_name,\n",
    "                    'task': task_name,\n",
    "                    'model': model,\n",
    "                    'alpha': alpha,\n",
    "                    'n_train': training_sizes[i],\n",
    "                    'val_mae_mean': cv_res['mae'][alpha],\n",
    "                    'val_mae_std': cv_res['mae_std'][alpha],\n",
    "                    'val_r2_mean': cv_res['r2'].get(alpha, np.nan),\n",
    "                    'val_r2_std': cv_res['r2_std'].get(alpha, np.nan),\n",
    "                    'train_mae_mean': cv_res.get('train_mae', {}).get(alpha, np.nan),\n",
    "                    'train_mae_std': cv_res.get('train_mae_std', {}).get(alpha, np.nan),\n",
    "                    'train_r2_mean': cv_res.get('train_r2', {}).get(alpha, np.nan),\n",
    "                    'train_r2_std': cv_res.get('train_r2_std', {}).get(alpha, np.nan),\n",
    "                    'test_mae_mean': results[model]['test_mae_mean'],\n",
    "                    'test_mae_std': results[model]['test_mae_std'],\n",
    "                    'test_r2_mean': results[model]['test_r2_mean'],\n",
    "                    'test_r2_std': results[model]['test_r2_std'],\n",
    "                })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# PPMI\n",
    "ppmi_sex_cv_df = export_cv_results_classification(ppmi_sex_results, ppmi_sizes, 'PPMI', 'Sex', ppmi_models)\n",
    "ppmi_park_cv_df = export_cv_results_classification(ppmi_parkinson_results, ppmi_sizes, 'PPMI', 'Parkinson', ppmi_models)\n",
    "ppmi_male_age_cv_df = export_cv_results_regression(ppmi_male_age_results, ppmi_sizes, 'PPMI', 'Male_Age', ppmi_models)\n",
    "ppmi_female_age_cv_df = export_cv_results_regression(ppmi_female_age_results, ppmi_sizes, 'PPMI', 'Female_Age', ppmi_models)\n",
    "\n",
    "# HBN\n",
    "hbn_sex_cv_df = export_cv_results_classification(hbn_sex_results, hbn_sizes, 'HBN', 'Sex', hbn_models)\n",
    "hbn_male_age_cv_df = export_cv_results_regression(hbn_male_age_results, hbn_sizes, 'HBN', 'Male_Age', hbn_models)\n",
    "hbn_female_age_cv_df = export_cv_results_regression(hbn_female_age_results, hbn_sizes, 'HBN', 'Female_Age', hbn_models)\n",
    "\n",
    "# Combined\n",
    "ppmi_classification_cv = pd.concat([ppmi_sex_cv_df, ppmi_park_cv_df], ignore_index=True)\n",
    "ppmi_regression_cv = pd.concat([ppmi_male_age_cv_df, ppmi_female_age_cv_df], ignore_index=True)\n",
    "hbn_classification_cv = hbn_sex_cv_df\n",
    "hbn_regression_cv = pd.concat([hbn_male_age_cv_df, hbn_female_age_cv_df], ignore_index=True)\n",
    "\n",
    "ppmi_classification_cv.to_csv('ppmi_classification_cv_results.csv', index=False)\n",
    "ppmi_regression_cv.to_csv('ppmi_regression_cv_results.csv', index=False)\n",
    "hbn_classification_cv.to_csv('hbn_classification_cv_results.csv', index=False)\n",
    "hbn_regression_cv.to_csv('hbn_regression_cv_results.csv', index=False)\n",
    "\n",
    "# All combined\n",
    "all_cv_results = pd.concat([\n",
    "    ppmi_classification_cv.assign(task_type='classification'),\n",
    "    ppmi_regression_cv.assign(task_type='regression'),\n",
    "    hbn_classification_cv.assign(task_type='classification'),\n",
    "    hbn_regression_cv.assign(task_type='regression')\n",
    "], ignore_index=True)\n",
    "all_cv_results.to_csv('all_cv_results.csv', index=False)\n",
    "\n",
    "print(\"Saved: ppmi_classification_cv_results.csv\")\n",
    "print(\"Saved: ppmi_regression_cv_results.csv\")\n",
    "print(\"Saved: hbn_classification_cv_results.csv\")\n",
    "print(\"Saved: hbn_regression_cv_results.csv\")\n",
    "print(\"Saved: all_cv_results.csv\")\n",
    "\n",
    "print(f\"\\nDone - PPMI: {len(ppmi_common_subjects)} subjects, HBN: {len(hbn_common_subjects)} subjects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfad9328-9d68-421c-9c9c-d73e2a9f586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "CSV_PATH = '/home/arelbaha/links/projects/rrg-glatard/arelbaha/all_cv_results.csv'\n",
    "OUTPUT_DIR = '/home/arelbaha/links/projects/rrg-glatard/arelbaha/'\n",
    "\n",
    "# Actual dataset sizes from your paper\n",
    "PPMI_TOTAL = 899\n",
    "PPMI_MALE = 559\n",
    "PPMI_FEMALE = 340\n",
    "\n",
    "HBN_TOTAL = 1000\n",
    "HBN_MALE = 519\n",
    "HBN_FEMALE = 481\n",
    "\n",
    "ALPHAS = [0.01, 0.05, 0.1, 0.15, 0.25, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "def compute_training_sizes(n_total):\n",
    "    \"\"\"Compute training sizes for each alpha value.\"\"\"\n",
    "    n_cv = n_total - int(0.1 * n_total)  # 90% for CV\n",
    "    return {alpha: int(alpha * n_cv * 0.8) for alpha in ALPHAS}\n",
    "\n",
    "# Precompute all training size mappings\n",
    "TRAINING_SIZES = {\n",
    "    ('PPMI', 'Sex'): compute_training_sizes(PPMI_TOTAL),\n",
    "    ('PPMI', 'Parkinson'): compute_training_sizes(PPMI_TOTAL),\n",
    "    ('PPMI', 'Male_Age'): compute_training_sizes(PPMI_MALE),\n",
    "    ('PPMI', 'Female_Age'): compute_training_sizes(PPMI_FEMALE),\n",
    "    ('HBN', 'Sex'): compute_training_sizes(HBN_TOTAL),\n",
    "    ('HBN', 'Male_Age'): compute_training_sizes(HBN_MALE),\n",
    "    ('HBN', 'Female_Age'): compute_training_sizes(HBN_FEMALE),\n",
    "}\n",
    "\n",
    "# Test set sizes (for plotting the rightmost point)\n",
    "TEST_SIZES = {\n",
    "    ('PPMI', 'Sex'): int(PPMI_TOTAL * 0.9),\n",
    "    ('PPMI', 'Parkinson'): int(PPMI_TOTAL * 0.9),\n",
    "    ('PPMI', 'Male_Age'): int(PPMI_MALE * 0.9),\n",
    "    ('PPMI', 'Female_Age'): int(PPMI_FEMALE * 0.9),\n",
    "    ('HBN', 'Sex'): int(HBN_TOTAL * 0.9),\n",
    "    ('HBN', 'Male_Age'): int(HBN_MALE * 0.9),\n",
    "    ('HBN', 'Female_Age'): int(HBN_FEMALE * 0.9),\n",
    "}\n",
    "\n",
    "\n",
    "def get_significance_stars(p):\n",
    "    if p < 0.001:\n",
    "        return '***'\n",
    "    elif p < 0.01:\n",
    "        return '**'\n",
    "    elif p < 0.05:\n",
    "        return '*'\n",
    "    return ''\n",
    "\n",
    "def get_significance_daggers(p):\n",
    "    if p < 0.001:\n",
    "        return '†††'\n",
    "    elif p < 0.01:\n",
    "        return '††'\n",
    "    elif p < 0.05:\n",
    "        return '†'\n",
    "    return ''\n",
    "\n",
    "\n",
    "ppmi_main = ['AnatCL', 'BrainIAC', 'SwinBrain', 'CNN', 'FreeSurfer']\n",
    "ppmi_ablation = ['AnatCL+FS', 'BrainIAC+FS', 'SwinBrain+FS', 'FM+FS', 'FreeSurfer']\n",
    "\n",
    "hbn_main = ['AnatCL', 'BrainIAC', 'SwinBrain', 'CNN', 'FS_Schaefer', 'FS_aparc']\n",
    "hbn_ablation = [\n",
    "    'AnatCL+Schaefer', 'BrainIAC+Schaefer', 'SwinBrain+Schaefer',\n",
    "    'AnatCL+aparc', 'BrainIAC+aparc', 'SwinBrain+aparc',\n",
    "    'FM+FS_Schaefer', 'FM+FS_aparc', 'FS_Schaefer', 'FS_aparc'\n",
    "]\n",
    "\n",
    "model_styles = {\n",
    "    # Foundation models - each with unique color and marker\n",
    "    'AnatCL': {'color': '#9B59B6', 'marker': 'o', 'label': 'AnatCL', 'linestyle': '-'},  # Purple, circle\n",
    "    'BrainIAC': {'color': '#E67E22', 'marker': 's', 'label': 'BrainIAC', 'linestyle': '-'},  # Orange, square\n",
    "    'SwinBrain': {'color': '#1ABC9C', 'marker': '^', 'label': 'SwinBrain', 'linestyle': '-'},  # Teal, triangle\n",
    "    'CNN': {'color': '#3498DB', 'marker': 'D', 'label': 'CNN', 'linestyle': '-'},  # Blue, diamond\n",
    "    \n",
    "    # FreeSurfer models - red, dashed, no markers\n",
    "    'FreeSurfer': {'color': '#E74C3C', 'marker': None, 'label': 'FreeSurfer', 'linestyle': '--'},\n",
    "    'FS_Schaefer': {'color': '#E74C3C', 'marker': None, 'label': 'FS: Schaefer', 'linestyle': '--'},\n",
    "    'FS_aparc': {'color': '#C0392B', 'marker': None, 'label': 'FS: aparc', 'linestyle': ':'},  # Darker red, dotted\n",
    "    \n",
    "    # Ablation models - same base color as their foundation model + FS suffix\n",
    "    'AnatCL+FS': {'color': '#9B59B6', 'marker': 'o', 'label': 'AnatCL+FS', 'linestyle': '-'},\n",
    "    'BrainIAC+FS': {'color': '#E67E22', 'marker': 's', 'label': 'BrainIAC+FS', 'linestyle': '-'},\n",
    "    'SwinBrain+FS': {'color': '#1ABC9C', 'marker': '^', 'label': 'SwinBrain+FS', 'linestyle': '-'},\n",
    "    'AnatCL+Schaefer': {'color': '#9B59B6', 'marker': 'o', 'label': 'AnatCL+Schaefer', 'linestyle': '-'},\n",
    "    'BrainIAC+Schaefer': {'color': '#E67E22', 'marker': 's', 'label': 'BrainIAC+Schaefer', 'linestyle': '-'},\n",
    "    'SwinBrain+Schaefer': {'color': '#1ABC9C', 'marker': '^', 'label': 'SwinBrain+Schaefer', 'linestyle': '-'},\n",
    "    'AnatCL+aparc': {'color': '#9B59B6', 'marker': 'o', 'label': 'AnatCL+aparc', 'linestyle': '-'},\n",
    "    'BrainIAC+aparc': {'color': '#E67E22', 'marker': 's', 'label': 'BrainIAC+aparc', 'linestyle': '-'},\n",
    "    'SwinBrain+aparc': {'color': '#1ABC9C', 'marker': '^', 'label': 'SwinBrain+aparc', 'linestyle': '-'},\n",
    "    'FM+FS': {'color': '#2C3E50', 'marker': '*', 'label': 'FM+FS', 'linestyle': '-'},  # Dark gray, star\n",
    "    'FM+FS_Schaefer': {'color': '#2C3E50', 'marker': '*', 'label': 'FM+FS: Schaefer', 'linestyle': '-'},\n",
    "    'FM+FS_aparc': {'color': '#34495E', 'marker': 'P', 'label': 'FM+FS: aparc', 'linestyle': '-'}  # Plus marker\n",
    "}\n",
    "\n",
    "BASELINES = {\n",
    "    'PPMI': {'classification': 0.5, 'mae': 8.0},\n",
    "    'HBN': {'classification': 0.5, 'mae': 3.0}\n",
    "}\n",
    "\n",
    "\n",
    "def _entry(diff, p_raw, p_corr):\n",
    "    return {\n",
    "        'diff': float(diff),\n",
    "        'p_raw': float(p_raw),\n",
    "        'p_corr': float(p_corr),\n",
    "        'significant': float(p_corr) < 0.05\n",
    "    }\n",
    "\n",
    "# PPMI: Sex (permutation vs baseline)\n",
    "ppmi_sex_perm = {\n",
    "    'AnatCL': _entry(0.0215, 0.0062, 0.0496),\n",
    "    'BrainIAC': _entry(0.0264, 0.0001, 0.0008),\n",
    "    'SwinBrain': _entry(0.0648, 0.0001, 0.0008),\n",
    "    'CNN': _entry(0.0621, 0.0001, 0.0008),\n",
    "    'AnatCL+FS': _entry(0.0406, 0.0001, 0.0008),\n",
    "    'BrainIAC+FS': _entry(0.0275, 0.0001, 0.0008),\n",
    "    'SwinBrain+FS': _entry(0.0626, 0.0001, 0.0008),\n",
    "    'FM+FS': _entry(0.0708, 0.0001, 0.0008)\n",
    "}\n",
    "\n",
    "# PPMI: Male Age (MAE)\n",
    "ppmi_male_age_perm = {\n",
    "    'AnatCL': _entry(-0.6145, 0.0001, 0.0006),\n",
    "    'CNN': _entry(-0.0285, 0.3467, 1.0000),\n",
    "    'AnatCL+FS': _entry(-0.6740, 0.0001, 0.0006),\n",
    "    'BrainIAC+FS': _entry(-0.0600, 0.0961, 0.5766),\n",
    "    'SwinBrain+FS': _entry(-0.2941, 0.0001, 0.0006),\n",
    "    'FM+FS': _entry(-0.7054, 0.0001, 0.0006)\n",
    "}\n",
    "\n",
    "# PPMI: Female Age (MAE)\n",
    "ppmi_female_age_perm = {\n",
    "    'AnatCL': _entry(-0.2462, 0.0009, 0.0054),\n",
    "    'SwinBrain': _entry(-0.0703, 0.2061, 1.0000),\n",
    "    'CNN': _entry(-0.0932, 0.1396, 0.8376),\n",
    "    'AnatCL+FS': _entry(-0.3689, 0.0001, 0.0006),\n",
    "    'SwinBrain+FS': _entry(-0.3046, 0.0002, 0.0012),\n",
    "    'FM+FS': _entry(-0.3964, 0.0001, 0.0006)\n",
    "}\n",
    "\n",
    "# HBN: Sex vs Schaefer\n",
    "hbn_sex_perm_schaefer = {\n",
    "    'SwinBrain': _entry(0.0054, 0.1375, 0.9625),\n",
    "    'AnatCL+Schaefer': _entry(0.0098, 0.0055, 0.0385),\n",
    "    'SwinBrain+Schaefer': _entry(0.0142, 0.0026, 0.0182),\n",
    "    'AnatCL+aparc': _entry(0.0008, 0.4379, 1.0000),\n",
    "    'SwinBrain+aparc': _entry(0.0105, 0.0206, 0.1442),\n",
    "    'FM+FS_Schaefer': _entry(0.0098, 0.0170, 0.1190),\n",
    "    'FM+FS_aparc': _entry(0.0040, 0.1717, 1.0000)\n",
    "}\n",
    "\n",
    "# HBN: Sex vs aparc\n",
    "hbn_sex_perm_aparc = {\n",
    "    'SwinBrain': _entry(0.0088, 0.0476, 0.3808),\n",
    "    'FS_Schaefer': _entry(0.0034, 0.1453, 1.0000),\n",
    "    'AnatCL+Schaefer': _entry(0.0133, 0.0007, 0.0056),\n",
    "    'SwinBrain+Schaefer': _entry(0.0176, 0.0012, 0.0096),\n",
    "    'AnatCL+aparc': _entry(0.0042, 0.1959, 1.0000),\n",
    "    'SwinBrain+aparc': _entry(0.0139, 0.0030, 0.0240),\n",
    "    'FM+FS_Schaefer': _entry(0.0132, 0.0044, 0.0352),\n",
    "    'FM+FS_aparc': _entry(0.0074, 0.0648, 0.5184)\n",
    "}\n",
    "\n",
    "# HBN: Male Age vs Schaefer\n",
    "hbn_male_perm_schaefer = {\n",
    "    'AnatCL': _entry(-0.0184, 0.2606, 1.0000),\n",
    "    'SwinBrain': _entry(-0.2993, 0.0001, 0.0012),\n",
    "    'CNN': _entry(-0.2845, 0.0001, 0.0012),\n",
    "    'FS_aparc': _entry(-0.0303, 0.0674, 0.8088),\n",
    "    'AnatCL+Schaefer': _entry(-0.1702, 0.0001, 0.0012),\n",
    "    'BrainIAC+Schaefer': _entry(-0.1394, 0.0001, 0.0012),\n",
    "    'SwinBrain+Schaefer': _entry(-0.3747, 0.0001, 0.0012),\n",
    "    'AnatCL+aparc': _entry(-0.1185, 0.0001, 0.0012),\n",
    "    'BrainIAC+aparc': _entry(-0.0877, 0.0003, 0.0036),\n",
    "    'SwinBrain+aparc': _entry(-0.4141, 0.0001, 0.0012),\n",
    "    'FM+FS_Schaefer': _entry(-0.4338, 0.0001, 0.0012),\n",
    "    'FM+FS_aparc': _entry(-0.4498, 0.0001, 0.0012)\n",
    "}\n",
    "\n",
    "# HBN: Male Age vs aparc\n",
    "hbn_male_perm_aparc = {\n",
    "    'SwinBrain': _entry(-0.2689, 0.0001, 0.0010),\n",
    "    'CNN': _entry(-0.2542, 0.0001, 0.0010),\n",
    "    'AnatCL+Schaefer': _entry(-0.1399, 0.0001, 0.0010),\n",
    "    'BrainIAC+Schaefer': _entry(-0.1091, 0.0002, 0.0020),\n",
    "    'SwinBrain+Schaefer': _entry(-0.3444, 0.0001, 0.0010),\n",
    "    'AnatCL+aparc': _entry(-0.0882, 0.0018, 0.0180),\n",
    "    'BrainIAC+aparc': _entry(-0.0574, 0.0192, 0.1920),\n",
    "    'SwinBrain+aparc': _entry(-0.3838, 0.0001, 0.0010),\n",
    "    'FM+FS_Schaefer': _entry(-0.4035, 0.0001, 0.0010),\n",
    "    'FM+FS_aparc': _entry(-0.4195, 0.0001, 0.0010)\n",
    "}\n",
    "\n",
    "# HBN: Female Age vs Schaefer\n",
    "hbn_female_perm_schaefer = {\n",
    "    'AnatCL': _entry(-0.0649, 0.0463, 0.5556),\n",
    "    'SwinBrain': _entry(-0.1101, 0.0081, 0.0972),\n",
    "    'CNN': _entry(-0.0728, 0.0710, 0.8520),\n",
    "    'FS_aparc': _entry(-0.1351, 0.0001, 0.0012),\n",
    "    'AnatCL+Schaefer': _entry(-0.2236, 0.0001, 0.0012),\n",
    "    'BrainIAC+Schaefer': _entry(-0.0422, 0.0029, 0.0348),\n",
    "    'SwinBrain+Schaefer': _entry(-0.2443, 0.0001, 0.0012),\n",
    "    'AnatCL+aparc': _entry(-0.1771, 0.0002, 0.0024),\n",
    "    'BrainIAC+aparc': _entry(-0.0276, 0.1697, 1.0000),\n",
    "    'SwinBrain+aparc': _entry(-0.2146, 0.0001, 0.0012),\n",
    "    'FM+FS_Schaefer': _entry(-0.4295, 0.0001, 0.0012),\n",
    "    'FM+FS_aparc': _entry(-0.4314, 0.0001, 0.0012)\n",
    "}\n",
    "\n",
    "# HBN: Female Age vs aparc\n",
    "hbn_female_perm_aparc = {\n",
    "    'AnatCL+Schaefer': _entry(-0.0885, 0.0007, 0.0042),\n",
    "    'SwinBrain+Schaefer': _entry(-0.1092, 0.0005, 0.0030),\n",
    "    'AnatCL+aparc': _entry(-0.0421, 0.0580, 0.3480),\n",
    "    'SwinBrain+aparc': _entry(-0.0796, 0.0043, 0.0258),\n",
    "    'FM+FS_Schaefer': _entry(-0.2944, 0.0001, 0.0006),\n",
    "    'FM+FS_aparc': _entry(-0.2963, 0.0001, 0.0006)\n",
    "}\n",
    "\n",
    "\n",
    "def get_correct_n_train(df_row, dataset, task):\n",
    "    \"\"\"Map the alpha value to the correct training size for this task.\"\"\"\n",
    "    alpha = float(df_row['alpha'])\n",
    "    size_map = TRAINING_SIZES.get((dataset, task))\n",
    "    if size_map is None:\n",
    "        return df_row['n_train']  # fallback to original\n",
    "    # Find closest alpha\n",
    "    closest_alpha = min(ALPHAS, key=lambda a: abs(a - alpha))\n",
    "    return size_map[closest_alpha]\n",
    "\n",
    "\n",
    "def plot_classification(ax, df, dataset, task, models, perm=None, sch=None, apc=None, ylabel=None):\n",
    "    sub = df[(df.dataset == dataset) & (df.task == task)].copy()\n",
    "    \n",
    "    # Recalculate correct n_train values\n",
    "    sub['n_train_corrected'] = sub.apply(lambda r: get_correct_n_train(r, dataset, task), axis=1)\n",
    "    \n",
    "    test_x = TEST_SIZES.get((dataset, task), int(900 * 0.9))\n",
    "    handles, labels = [], []\n",
    "    max_n_train = sub.n_train_corrected.max()\n",
    "    \n",
    "    # Collect boxplot data for inset\n",
    "    boxplot_data = []\n",
    "    boxplot_labels = []\n",
    "    boxplot_colors = []\n",
    "    boxplot_significance = []  # Store significance markers for each model\n",
    "\n",
    "    for m in models:\n",
    "        if m not in model_styles:\n",
    "            continue\n",
    "        mdf = sub[sub.model == m].sort_values('n_train_corrected')\n",
    "        if mdf.empty:\n",
    "            continue\n",
    "\n",
    "        s = model_styles[m]\n",
    "        linestyle = s.get('linestyle', '-')\n",
    "        marker = s.get('marker')\n",
    "        \n",
    "        line, = ax.plot(\n",
    "            mdf.n_train_corrected, mdf.val_acc_mean,\n",
    "            marker=marker, color=s['color'],\n",
    "            linewidth=1.5, markersize=4 if marker else 0,\n",
    "            linestyle=linestyle\n",
    "        )\n",
    "        # Plot test point (no marker for FreeSurfer models)\n",
    "        if marker:\n",
    "            ax.plot(test_x, mdf.test_acc_mean.iloc[0],\n",
    "                    marker=marker, color=s['color'], markersize=4,\n",
    "                    linestyle='None')\n",
    "        else:\n",
    "            ax.plot(test_x, mdf.test_acc_mean.iloc[0],\n",
    "                    marker='x', color=s['color'], markersize=6,\n",
    "                    linestyle='None')\n",
    "        \n",
    "        # Collect boxplot data for inset\n",
    "        max_row = mdf[mdf.n_train_corrected == max_n_train]\n",
    "        if not max_row.empty and 'val_acc_std' in max_row.columns:\n",
    "            mean_val = max_row.val_acc_mean.iloc[0]\n",
    "            std_val = max_row.val_acc_std.iloc[0]\n",
    "            q1 = mean_val - 0.675 * std_val\n",
    "            q3 = mean_val + 0.675 * std_val\n",
    "            whislo = mean_val - 1.5 * std_val\n",
    "            whishi = mean_val + 1.5 * std_val\n",
    "            \n",
    "            box_stats = [{\n",
    "                'med': mean_val,\n",
    "                'q1': q1,\n",
    "                'q3': q3,\n",
    "                'whislo': max(0, whislo),\n",
    "                'whishi': min(1, whishi),\n",
    "                'fliers': []\n",
    "            }]\n",
    "            \n",
    "            boxplot_data.append(box_stats[0])\n",
    "            boxplot_labels.append(s['label'])\n",
    "            boxplot_colors.append(s['color'])\n",
    "            \n",
    "            # Collect significance for this model's boxplot\n",
    "            sig_text = ''\n",
    "            if dataset == 'PPMI' and perm and m in perm and perm[m]['significant']:\n",
    "                sig_text = get_significance_stars(perm[m]['p_corr'])\n",
    "            if dataset == 'HBN':\n",
    "                if sch and m in sch and sch[m]['significant']:\n",
    "                    sig_text += get_significance_stars(sch[m]['p_corr'])\n",
    "                if apc and m in apc and apc[m]['significant']:\n",
    "                    sig_text += get_significance_daggers(apc[m]['p_corr'])\n",
    "            boxplot_significance.append(sig_text)\n",
    "\n",
    "        lbl = s['label']\n",
    "\n",
    "        handles.append(line)\n",
    "        labels.append(lbl)\n",
    "\n",
    "    ax.axhline(BASELINES[dataset]['classification'], color='gray', linestyle=':', linewidth=2)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Set custom x-axis ticks with bold for largest sample\n",
    "    tick_interval = 100\n",
    "    x_ticks = list(range(0, int(max_n_train) + tick_interval, tick_interval))\n",
    "    x_ticks = [t for t in x_ticks if abs(t - max_n_train) > 25 or t == 0]\n",
    "    x_ticks.append(int(max_n_train))\n",
    "    x_ticks = sorted(x_ticks)\n",
    "    ax.set_xticks(x_ticks)\n",
    "    \n",
    "    tick_labels = []\n",
    "    for tick in x_ticks:\n",
    "        if tick == int(max_n_train):\n",
    "            tick_labels.append(f'$\\\\mathbf{{{tick}}}$')\n",
    "        else:\n",
    "            tick_labels.append(f'{tick}')\n",
    "    ax.set_xticklabels(tick_labels)\n",
    "    \n",
    "    ax.set_xlabel('Number of Training Subjects', fontsize=12, weight='bold')\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel, fontsize=12, weight='bold')\n",
    "    \n",
    "    main_legend = ax.legend(handles, labels, fontsize=6, loc='lower left', frameon=True)\n",
    "    \n",
    "    if dataset == 'HBN':\n",
    "        from matplotlib.patches import Patch\n",
    "        sig_handles = [\n",
    "            Patch(facecolor='none', edgecolor='none', label='* = vs Schaefer'),\n",
    "            Patch(facecolor='none', edgecolor='none', label='† = vs aparc')\n",
    "        ]\n",
    "        ax.legend(handles=sig_handles, fontsize=6, loc='upper right', frameon=True, \n",
    "                 handlelength=0, handletextpad=0)\n",
    "        ax.add_artist(main_legend)\n",
    "    \n",
    "    # Create inset for boxplots - position at bottom, sized to avoid collision\n",
    "    if boxplot_data:\n",
    "        ax_inset = ax.inset_axes([0.42, 0.02, 0.48, 0.365])  # Compact, avoid learning curves\n",
    "        positions = list(range(1, len(boxplot_data) + 1))\n",
    "        \n",
    "        bp = ax_inset.bxp(boxplot_data, positions=positions, widths=0.6,\n",
    "                          patch_artist=True, showfliers=False,\n",
    "                          boxprops={'linewidth': 1.0},\n",
    "                          whiskerprops={'linewidth': 0.8},\n",
    "                          capprops={'linewidth': 0.8},\n",
    "                          medianprops={'color': 'black', 'linewidth': 1.0})\n",
    "        \n",
    "        for patch, color in zip(bp['boxes'], boxplot_colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_edgecolor(color)\n",
    "            patch.set_alpha(0.5)\n",
    "        \n",
    "        ax_inset.set_xticks([])\n",
    "        ax_inset.set_ylabel('Accuracy', fontsize=7)\n",
    "        ax_inset.grid(alpha=0.2, axis='y')\n",
    "        ax_inset.tick_params(labelsize=5, pad=0)\n",
    "        ax_inset.margins(x=0.08)\n",
    "        \n",
    "        # Find reference positions for brackets\n",
    "        fs_idx = None  # FreeSurfer or FS_Schaefer\n",
    "        fs_aparc_idx = None  # FS_aparc (HBN only)\n",
    "        for i, lbl in enumerate(boxplot_labels):\n",
    "            if lbl == 'FreeSurfer' or lbl == 'FS: Schaefer':\n",
    "                fs_idx = i\n",
    "            if lbl == 'FS: aparc':\n",
    "                fs_aparc_idx = i\n",
    "        \n",
    "        # Collect comparisons - separate stars and daggers\n",
    "        star_comparisons = []  # Connect to fs_idx\n",
    "        dagger_comparisons = []  # Connect to fs_aparc_idx\n",
    "        \n",
    "        for i, (pos, box_stat, sig_text) in enumerate(zip(positions, boxplot_data, boxplot_significance)):\n",
    "            if not sig_text or i == fs_idx or i == fs_aparc_idx:\n",
    "                continue\n",
    "            stars = ''.join(c for c in sig_text if c == '*')\n",
    "            daggers = ''.join(c for c in sig_text if c == '†')\n",
    "            \n",
    "            if stars and fs_idx is not None:\n",
    "                distance = abs(pos - positions[fs_idx])\n",
    "                star_comparisons.append((distance, pos, stars))\n",
    "            if daggers and fs_aparc_idx is not None:\n",
    "                distance = abs(pos - positions[fs_aparc_idx])\n",
    "                dagger_comparisons.append((distance, pos, daggers))\n",
    "        \n",
    "        star_comparisons.sort(key=lambda x: x[0])\n",
    "        dagger_comparisons.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Calculate y-limits to fit all brackets inside\n",
    "        n_brackets = len(star_comparisons) + len(dagger_comparisons)\n",
    "        max_whishi = max(b['whishi'] for b in boxplot_data)\n",
    "        min_whislo = min(b['whislo'] for b in boxplot_data)\n",
    "        \n",
    "        # More space for brackets\n",
    "        bracket_space = n_brackets * 0.018 + 0.025\n",
    "        y_top = max_whishi + bracket_space\n",
    "        y_bottom = min_whislo - 0.02\n",
    "        ax_inset.set_ylim(y_bottom, y_top)\n",
    "        \n",
    "        # Draw brackets with better spacing\n",
    "        bracket_y = max_whishi + 0.012\n",
    "        tick_len = 0.006\n",
    "        \n",
    "        # Draw star brackets (to FS_Schaefer or FreeSurfer)\n",
    "        if fs_idx is not None:\n",
    "            fs_pos = positions[fs_idx]\n",
    "            for distance, pos, sig_text in star_comparisons:\n",
    "                ax_inset.plot([pos, fs_pos], [bracket_y, bracket_y], color='black', linewidth=0.5, clip_on=True)\n",
    "                ax_inset.plot([pos, pos], [bracket_y - tick_len, bracket_y], color='black', linewidth=0.5, clip_on=True)\n",
    "                ax_inset.plot([fs_pos, fs_pos], [bracket_y - tick_len, bracket_y], color='black', linewidth=0.5, clip_on=True)\n",
    "                mid_x = (pos + fs_pos) / 2\n",
    "                ax_inset.text(mid_x, bracket_y + 0.001, sig_text, fontsize=5, fontweight='bold', ha='center', va='bottom')\n",
    "                bracket_y += 0.018\n",
    "        \n",
    "        # Draw dagger brackets (to FS_aparc)\n",
    "        if fs_aparc_idx is not None:\n",
    "            fs_aparc_pos = positions[fs_aparc_idx]\n",
    "            for distance, pos, sig_text in dagger_comparisons:\n",
    "                ax_inset.plot([pos, fs_aparc_pos], [bracket_y, bracket_y], color='black', linewidth=0.5, clip_on=True)\n",
    "                ax_inset.plot([pos, pos], [bracket_y - tick_len, bracket_y], color='black', linewidth=0.5, clip_on=True)\n",
    "                ax_inset.plot([fs_aparc_pos, fs_aparc_pos], [bracket_y - tick_len, bracket_y], color='black', linewidth=0.5, clip_on=True)\n",
    "                mid_x = (pos + fs_aparc_pos) / 2\n",
    "                ax_inset.text(mid_x, bracket_y + 0.001, sig_text, fontsize=5, fontweight='bold', ha='center', va='bottom')\n",
    "                bracket_y += 0.018\n",
    "\n",
    "\n",
    "def plot_regression(ax, df, dataset, task, models, perm=None, sch=None, apc=None, ylabel=None):\n",
    "    sub = df[(df.dataset == dataset) & (df.task == task)].copy()\n",
    "    \n",
    "    # Recalculate correct n_train values\n",
    "    sub['n_train_corrected'] = sub.apply(lambda r: get_correct_n_train(r, dataset, task), axis=1)\n",
    "    \n",
    "    test_x = TEST_SIZES.get((dataset, task), int(900 * 0.9))\n",
    "    ylim = (0, 4) if dataset == 'HBN' else (4, 10)\n",
    "    max_n_train = sub.n_train_corrected.max()\n",
    "\n",
    "    handles, labels = [], []\n",
    "    \n",
    "    # Collect boxplot data for inset\n",
    "    boxplot_data = []\n",
    "    boxplot_labels = []\n",
    "    boxplot_colors = []\n",
    "    boxplot_significance = []  # Store significance markers for each model\n",
    "\n",
    "    for m in models:\n",
    "        if m not in model_styles:\n",
    "            continue\n",
    "        mdf = sub[sub.model == m].sort_values('n_train_corrected')\n",
    "        if mdf.empty:\n",
    "            continue\n",
    "\n",
    "        s = model_styles[m]\n",
    "        linestyle = s.get('linestyle', '-')\n",
    "        marker = s.get('marker')\n",
    "        \n",
    "        line, = ax.plot(\n",
    "            mdf.n_train_corrected, mdf.val_mae_mean,\n",
    "            marker=marker, color=s['color'],\n",
    "            linewidth=1.5, markersize=4 if marker else 0,\n",
    "            linestyle=linestyle\n",
    "        )\n",
    "        # Plot test point (no marker for FreeSurfer models)\n",
    "        if marker:\n",
    "            ax.plot(test_x, mdf.test_mae_mean.iloc[0],\n",
    "                    marker=marker, color=s['color'], markersize=4,\n",
    "                    linestyle='None')\n",
    "        else:\n",
    "            ax.plot(test_x, mdf.test_mae_mean.iloc[0],\n",
    "                    marker='x', color=s['color'], markersize=6,\n",
    "                    linestyle='None')\n",
    "        \n",
    "        # Collect boxplot data for inset\n",
    "        max_row = mdf[mdf.n_train_corrected == max_n_train]\n",
    "        if not max_row.empty and 'val_mae_std' in max_row.columns:\n",
    "            mean_val = max_row.val_mae_mean.iloc[0]\n",
    "            std_val = max_row.val_mae_std.iloc[0]\n",
    "            q1 = mean_val - 0.675 * std_val\n",
    "            q3 = mean_val + 0.675 * std_val\n",
    "            whislo = mean_val - 1.5 * std_val\n",
    "            whishi = mean_val + 1.5 * std_val\n",
    "            \n",
    "            box_stats = [{\n",
    "                'med': mean_val,\n",
    "                'q1': q1,\n",
    "                'q3': q3,\n",
    "                'whislo': max(ylim[0], whislo),\n",
    "                'whishi': min(ylim[1], whishi),\n",
    "                'fliers': []\n",
    "            }]\n",
    "            \n",
    "            boxplot_data.append(box_stats[0])\n",
    "            boxplot_labels.append(s['label'])\n",
    "            boxplot_colors.append(s['color'])\n",
    "            \n",
    "            # Collect significance for this model's boxplot\n",
    "            sig_text = ''\n",
    "            if dataset == 'PPMI' and perm and m in perm and perm[m]['significant']:\n",
    "                sig_text = get_significance_stars(perm[m]['p_corr'])\n",
    "            if dataset == 'HBN':\n",
    "                if sch and m in sch and sch[m]['significant']:\n",
    "                    sig_text += get_significance_stars(sch[m]['p_corr'])\n",
    "                if apc and m in apc and apc[m]['significant']:\n",
    "                    sig_text += get_significance_daggers(apc[m]['p_corr'])\n",
    "            boxplot_significance.append(sig_text)\n",
    "\n",
    "        lbl = s['label']\n",
    "\n",
    "        handles.append(line)\n",
    "        labels.append(lbl)\n",
    "\n",
    "    ax.axhline(BASELINES[dataset]['mae'], color='gray', linestyle=':', linewidth=2)\n",
    "    ax.set_ylim(*ylim)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Set custom x-axis ticks with bold for largest sample\n",
    "    tick_interval = 100\n",
    "    x_ticks = list(range(0, int(max_n_train) + tick_interval, tick_interval))\n",
    "    x_ticks = [t for t in x_ticks if abs(t - max_n_train) > 25 or t == 0]\n",
    "    x_ticks.append(int(max_n_train))\n",
    "    x_ticks = sorted(x_ticks)\n",
    "    ax.set_xticks(x_ticks)\n",
    "    \n",
    "    tick_labels = []\n",
    "    for tick in x_ticks:\n",
    "        if tick == int(max_n_train):\n",
    "            tick_labels.append(f'$\\\\mathbf{{{tick}}}$')\n",
    "        else:\n",
    "            tick_labels.append(f'{tick}')\n",
    "    ax.set_xticklabels(tick_labels)\n",
    "    \n",
    "    ax.set_xlabel('Number of Training Subjects', fontsize=12, weight='bold')\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel, fontsize=12, weight='bold')\n",
    "    \n",
    "    main_legend = ax.legend(handles, labels, fontsize=6, loc='lower left', frameon=True)\n",
    "    \n",
    "    if dataset == 'HBN':\n",
    "        from matplotlib.patches import Patch\n",
    "        sig_handles = [\n",
    "            Patch(facecolor='none', edgecolor='none', label='* = vs Schaefer'),\n",
    "            Patch(facecolor='none', edgecolor='none', label='† = vs aparc')\n",
    "        ]\n",
    "        ax.legend(handles=sig_handles, fontsize=6, loc='upper right', frameon=True, \n",
    "                 handlelength=0, handletextpad=0)\n",
    "        ax.add_artist(main_legend)\n",
    "    \n",
    "    # Create inset for boxplots - position at bottom, sized to avoid collision\n",
    "    if boxplot_data:\n",
    "        ax_inset = ax.inset_axes([0.42, 0.02, 0.48, 0.365])  # Compact, avoid learning curves\n",
    "        positions = list(range(1, len(boxplot_data) + 1))\n",
    "        \n",
    "        bp = ax_inset.bxp(boxplot_data, positions=positions, widths=0.6,\n",
    "                          patch_artist=True, showfliers=False,\n",
    "                          boxprops={'linewidth': 1.0},\n",
    "                          whiskerprops={'linewidth': 0.8},\n",
    "                          capprops={'linewidth': 0.8},\n",
    "                          medianprops={'color': 'black', 'linewidth': 1.0})\n",
    "        \n",
    "        for patch, color in zip(bp['boxes'], boxplot_colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_edgecolor(color)\n",
    "            patch.set_alpha(0.5)\n",
    "        \n",
    "        ax_inset.set_xticks([])\n",
    "        ax_inset.set_ylabel('MAE', fontsize=7)\n",
    "        ax_inset.grid(alpha=0.2, axis='y')\n",
    "        ax_inset.tick_params(labelsize=5, pad=0)\n",
    "        ax_inset.margins(x=0.08)\n",
    "        \n",
    "        y_range = ylim[1] - ylim[0]\n",
    "        \n",
    "        # Find reference positions for brackets\n",
    "        fs_idx = None  # FreeSurfer or FS_Schaefer\n",
    "        fs_aparc_idx = None  # FS_aparc (HBN only)\n",
    "        for i, lbl in enumerate(boxplot_labels):\n",
    "            if lbl == 'FreeSurfer' or lbl == 'FS: Schaefer':\n",
    "                fs_idx = i\n",
    "            if lbl == 'FS: aparc':\n",
    "                fs_aparc_idx = i\n",
    "        \n",
    "        # Collect comparisons - separate stars and daggers\n",
    "        star_comparisons = []  # Connect to fs_idx\n",
    "        dagger_comparisons = []  # Connect to fs_aparc_idx\n",
    "        \n",
    "        for i, (pos, box_stat, sig_text) in enumerate(zip(positions, boxplot_data, boxplot_significance)):\n",
    "            if not sig_text or i == fs_idx or i == fs_aparc_idx:\n",
    "                continue\n",
    "            stars = ''.join(c for c in sig_text if c == '*')\n",
    "            daggers = ''.join(c for c in sig_text if c == '†')\n",
    "            \n",
    "            if stars and fs_idx is not None:\n",
    "                distance = abs(pos - positions[fs_idx])\n",
    "                star_comparisons.append((distance, pos, stars))\n",
    "            if daggers and fs_aparc_idx is not None:\n",
    "                distance = abs(pos - positions[fs_aparc_idx])\n",
    "                dagger_comparisons.append((distance, pos, daggers))\n",
    "        \n",
    "        star_comparisons.sort(key=lambda x: x[0])\n",
    "        dagger_comparisons.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Calculate y-limits to fit all brackets inside - more space\n",
    "        n_brackets = len(star_comparisons) + len(dagger_comparisons)\n",
    "        max_whishi = max(b['whishi'] for b in boxplot_data)\n",
    "        min_whislo = min(b['whislo'] for b in boxplot_data)\n",
    "        \n",
    "        # More space for brackets\n",
    "        bracket_space = n_brackets * y_range * 0.022 + y_range * 0.03\n",
    "        y_top = max_whishi + bracket_space\n",
    "        y_bottom = min_whislo - y_range * 0.02\n",
    "        ax_inset.set_ylim(y_bottom, y_top)\n",
    "        \n",
    "        # Draw brackets with better spacing\n",
    "        bracket_y = max_whishi + y_range * 0.015\n",
    "        tick_len = y_range * 0.008\n",
    "        \n",
    "        # Draw star brackets (to FS_Schaefer or FreeSurfer)\n",
    "        if fs_idx is not None:\n",
    "            fs_pos = positions[fs_idx]\n",
    "            for distance, pos, sig_text in star_comparisons:\n",
    "                ax_inset.plot([pos, fs_pos], [bracket_y, bracket_y], color='black', linewidth=0.5, clip_on=True)\n",
    "                ax_inset.plot([pos, pos], [bracket_y - tick_len, bracket_y], color='black', linewidth=0.5, clip_on=True)\n",
    "                ax_inset.plot([fs_pos, fs_pos], [bracket_y - tick_len, bracket_y], color='black', linewidth=0.5, clip_on=True)\n",
    "                mid_x = (pos + fs_pos) / 2\n",
    "                ax_inset.text(mid_x, bracket_y + tick_len * 0.1, sig_text, fontsize=5, fontweight='bold', ha='center', va='bottom')\n",
    "                bracket_y += y_range * 0.022\n",
    "        \n",
    "        # Draw dagger brackets (to FS_aparc)\n",
    "        if fs_aparc_idx is not None:\n",
    "            fs_aparc_pos = positions[fs_aparc_idx]\n",
    "            for distance, pos, sig_text in dagger_comparisons:\n",
    "                ax_inset.plot([pos, fs_aparc_pos], [bracket_y, bracket_y], color='black', linewidth=0.5, clip_on=True)\n",
    "                ax_inset.plot([pos, pos], [bracket_y - tick_len, bracket_y], color='black', linewidth=0.5, clip_on=True)\n",
    "                ax_inset.plot([fs_aparc_pos, fs_aparc_pos], [bracket_y - tick_len, bracket_y], color='black', linewidth=0.5, clip_on=True)\n",
    "                mid_x = (pos + fs_aparc_pos) / 2\n",
    "                ax_inset.text(mid_x, bracket_y + tick_len * 0.1, sig_text, fontsize=5, fontweight='bold', ha='center', va='bottom')\n",
    "                bracket_y += y_range * 0.022\n",
    "\n",
    "\n",
    "def create_figure(df, ppmi_models, hbn_models, filename,\n",
    "                  ppmi_sex, ppmi_male, ppmi_female,\n",
    "                  hbn_sex_s, hbn_male_s, hbn_female_s,\n",
    "                  hbn_sex_a, hbn_male_a, hbn_female_a):\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 28))\n",
    "    gs = fig.add_gridspec(\n",
    "        5, 2,\n",
    "        height_ratios=[1, 1, 1, 1, 0.15],\n",
    "        hspace=0.3,\n",
    "        wspace=0.25\n",
    "    )\n",
    "    axes = [[fig.add_subplot(gs[i, j]) for j in range(2)] for i in range(4)]\n",
    "\n",
    "    axes[0][0].set_title('PPMI', fontsize=18, weight='bold', pad=20)\n",
    "    axes[0][1].set_title('HBN', fontsize=18, weight='bold', pad=20)\n",
    "\n",
    "    # Row 0: Sex Classification\n",
    "    plot_classification(\n",
    "        axes[0][0], df, 'PPMI', 'Sex',\n",
    "        ppmi_models, perm=ppmi_sex, ylabel='Balanced Accuracy'\n",
    "    )\n",
    "    plot_classification(\n",
    "        axes[0][1], df, 'HBN', 'Sex',\n",
    "        hbn_models, sch=hbn_sex_s, apc=hbn_sex_a\n",
    "    )\n",
    "\n",
    "    # Row 1: Male Age\n",
    "    plot_regression(\n",
    "        axes[1][0], df, 'PPMI', 'Male_Age',\n",
    "        ppmi_models, perm=ppmi_male, ylabel='MAE (years)'\n",
    "    )\n",
    "    plot_regression(\n",
    "        axes[1][1], df, 'HBN', 'Male_Age',\n",
    "        hbn_models, sch=hbn_male_s, apc=hbn_male_a\n",
    "    )\n",
    "\n",
    "    # Row 2: Female Age\n",
    "    plot_regression(\n",
    "        axes[2][0], df, 'PPMI', 'Female_Age',\n",
    "        ppmi_models, perm=ppmi_female, ylabel='MAE (years)'\n",
    "    )\n",
    "    plot_regression(\n",
    "        axes[2][1], df, 'HBN', 'Female_Age',\n",
    "        hbn_models, sch=hbn_female_s, apc=hbn_female_a\n",
    "    )\n",
    "\n",
    "    # Row 3: Parkinson (PPMI only)\n",
    "    plot_classification(\n",
    "        axes[3][0], df, 'PPMI', 'Parkinson',\n",
    "        ppmi_models, ylabel='Balanced Accuracy'\n",
    "    )\n",
    "    axes[3][1].axis('off')\n",
    "\n",
    "    row_titles = [\n",
    "        'Sex Classification',\n",
    "        'Male Age Prediction',\n",
    "        'Female Age Prediction',\n",
    "        'Parkinson Classification'\n",
    "    ]\n",
    "    for i, title in enumerate(row_titles):\n",
    "        axes[i][0].text(\n",
    "            -0.15, 0.5, title,\n",
    "            transform=axes[i][0].transAxes,\n",
    "            fontsize=14, weight='bold',\n",
    "            va='center', ha='center',\n",
    "            rotation=90\n",
    "        )\n",
    "\n",
    "    subplot_labels = ['(a)', '(b)', '(c)', '(d)', '(e)', '(f)', '(g)']\n",
    "    idx = 0\n",
    "    for r in range(4):\n",
    "        for c in range(2):\n",
    "            if r == 3 and c == 1:  # Skip empty subplot\n",
    "                continue\n",
    "            axes[r][c].text(\n",
    "                -0.08, 1.05, subplot_labels[idx],\n",
    "                transform=axes[r][c].transAxes,\n",
    "                fontsize=14, weight='bold',\n",
    "                va='top', ha='left'\n",
    "            )\n",
    "            idx += 1\n",
    "\n",
    "    plt.savefig(f'{OUTPUT_DIR}{filename}', dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Print the corrected training sizes for verification\n",
    "    print(\"Corrected training sizes:\")\n",
    "    print(f\"\\nPPMI Sex/Parkinson (N={PPMI_TOTAL}):\")\n",
    "    print(f\"  Max training: {TRAINING_SIZES[('PPMI', 'Sex')][1.0]}\")\n",
    "    print(f\"\\nPPMI Male Age (N={PPMI_MALE}):\")\n",
    "    print(f\"  Max training: {TRAINING_SIZES[('PPMI', 'Male_Age')][1.0]}\")\n",
    "    print(f\"\\nPPMI Female Age (N={PPMI_FEMALE}):\")\n",
    "    print(f\"  Max training: {TRAINING_SIZES[('PPMI', 'Female_Age')][1.0]}\")\n",
    "    print(f\"\\nHBN Sex (N={HBN_TOTAL}):\")\n",
    "    print(f\"  Max training: {TRAINING_SIZES[('HBN', 'Sex')][1.0]}\")\n",
    "    print(f\"\\nHBN Male Age (N={HBN_MALE}):\")\n",
    "    print(f\"  Max training: {TRAINING_SIZES[('HBN', 'Male_Age')][1.0]}\")\n",
    "    print(f\"\\nHBN Female Age (N={HBN_FEMALE}):\")\n",
    "    print(f\"  Max training: {TRAINING_SIZES[('HBN', 'Female_Age')][1.0]}\")\n",
    "    \n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    \n",
    "    create_figure(\n",
    "        df, ppmi_main, hbn_main, 'learning_curves_main.pdf',\n",
    "        ppmi_sex_perm, ppmi_male_age_perm, ppmi_female_age_perm,\n",
    "        hbn_sex_perm_schaefer, hbn_male_perm_schaefer, hbn_female_perm_schaefer,\n",
    "        hbn_sex_perm_aparc, hbn_male_perm_aparc, hbn_female_perm_aparc\n",
    "    )\n",
    "    \n",
    "    create_figure(\n",
    "        df, ppmi_ablation, hbn_ablation, 'learning_curves_ablations.pdf',\n",
    "        ppmi_sex_perm, ppmi_male_age_perm, ppmi_female_age_perm,\n",
    "        hbn_sex_perm_schaefer, hbn_male_perm_schaefer, hbn_female_perm_schaefer,\n",
    "        hbn_sex_perm_aparc, hbn_male_perm_aparc, hbn_female_perm_aparc\n",
    "    )\n",
    "    \n",
    "    print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac3ca8-144f-4fe4-ba73-809801c098cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "import subprocess\n",
    "\n",
    "for pdf in ['learning_curves_main.pdf', 'learning_curves_ablations.pdf']:\n",
    "    path = f'{OUTPUT_DIR}{pdf}'\n",
    "    png_path = path.replace('.pdf', '.png')\n",
    "    subprocess.run(['convert', '-density', '150', path, png_path], check=True)\n",
    "    display(Image(png_path))\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
