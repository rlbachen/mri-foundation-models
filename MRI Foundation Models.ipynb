{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b6b822-71f3-49ce-aeed-22c8b4febb7f",
   "metadata": {},
   "source": [
    "# MRI Foundation Models: Pre-Trained Models' Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653b3cf-1728-4a6d-b2aa-f01ea0a5dadb",
   "metadata": {},
   "source": [
    "## SPM/CAT12 Preprocessing | AnatCL | HBN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c2d56-25ab-4b65-a537-a8f2288012f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/links/projects/rrg-glatard/arelbaha/HBN_BIDS/new_cat12_preprocessing.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import boutiques\n",
    "from boutiques import bosh\n",
    "from boutiques.descriptor2func import function\n",
    "\n",
    "boutiques_descriptor = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/descriptors/cat12_prepro.json\"\n",
    "base_dir = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/HBN_BIDS\"\n",
    "output_dir = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/preprocessed_output\"\n",
    "\n",
    "bosh([\"exec\", \n",
    "      \"prepare\", \n",
    "      boutiques_descriptor, \n",
    "      \"--imagepath\", \n",
    "      \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif\"\n",
    "])\n",
    "\n",
    "cat12 = boutiques.descriptor2func.function(boutiques_descriptor)\n",
    "\n",
    "t1_nii_files = glob.glob(os.path.join(base_dir, \"sub-*\", \"ses-*\", \"anat\", \"sub-*_T1w.nii\"))\n",
    "print(f\"Found {len(t1_nii_files)} T1w files.\")\n",
    "\n",
    "task_id = int(os.environ[\"SLURM_ARRAY_TASK_ID\"])\n",
    "\n",
    "if task_id >= len(t1_nii_files) or task_id < 0:\n",
    "    print(f\"SLURM_ARRAY_TASK_ID={task_id} out of range\")\n",
    "    exit(1)\n",
    "\n",
    "input_file = t1_nii_files[task_id]\n",
    "\n",
    "path_parts = input_file.split(os.sep)\n",
    "subject_id = next((part for part in path_parts if part.startswith(\"sub-\")), None)\n",
    "\n",
    "if subject_id is None:\n",
    "        print(f\"No subject ID in path: {input_file}\")\n",
    "        exit(1)\n",
    "\n",
    "filename = os.path.basename(input_file)\n",
    "\n",
    "subject_output_dir = os.path.join(output_dir, subject_id)\n",
    "os.makedirs(subject_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Processing={task_id} -> file: {filename} (subject: {subject_id})\")\n",
    "\n",
    "result = cat12('--imagepath=/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif', \n",
    "                   input_file=input_file, \n",
    "                   output_dir=subject_output_dir)\n",
    "\n",
    "result_dict = vars(result)\n",
    "print(\"Available result keys:\", result_dict.keys())\n",
    "print(\"\\nExit code:\", result_dict.get(\"exit_code\"))\n",
    "print(\"\\nStdout:\\n\", result_dict.get (\"stdout\" ))\n",
    "print(\"\\nStderr:\\n\", result_dict.get (\"stderr\"))\n",
    "print(\"\\nOutput files:\\n\", result_dict.get(\"output_files\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d9624-6f3c-41b0-9fdc-54d3878e50ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch --array=500-520\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=CAT12_preproc\n",
    "#SBATCH --account=rrg-glatard\n",
    "#SBATCH --mem=12G\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --output=CAT12_preproc_%A_%a.out\n",
    "#SBATCH --error=CAT12_preproc_%A_%a.err\n",
    "#SBATCH --time=1:0:0\n",
    "\n",
    "source ~/.venvs/jupyter_py3/bin/activate\n",
    "\n",
    "module load apptainer\n",
    "\n",
    "cd ~/links/projects/rrg-glatard/arelbaha/HBN_BIDS #Location of preprocessing data and script\n",
    "\n",
    "echo \"Running task ID: $SLURM_ARRAY_TASK_ID\"\n",
    "\n",
    "python new_cat12_preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56370de7-6f6c-47d8-bf17-ad17b7dbfabb",
   "metadata": {},
   "source": [
    "## SPM/CAT12 Preprocessing | AnatCL | PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dcbe07-4e28-43bc-b009-ae1d394712a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/links/projects/def-glatard/arelbaha/data/inputs/ppmicat12_preprocessing.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import boutiques\n",
    "from boutiques import bosh\n",
    "from boutiques.descriptor2func import function\n",
    "\n",
    "#Downloading Container Part + Paths\n",
    "boutiques_descriptor = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/descriptors/cat12_prepro.json\"\n",
    "base_dir = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data/inputs\"\n",
    "output_dir = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/preprocessed_output\"\n",
    "\n",
    "bosh([\"exec\", \"prepare\", boutiques_descriptor, \"--imagepath\", \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif\"])\n",
    "cat12 = boutiques.descriptor2func.function(boutiques_descriptor)\n",
    "\n",
    "#Task ID Extraction\n",
    "data = glob.glob(os.path.join(base_dir, \"sub-*\", \"ses-*\", \"anat\", \"*.nii\"))\n",
    "print(f\"Found {len(data)}\")\n",
    "\n",
    "task_id = int(os.environ[\"SLURM_ARRAY_TASK_ID\"])\n",
    "\n",
    "if task_id >= len(data) or task_id < 0:\n",
    "    print(f\"SLURM_ARRAY_TASK_ID={task_id} out of range\")\n",
    "    exit(1)\n",
    "\n",
    "input_file = data[task_id]\n",
    "\n",
    "path_parts = input_file.split(os.sep)\n",
    "subject_id = next((part for part in path_parts if part.startswith(\"sub-\")), None)\n",
    "\n",
    "if subject_id is None:\n",
    "        print(f\"Could not find subject ID in path: {input_file}\")\n",
    "        exit(1)\n",
    "\n",
    "filename = os.path.basename(input_file)\n",
    "\n",
    "subject_output_dir = os.path.join(output_dir, subject_id)\n",
    "os.makedirs(subject_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Processing SLURM_ARRAY_TASK_ID ={task_id} -> file: {filename} (subject: {subject_id})\")\n",
    "\n",
    "result = cat12('--imagepath=/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif', \n",
    "                   input_file=input_file, \n",
    "                   output_dir=subject_output_dir)\n",
    "\n",
    "result_dict = vars(result)\n",
    "print(\"Available result keys:\", result_dict.keys())\n",
    "print(\"\\nExit code:\", result_dict.get(\"exit_code\"))\n",
    "print(\"\\nStdout:\\n\", result_dict.get (\"stdout\" ))\n",
    "print(\"\\nStderr:\\n\", result_dict.get (\"stderr\"))\n",
    "print(\"\\nOutput files:\\n\", result_dict.get(\"output_files\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472cc4a-c309-45a3-bc27-2ff39eed37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch --array=1040-1040\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=CAT12_parkinson_preproc\n",
    "#SBATCH --account=rrg-glatard\n",
    "#SBATCH --mem=12G\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --output=parkinson_preproc_%A_%a.out\n",
    "#SBATCH --error=parkinson_preproc_%A_%a.err\n",
    "#SBATCH --time=1:0:0\n",
    "\n",
    "source ~/.venvs/jupyter_py3/bin/activate\n",
    "\n",
    "module load apptainer\n",
    "\n",
    "cd ~/links/projects/def-glatard/arelbaha/data/inputs #Location of preprocessing data and script\n",
    "\n",
    "echo \"Running task ID: $SLURM_ARRAY_TASK_ID\"\n",
    "\n",
    "python classification_parkinson.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015677a-ccfc-42e3-8a47-ee62150d4954",
   "metadata": {},
   "source": [
    "### BrainIAC and CNN Preprocessing | HD-BET | PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c39d8-15dd-469b-913a-c243854f1583",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch --array=23,24\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-glatard\n",
    "#SBATCH --time=20:00:00\n",
    "#SBATCH --mem=16G\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --job-name=brainiac_batch\n",
    "#SBATCH --output=logs/prep_batch_%a.out\n",
    "#SBATCH --error=logs/prep_batch_%a.err\n",
    "\n",
    "export BASE_DIR=\"/home/arelbaha/links/projects/def-glatard/arelbaha/data\"\n",
    "export RAW_DIR=\"${BASE_DIR}/raw_files_brainiac\"\n",
    "export OUTPUT_DIR=\"${BASE_DIR}/processed_outputs\"\n",
    "\n",
    "# Load environment\n",
    "module load python/3.11\n",
    "module load opencv\n",
    "source /home/arelbaha/.venvs/brainiac_env/bin/activate\n",
    "\n",
    "BATCH_DIR=\"${RAW_DIR}/batch_${SLURM_ARRAY_TASK_ID}\"\n",
    "BATCH_OUTPUT=\"${OUTPUT_DIR}/batch_${SLURM_ARRAY_TASK_ID}\"\n",
    "\n",
    "mkdir -p \"$BATCH_OUTPUT\"\n",
    "\n",
    "echo \"Processing batch ${SLURM_ARRAY_TASK_ID} from ${BATCH_DIR}\"\n",
    "\n",
    "python /home/arelbaha/.venvs/brainiac_env/lib/python3.11/site-packages/BrainIAC/src/preprocessing/mri_preprocess_3d_simple.py \\\n",
    "    --temp_img /home/arelbaha/.venvs/brainiac_env/lib/python3.11/site-packages/BrainIAC/src/preprocessing/atlases/temp_head.nii.gz \\\n",
    "    --input_dir \"$BATCH_DIR\" \\\n",
    "    --output_dir \"$BATCH_OUTPUT\"\n",
    "\n",
    "echo \"Completed batch ${SLURM_ARRAY_TASK_ID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06103194-9cdf-45bb-89c0-955135a533e4",
   "metadata": {},
   "source": [
    "## PPMI Multi-task Evaluation | AnatCL vs BrainIAC vs CNN vs FreeSurfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "561b885a-5e97-498e-ab61-2bc89068c747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/arelbaha/links/projects/def-glatard/arelbaha/data/inputs/ppmi_multimodel_comparison.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/links/projects/def-glatard/arelbaha/data/inputs/ppmi_multimodel_comparison.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import nibabel as nib\n",
    "from anatcl import AnatCL\n",
    "from monai.networks.nets import ViT\n",
    "import torch.nn.functional as F\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.set_num_threads(1)\n",
    "torch.set_num_interop_threads(1)\n",
    "\n",
    "CAT12_BASE_DIR = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data/inputs\"\n",
    "DATA_DIR = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data\"\n",
    "LABELS_PATH = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data/processed_cohort_with_mri.csv\"\n",
    "BRAINIAC_MAPPING_CSV = os.path.join(DATA_DIR, \"processed_files_mapping.csv\")\n",
    "ANATCL_ENCODER_PATH = \"/home/arelbaha/.venvs/jupyter_py3/bin\"\n",
    "BRAINIAC_CKPT = \"/home/arelbaha/.venvs/jupyter_py3/bin/BrainIAC.ckpt\"\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Random seed: {SEED}\\n\")\n",
    "\n",
    "# Load demographic labels\n",
    "print(\"Loading demographics\")\n",
    "\n",
    "labels_df = pd.read_csv(LABELS_PATH)\n",
    "id_sex_dict = {}\n",
    "id_parkinson_dict = {}\n",
    "id_age_dict = {}\n",
    "\n",
    "for _, row in labels_df.iterrows():\n",
    "    patno = str(int(row['PATNO']))\n",
    "    sex = row['Sex'].strip().upper()\n",
    "    if sex == 'F':\n",
    "        id_sex_dict[patno] = 1\n",
    "    elif sex == 'M':\n",
    "        id_sex_dict[patno] = 0\n",
    "    group = row['Group'].strip()\n",
    "    if group == 'PD':\n",
    "        id_parkinson_dict[patno] = 1\n",
    "    elif group == 'HC':\n",
    "        id_parkinson_dict[patno] = 0\n",
    "    id_age_dict[patno] = row['Age']\n",
    "\n",
    "print(f\"Loaded demographics for {len(id_sex_dict)} subjects\")\n",
    "print(f\"Sex distribution: {sum(id_sex_dict.values())} Female, {len(id_sex_dict) - sum(id_sex_dict.values())} Male\")\n",
    "print(f\"PD distribution: {sum(id_parkinson_dict.values())} PD, {len(id_parkinson_dict) - sum(id_parkinson_dict.values())} HC\")\n",
    "print(f\"Age range: {min(id_age_dict.values()):.1f} - {max(id_age_dict.values()):.1f} years\\n\")\n",
    "\n",
    "def extract_patno_from_path(filepath):\n",
    "    for part in filepath.split(os.sep):\n",
    "        if part.startswith('sub-'):\n",
    "            return part[4:]\n",
    "    return None\n",
    "\n",
    "# Load CAT12 VBM data\n",
    "print(\"Finding CAT12 (s6mwp1) files\")\n",
    "\n",
    "cat12_files = glob.glob(os.path.join(CAT12_BASE_DIR, \"**\", \"*s6mwp1*.nii*\"), recursive=True)\n",
    "cat12_data = {}\n",
    "for f in cat12_files:\n",
    "    if not os.path.isfile(f):\n",
    "        continue\n",
    "    patno = extract_patno_from_path(f)\n",
    "    if patno and patno in id_sex_dict and patno in id_parkinson_dict and patno in id_age_dict:\n",
    "        cat12_data[patno] = f\n",
    "\n",
    "print(f\"Found {len(cat12_data)} CAT12 files\\n\")\n",
    "\n",
    "# Load BrainIAC data\n",
    "print(\"Finding BrainIAC preprocessed files\")\n",
    "\n",
    "brainiac_df = pd.read_csv(BRAINIAC_MAPPING_CSV).dropna(subset=[\"processed_file\", \"Age\", \"subject_id\"])\n",
    "brainiac_data = {}\n",
    "for _, row in brainiac_df.iterrows():\n",
    "    patno = str(row['subject_id'])\n",
    "    if patno in id_sex_dict and patno in id_parkinson_dict and patno in id_age_dict:\n",
    "        if os.path.exists(row['processed_file']):\n",
    "            brainiac_data[patno] = row['processed_file']\n",
    "\n",
    "print(f\"Found {len(brainiac_data)} BrainIAC files\\n\")\n",
    "\n",
    "# Load FreeSurfer data\n",
    "print(\"Extracting FreeSurfer features\")\n",
    "\n",
    "fs_cth_df = pd.read_csv(os.path.join(CAT12_BASE_DIR, \"FS7_APARC_CTH_23Oct2025.csv\"))\n",
    "fs_sa_df = pd.read_csv(os.path.join(CAT12_BASE_DIR, \"FS7_APARC_SA_23Oct2025.csv\"))\n",
    "fs_cth_df = fs_cth_df[fs_cth_df['EVENT_ID'] == 'BL'].copy()\n",
    "fs_sa_df = fs_sa_df[fs_sa_df['EVENT_ID'] == 'BL'].copy()\n",
    "fs_cth_df['PATNO'] = fs_cth_df['PATNO'].astype(str)\n",
    "fs_sa_df['PATNO'] = fs_sa_df['PATNO'].astype(str)\n",
    "cth_features = [c for c in fs_cth_df.columns if c not in ['PATNO', 'EVENT_ID']]\n",
    "sa_features = [c for c in fs_sa_df.columns if c not in ['PATNO', 'EVENT_ID']]\n",
    "\n",
    "# Overall Common Subjects\n",
    "print(\"Overall common subjects across all modalities\")\n",
    "\n",
    "common_subjects = sorted(list(set(cat12_data.keys()) & set(brainiac_data.keys())))\n",
    "fs_data = {}\n",
    "for patno in common_subjects:\n",
    "    cth_row = fs_cth_df[fs_cth_df['PATNO'] == patno]\n",
    "    sa_row = fs_sa_df[fs_sa_df['PATNO'] == patno]\n",
    "    if len(cth_row) > 0 and len(sa_row) > 0:\n",
    "        combined = np.concatenate([cth_row[cth_features].values.flatten(), \n",
    "                                  sa_row[sa_features].values.flatten()])\n",
    "        if not np.any(np.isnan(combined)):\n",
    "            fs_data[patno] = combined\n",
    "\n",
    "common_subjects = sorted(list(set(cat12_data.keys()) & set(brainiac_data.keys()) & set(fs_data.keys())))\n",
    "\n",
    "print(f\"Subjects with CAT12: {len(cat12_data)}\")\n",
    "print(f\"Subjects with BrainIAC: {len(brainiac_data)}\")\n",
    "print(f\"Subjects with FreeSurfer: {len(fs_data)}\")\n",
    "print(f\"Common subjects (all modalities): {len(common_subjects)}\\n\")\n",
    "\n",
    "if len(common_subjects) == 0:\n",
    "    print(\"ERROR: No common subjects found across all modalities\")\n",
    "    exit(1)\n",
    "\n",
    "cat12_paths = [cat12_data[p] for p in common_subjects]\n",
    "brainiac_paths = [brainiac_data[p] for p in common_subjects]\n",
    "fs_features = np.array([fs_data[p] for p in common_subjects])\n",
    "sex_labels = np.array([id_sex_dict[p] for p in common_subjects])\n",
    "parkinson_labels = np.array([id_parkinson_dict[p] for p in common_subjects])\n",
    "age_labels = np.array([id_age_dict[p] for p in common_subjects])\n",
    "\n",
    "print(f\"Final matched dataset: {len(common_subjects)} subjects\")\n",
    "print(f\"FreeSurfer features shape: {fs_features.shape}\")\n",
    "print(f\"Sex distribution: {np.sum(sex_labels)} Female, {len(sex_labels) - np.sum(sex_labels)} Male\")\n",
    "print(f\"PD distribution: {np.sum(parkinson_labels)} PD, {len(parkinson_labels) - np.sum(parkinson_labels)} HC\")\n",
    "print(f\"Age: mean={age_labels.mean():.1f}, range={age_labels.min():.1f}-{age_labels.max():.1f}\\n\")\n",
    "\n",
    "pd.DataFrame({\n",
    "    'subject_id': common_subjects,\n",
    "    'cat12_path': cat12_paths,\n",
    "    'brainiac_path': brainiac_paths,\n",
    "    'sex': sex_labels,\n",
    "    'parkinson': parkinson_labels,\n",
    "    'age': age_labels\n",
    "}).to_csv('ppmi_matched_subjects.csv', index=False)\n",
    "print(\"Saved: ppmi_matched_subjects.csv\\n\")\n",
    "\n",
    "#Dataset Classes\n",
    "\n",
    "class CAT12VBMDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform):\n",
    "        self.data, self.labels, self.transform = data, labels, transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.transform(nib.load(self.data[idx]).get_fdata()).unsqueeze(0)\n",
    "        return img, torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "class BrainIACDataset(Dataset):\n",
    "    def __init__(self, paths, labels, transform):\n",
    "        self.paths, self.labels, self.transform = paths, labels, transform\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.paths[idx]).get_fdata().astype(np.float32)\n",
    "        if img.shape != (96, 96, 96):\n",
    "            img = F.interpolate(torch.from_numpy(img[None, None]), size=(96, 96, 96), \n",
    "                              mode='trilinear', align_corners=False).squeeze().numpy()\n",
    "        img_tensor = torch.from_numpy(img)\n",
    "        img_normalized = self.transform(img_tensor)\n",
    "        return img_normalized[None], torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "class CNNDataset(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.paths[idx]).get_fdata().astype(np.float32)\n",
    "        if img.shape != (92, 110, 92):\n",
    "            img = ndi.zoom(img, [92/img.shape[0], 110/img.shape[1], 92/img.shape[2]], order=1)\n",
    "        img = (img - img.mean()) / (img.std() + 1e-6)\n",
    "        return torch.from_numpy(img[None]).float()\n",
    "\n",
    "class CNN3D(nn.Module):\n",
    "    def __init__(self, dropout_rate=DROPOUT_RATE):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 64, 3, padding=1)\n",
    "        self.bn1, self.pool1, self.drop1 = nn.BatchNorm3d(64), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv2 = nn.Conv3d(64, 64, 3, padding=1)\n",
    "        self.bn2, self.pool2, self.drop2 = nn.BatchNorm3d(64), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv3 = nn.Conv3d(64, 128, 3, padding=1)\n",
    "        self.bn3, self.pool3, self.drop3 = nn.BatchNorm3d(128), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv4 = nn.Conv3d(128, 256, 3, padding=1)\n",
    "        self.bn4, self.pool4, self.drop4 = nn.BatchNorm3d(256), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc1, self.drop5 = nn.Linear(256, 512), nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop1(self.bn1(self.pool1(F.relu(self.conv1(x)))))\n",
    "        x = self.drop2(self.bn2(self.pool2(F.relu(self.conv2(x)))))\n",
    "        x = self.drop3(self.bn3(self.pool3(F.relu(self.conv3(x)))))\n",
    "        x = self.drop4(self.bn4(self.pool4(F.relu(self.conv4(x)))))\n",
    "        return self.drop5(F.relu(self.fc1(self.global_pool(x).view(x.size(0), -1))))\n",
    "\n",
    "def load_brainiac_vit(ckpt_path, device):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    vit = ViT(in_channels=1, img_size=(96, 96, 96), patch_size=(16, 16, 16),\n",
    "              hidden_size=768, mlp_dim=3072, num_layers=12, num_heads=12, classification=False)\n",
    "    pe_key = \"backbone.patch_embedding.position_embeddings\"\n",
    "    if pe_key in ckpt.get(\"state_dict\", ckpt):\n",
    "        vit.patch_embedding.position_embeddings = nn.Parameter(ckpt[\"state_dict\"][pe_key].clone())\n",
    "    backbone_state = {k.replace(\"backbone.\", \"\"): v for k, v in ckpt.get(\"state_dict\", ckpt).items() \n",
    "                     if k.startswith(\"backbone.\")}\n",
    "    vit.load_state_dict(backbone_state, strict=False)\n",
    "    return vit.to(device).eval()\n",
    "\n",
    "# Extracting AnatCL Features\n",
    "print(\"Extracting AnatCL features (averaging across 5 cross-validation folds)\")\n",
    "\n",
    "anatcl_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: torch.from_numpy(x.copy()).float()),\n",
    "    transforms.Normalize(mean=0.0, std=1.0)\n",
    "])\n",
    "\n",
    "num_folds = 5\n",
    "all_fold_features = []\n",
    "\n",
    "for fold_idx in range(num_folds):\n",
    "    path = os.path.join(ANATCL_ENCODER_PATH, f\"fold{fold_idx}.pth\")\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"ERROR: fold{fold_idx}.pth not found at {path}\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"Loading fold {fold_idx}...\")\n",
    "    encoder = AnatCL(descriptor=\"global\", fold=fold_idx, pretrained=False).to(device).eval()\n",
    "    encoder.backbone.load_state_dict(torch.load(path, map_location=device, weights_only=False)['model'])\n",
    "    \n",
    "    for p in encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    dl = DataLoader(CAT12VBMDataset(cat12_paths, age_labels, anatcl_transform), \n",
    "                    batch_size=32, num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fold_features = torch.cat([encoder(vol.to(device)).cpu() for vol, _ in dl]).numpy()\n",
    "    \n",
    "    all_fold_features.append(fold_features)\n",
    "    print(f\"  Fold {fold_idx} features: {fold_features.shape}\")\n",
    "    \n",
    "    del encoder\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "anatcl_features = np.mean(all_fold_features, axis=0)\n",
    "\n",
    "print(f\"\\nAnatCL features (averaged across {num_folds} folds): {anatcl_features.shape}\\n\")\n",
    "\n",
    "#Extracting BrainIAC Features\n",
    "print(\"Extracting BrainIAC features\")\n",
    "\n",
    "brainiac_vit = load_brainiac_vit(BRAINIAC_CKPT, device)\n",
    "brainiac_transform = lambda x: (x - x.mean()) / (x.std() + 1e-6)\n",
    "dl = DataLoader(BrainIACDataset(brainiac_paths, age_labels, brainiac_transform), \n",
    "                batch_size=16, num_workers=0)\n",
    "\n",
    "brainiac_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, _ in dl:\n",
    "        out = brainiac_vit(x.to(device))\n",
    "        \n",
    "        # Handle tuple or tensor\n",
    "        if isinstance(out, tuple):\n",
    "            cls_token = out[0][:, 0]\n",
    "        else:\n",
    "            cls_token = out[:, 0]\n",
    "        \n",
    "        brainiac_features.append(cls_token.cpu().numpy())\n",
    "\n",
    "brainiac_features = np.vstack(brainiac_features)\n",
    "\n",
    "print(f\"BrainIAC features: {brainiac_features.shape}\")\n",
    "del brainiac_vit\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Extracting CNN Features\n",
    "print(\"Extracting CNN features\")\n",
    "\n",
    "cnn_model = CNN3D().to(device).eval()\n",
    "dl = DataLoader(CNNDataset(brainiac_paths), batch_size=8, num_workers=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    cnn_features = np.vstack([cnn_model(x.to(device)).cpu().numpy() for x in dl])\n",
    "\n",
    "print(f\"CNN features: {cnn_features.shape}\\n\")\n",
    "del cnn_model\n",
    "\n",
    "# Organize Features\n",
    "features_dict = {\n",
    "    'AnatCL': anatcl_features,\n",
    "    'BrainIAC': brainiac_features,\n",
    "    'CNN': cnn_features,\n",
    "    'FreeSurfer': fs_features\n",
    "}\n",
    "print(\"Feature extraction complete\")\n",
    "for name, feats in features_dict.items():\n",
    "    print(f\"{name}: {feats.shape}\")\n",
    "print()\n",
    "\n",
    "# Correlation Analysis\n",
    "print(\"Computing cross-model feature correlations\")\n",
    "\n",
    "def compute_correlation_within_models(features_dict):\n",
    "    all_features = []\n",
    "    boundaries = [0]\n",
    "    reordered_indices = {}\n",
    "\n",
    "    for model in ['AnatCL', 'BrainIAC', 'CNN', 'FreeSurfer']:\n",
    "        if model not in features_dict:\n",
    "            continue\n",
    "\n",
    "        feats = features_dict[model]\n",
    "        feats = StandardScaler().fit_transform(feats)\n",
    "\n",
    "        valid = ~np.isnan(feats).any(axis=0) & (np.std(feats, axis=0) > 1e-10)\n",
    "        feats = feats[:, valid]\n",
    "\n",
    "        if feats.shape[1] == 0:\n",
    "            continue\n",
    "\n",
    "        print(f\"  {model}: {feats.shape[1]} features\")\n",
    "\n",
    "        corr = np.corrcoef(feats.T)\n",
    "        corr = np.nan_to_num((corr + corr.T) / 2, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        np.fill_diagonal(corr, 1.0)\n",
    "\n",
    "        dist = np.abs(1 - np.abs(corr))\n",
    "        np.fill_diagonal(dist, 0)\n",
    "        condensed = np.nan_to_num(squareform((dist + dist.T) / 2, checks=False), \n",
    "                                  nan=0.0, posinf=1.0, neginf=0.0)\n",
    "\n",
    "        reorder_idx = dendrogram(linkage(condensed, method='ward'), no_plot=True)['leaves']\n",
    "        all_features.append(feats[:, reorder_idx])\n",
    "        boundaries.append(boundaries[-1] + feats.shape[1])\n",
    "        reordered_indices[model] = reorder_idx\n",
    "\n",
    "    combined = np.hstack(all_features)\n",
    "    corr = np.nan_to_num((np.corrcoef(combined.T) + np.corrcoef(combined.T).T) / 2, \n",
    "                         nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "\n",
    "    print(f\"\\nTotal features: {combined.shape[1]}\")\n",
    "    \n",
    "    return corr, boundaries, ['AnatCL', 'BrainIAC', 'CNN', 'FreeSurfer'], reordered_indices\n",
    "\n",
    "corr_matrix, boundaries, model_names, reorder_indices = compute_correlation_within_models(features_dict)\n",
    "pd.DataFrame(corr_matrix).to_csv('ppmi_feature_correlation_matrix.csv', index=False)\n",
    "print(f\"Correlation matrix: {corr_matrix.shape}\")\n",
    "print(f\"Model boundaries: {boundaries}\\n\")\n",
    "\n",
    "# Plot correlation matrix (FULL SIZE)\n",
    "print(\"Plotting cross-model correlation matrix...\")\n",
    "\n",
    "plt.figure(figsize=(40, 38))\n",
    "sns.heatmap(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1, center=0, square=True, \n",
    "            linewidths=0, cbar_kws={\"shrink\": 0.3},\n",
    "            xticklabels=False, yticklabels=False)\n",
    "\n",
    "for b in boundaries[1:-1]:\n",
    "    plt.axhline(y=b, color='black', linewidth=4)\n",
    "    plt.axvline(x=b, color='black', linewidth=4)\n",
    "\n",
    "for i, (pos, name) in enumerate(zip([(boundaries[i] + boundaries[i+1]) / 2 \n",
    "                                      for i in range(len(boundaries)-1)], model_names)):\n",
    "    plt.text(pos, -30, name, ha='center', fontsize=24, weight='bold')\n",
    "    plt.text(-30, pos, name, ha='center', va='center', fontsize=24, weight='bold', rotation=90)\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.savefig('ppmi_cross_model_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: ppmi_cross_model_correlation.png\\n\")\n",
    "\n",
    "# Model Training Functions\n",
    "\n",
    "def run_classification(X_dict, y, test_size=0.1, task_name=\"Classification\"):\n",
    "    print(f\"Running {task_name}...\")\n",
    "    results = {}\n",
    "    cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)), \n",
    "                                        stratify=y, random_state=SEED)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    alphas = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "    print(\"Training DummyClassifier baseline...\")\n",
    "    dummy_val_auc = []\n",
    "    dummy_val_acc = []\n",
    "    \n",
    "    for train_rel, val_rel in skf.split(cv_idx, y[cv_idx]):\n",
    "        train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "        dummy = DummyClassifier(strategy='stratified', random_state=SEED)\n",
    "        dummy.fit(np.zeros((len(train_idx), 1)), y[train_idx])\n",
    "        \n",
    "        pred = dummy.predict(np.zeros((len(val_idx), 1)))\n",
    "        pred_proba = dummy.predict_proba(np.zeros((len(val_idx), 1)))[:, 1]\n",
    "        \n",
    "        dummy_val_acc.append(balanced_accuracy_score(y[val_idx], pred))\n",
    "        dummy_val_auc.append(roc_auc_score(y[val_idx], pred_proba))\n",
    "    \n",
    "    dummy = DummyClassifier(strategy='stratified', random_state=SEED)\n",
    "    dummy.fit(np.zeros((len(cv_idx), 1)), y[cv_idx])\n",
    "    dummy_test_pred = dummy.predict(np.zeros((len(test_idx), 1)))\n",
    "    dummy_test_proba = dummy.predict_proba(np.zeros((len(test_idx), 1)))[:, 1]\n",
    "    \n",
    "    results['DummyClassifier'] = {\n",
    "        'best_alpha': 1.0,\n",
    "        'test_balanced_accuracy': balanced_accuracy_score(y[test_idx], dummy_test_pred),\n",
    "        'test_auc': roc_auc_score(y[test_idx], dummy_test_proba),\n",
    "        'cv_auc_mean': np.mean(dummy_val_auc),\n",
    "        'cv_auc_std': np.std(dummy_val_auc),\n",
    "        'cv_acc_mean': np.mean(dummy_val_acc),\n",
    "        'cv_acc_std': np.std(dummy_val_acc),\n",
    "        'fold_results': {'val_auc': dummy_val_auc, 'val_acc': dummy_val_acc}\n",
    "    }\n",
    "    print(f\"  DummyClassifier: AUC={results['DummyClassifier']['test_auc']:.4f}, Bal_Acc={results['DummyClassifier']['test_balanced_accuracy']:.4f}\")\n",
    "\n",
    "    for model, X in X_dict.items():\n",
    "        val_auc = {a: [] for a in alphas}\n",
    "        val_acc = {a: [] for a in alphas}\n",
    "        train_auc = {a: [] for a in alphas}\n",
    "        train_acc = {a: [] for a in alphas}\n",
    "\n",
    "        for train_rel, val_rel in skf.split(cv_idx, y[cv_idx]):\n",
    "            train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "\n",
    "            for alpha in alphas:\n",
    "                n = max(4, int(alpha * len(train_idx)))\n",
    "                try:\n",
    "                    tr_idx, _ = train_test_split(train_idx, train_size=n, stratify=y[train_idx], random_state=SEED)\n",
    "                except:\n",
    "                    tr_idx = np.random.choice(train_idx, n, replace=False)\n",
    "\n",
    "                if len(np.unique(y[tr_idx])) < 2:\n",
    "                    continue\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                rf = RandomForestClassifier(n_estimators=500, random_state=SEED, n_jobs=-1, \n",
    "                                           max_features='sqrt', class_weight='balanced')\n",
    "                rf.fit(scaler.fit_transform(X[tr_idx]), y[tr_idx])\n",
    "\n",
    "                pred_proba = rf.predict_proba(scaler.transform(X[val_idx]))[:, 1]\n",
    "                val_acc[alpha].append(balanced_accuracy_score(y[val_idx], rf.predict(scaler.transform(X[val_idx]))))\n",
    "                val_auc[alpha].append(roc_auc_score(y[val_idx], pred_proba))\n",
    "                \n",
    "                train_pred_proba = rf.predict_proba(scaler.transform(X[tr_idx]))[:, 1]\n",
    "                train_acc[alpha].append(balanced_accuracy_score(y[tr_idx], rf.predict(scaler.transform(X[tr_idx]))))\n",
    "                train_auc[alpha].append(roc_auc_score(y[tr_idx], train_pred_proba))\n",
    "\n",
    "        avg_auc = {a: np.mean(val_auc[a]) for a in alphas if val_auc[a]}\n",
    "        best_alpha = max(avg_auc, key=avg_auc.get)\n",
    "\n",
    "        try:\n",
    "            final_tr, _ = train_test_split(cv_idx, train_size=max(4, int(best_alpha * len(cv_idx))), \n",
    "                                          stratify=y[cv_idx], random_state=SEED)\n",
    "        except:\n",
    "            final_tr = cv_idx\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        rf = RandomForestClassifier(n_estimators=500, random_state=SEED, n_jobs=-1, \n",
    "                                    max_features='sqrt', class_weight='balanced')\n",
    "        rf.fit(scaler.fit_transform(X[final_tr]), y[final_tr])\n",
    "        test_pred_proba = rf.predict_proba(scaler.transform(X[test_idx]))[:, 1]\n",
    "\n",
    "        train_val_gap = np.mean(train_auc[best_alpha]) - np.mean(val_auc[best_alpha])\n",
    "        is_overfitting = train_val_gap > 0.25\n",
    "\n",
    "        results[model] = {\n",
    "            'best_alpha': best_alpha,\n",
    "            'test_balanced_accuracy': balanced_accuracy_score(y[test_idx], rf.predict(scaler.transform(X[test_idx]))),\n",
    "            'test_auc': roc_auc_score(y[test_idx], test_pred_proba),\n",
    "            'cv_auc_mean': np.mean(val_auc[best_alpha]),\n",
    "            'cv_auc_std': np.std(val_auc[best_alpha]),\n",
    "            'cv_acc_mean': np.mean(val_acc[best_alpha]),\n",
    "            'cv_acc_std': np.std(val_acc[best_alpha]),\n",
    "            'train_auc_mean': np.mean(train_auc[best_alpha]),\n",
    "            'train_val_gap': train_val_gap,\n",
    "            'is_overfitting': is_overfitting,\n",
    "            'fold_results': {\n",
    "                'val_auc': val_auc[best_alpha],\n",
    "                'val_acc': val_acc[best_alpha],\n",
    "                'train_auc': train_auc[best_alpha]\n",
    "            },\n",
    "            'cv_results': {\n",
    "                'alphas': alphas, \n",
    "                'acc': {a: np.mean(val_acc[a]) for a in alphas if val_acc[a]},\n",
    "                'auc': avg_auc,\n",
    "                'acc_std': {a: np.std(val_acc[a]) for a in alphas if val_acc[a]},\n",
    "                'auc_std': {a: np.std(val_auc[a]) for a in alphas if val_auc[a]}\n",
    "            }\n",
    "        }\n",
    "        print(f\"  {model}: AUC={results[model]['test_auc']:.4f}, Bal_Acc={results[model]['test_balanced_accuracy']:.4f}\" + \n",
    "              (f\" [OVERFITTING: train-val gap={train_val_gap:.3f}]\" if is_overfitting else \"\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_regression(X_dict, y, test_size=0.1):\n",
    "    print(\"Running age prediction...\")\n",
    "    results = {}\n",
    "    cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)), random_state=SEED)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    alphas = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "    print(\"  Training DummyRegressor baseline...\")\n",
    "    dummy_val_r2 = []\n",
    "    dummy_val_mae = []\n",
    "    \n",
    "    for train_rel, val_rel in kf.split(cv_idx):\n",
    "        train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "        dummy = DummyRegressor(strategy='mean')\n",
    "        dummy.fit(np.zeros((len(train_idx), 1)), y[train_idx])\n",
    "        \n",
    "        pred = dummy.predict(np.zeros((len(val_idx), 1)))\n",
    "        dummy_val_r2.append(r2_score(y[val_idx], pred))\n",
    "        dummy_val_mae.append(mean_absolute_error(y[val_idx], pred))\n",
    "    \n",
    "    dummy = DummyRegressor(strategy='mean')\n",
    "    dummy.fit(np.zeros((len(cv_idx), 1)), y[cv_idx])\n",
    "    dummy_test_pred = dummy.predict(np.zeros((len(test_idx), 1)))\n",
    "    \n",
    "    results['DummyRegressor'] = {\n",
    "        'best_alpha': 1.0,\n",
    "        'test_r2': r2_score(y[test_idx], dummy_test_pred),\n",
    "        'test_mae': mean_absolute_error(y[test_idx], dummy_test_pred),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y[test_idx], dummy_test_pred)),\n",
    "        'cv_r2_mean': np.mean(dummy_val_r2),\n",
    "        'cv_r2_std': np.std(dummy_val_r2),\n",
    "        'cv_mae_mean': np.mean(dummy_val_mae),\n",
    "        'cv_mae_std': np.std(dummy_val_mae),\n",
    "        'fold_results': {'val_r2': dummy_val_r2, 'val_mae': dummy_val_mae}\n",
    "    }\n",
    "    print(f\"  DummyRegressor: R2={results['DummyRegressor']['test_r2']:.4f}, MAE={results['DummyRegressor']['test_mae']:.2f}\")\n",
    "\n",
    "    for model, X in X_dict.items():\n",
    "        val_r2 = {a: [] for a in alphas}\n",
    "        val_mae = {a: [] for a in alphas}\n",
    "        train_r2 = {a: [] for a in alphas}\n",
    "        train_mae = {a: [] for a in alphas}\n",
    "\n",
    "        for train_rel, val_rel in kf.split(cv_idx):\n",
    "            train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "\n",
    "            for alpha in alphas:\n",
    "                n = max(40, int(alpha * len(train_idx)))\n",
    "                tr_idx, _ = train_test_split(train_idx, train_size=n, random_state=SEED) \\\n",
    "                           if n < len(train_idx) else (train_idx, None)\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                rf = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_split=5, \n",
    "                                          random_state=SEED, n_jobs=-1)\n",
    "                rf.fit(scaler.fit_transform(X[tr_idx]), y[tr_idx])\n",
    "\n",
    "                pred = rf.predict(scaler.transform(X[val_idx]))\n",
    "                val_r2[alpha].append(r2_score(y[val_idx], pred))\n",
    "                val_mae[alpha].append(mean_absolute_error(y[val_idx], pred))\n",
    "                \n",
    "                train_pred = rf.predict(scaler.transform(X[tr_idx]))\n",
    "                train_r2[alpha].append(r2_score(y[tr_idx], train_pred))\n",
    "                train_mae[alpha].append(mean_absolute_error(y[tr_idx], train_pred))\n",
    "\n",
    "        avg_r2 = {a: np.mean(val_r2[a]) for a in alphas if val_r2[a]}\n",
    "        best_alpha = max(avg_r2, key=avg_r2.get)\n",
    "\n",
    "        n = max(40, int(best_alpha * len(cv_idx)))\n",
    "        final_tr, _ = train_test_split(cv_idx, train_size=n, random_state=SEED) \\\n",
    "                     if n < len(cv_idx) else (cv_idx, None)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        rf = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_split=5, random_state=SEED, n_jobs=-1)\n",
    "        rf.fit(scaler.fit_transform(X[final_tr]), y[final_tr])\n",
    "        test_pred = rf.predict(scaler.transform(X[test_idx]))\n",
    "\n",
    "        train_val_gap = np.mean(train_r2[best_alpha]) - np.mean(val_r2[best_alpha])\n",
    "        is_overfitting = train_val_gap > 0.35\n",
    "\n",
    "        results[model] = {\n",
    "            'best_alpha': best_alpha,\n",
    "            'test_r2': r2_score(y[test_idx], test_pred),\n",
    "            'test_mae': mean_absolute_error(y[test_idx], test_pred),\n",
    "            'test_rmse': np.sqrt(mean_squared_error(y[test_idx], test_pred)),\n",
    "            'predictions': test_pred,\n",
    "            'actual': y[test_idx],\n",
    "            'cv_r2_mean': np.mean(val_r2[best_alpha]),\n",
    "            'cv_r2_std': np.std(val_r2[best_alpha]),\n",
    "            'cv_mae_mean': np.mean(val_mae[best_alpha]),\n",
    "            'cv_mae_std': np.std(val_mae[best_alpha]),\n",
    "            'train_r2_mean': np.mean(train_r2[best_alpha]),\n",
    "            'train_val_gap': train_val_gap,\n",
    "            'is_overfitting': is_overfitting,\n",
    "            'fold_results': {\n",
    "                'val_r2': val_r2[best_alpha],\n",
    "                'val_mae': val_mae[best_alpha],\n",
    "                'train_r2': train_r2[best_alpha]\n",
    "            },\n",
    "            'cv_results': {\n",
    "                'alphas': alphas, \n",
    "                'r2': avg_r2, \n",
    "                'mae': {a: np.mean(val_mae[a]) for a in alphas if val_mae[a]},\n",
    "                'r2_std': {a: np.std(val_r2[a]) for a in alphas if val_r2[a]},\n",
    "                'mae_std': {a: np.std(val_mae[a]) for a in alphas if val_mae[a]}\n",
    "            }\n",
    "        }\n",
    "        print(f\"  {model}: R2={results[model]['test_r2']:.4f}, MAE={results[model]['test_mae']:.2f}\" +\n",
    "              (f\" [OVERFITTING: train-val gap={train_val_gap:.3f}]\" if is_overfitting else \"\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run Sex Classification\n",
    "print(\"Sex Classification\")\n",
    "\n",
    "sex_results = run_classification(features_dict, sex_labels, test_size=0.1, task_name=\"Sex classification\")\n",
    "print()\n",
    "\n",
    "# Run Parkinson Classification\n",
    "print(\"Parkinson Classification\")\n",
    "\n",
    "parkinson_results = run_classification(features_dict, parkinson_labels, test_size=0.1, task_name=\"Parkinson classification\")\n",
    "print()\n",
    "\n",
    "# Run Age Prediction\n",
    "print(\"Age Prediction\")\n",
    "\n",
    "age_results = run_regression(features_dict, age_labels, test_size=0.1)\n",
    "print()\n",
    "\n",
    "# Overfitting Analysis\n",
    "print(\"Overfitting Analysis\")\n",
    "\n",
    "overfitting_report = []\n",
    "for model in features_dict.keys():\n",
    "    sex_overfit = sex_results[model]['is_overfitting']\n",
    "    parkinson_overfit = parkinson_results[model]['is_overfitting']\n",
    "    age_overfit = age_results[model]['is_overfitting']\n",
    "    \n",
    "    overfitting_report.append({\n",
    "        'Model': model,\n",
    "        'Sex_Overfitting': sex_overfit,\n",
    "        'Sex_Train_Val_Gap': sex_results[model]['train_val_gap'],\n",
    "        'Parkinson_Overfitting': parkinson_overfit,\n",
    "        'Parkinson_Train_Val_Gap': parkinson_results[model]['train_val_gap'],\n",
    "        'Age_Overfitting': age_overfit,\n",
    "        'Age_Train_Val_Gap': age_results[model]['train_val_gap']\n",
    "    })\n",
    "    \n",
    "    if sex_overfit or parkinson_overfit or age_overfit:\n",
    "        print(f\"{model}:\")\n",
    "        if sex_overfit:\n",
    "            print(f\"  Sex classification: Train-Val AUC gap = {sex_results[model]['train_val_gap']:.3f}\")\n",
    "        if parkinson_overfit:\n",
    "            print(f\"  Parkinson classification: Train-Val AUC gap = {parkinson_results[model]['train_val_gap']:.3f}\")\n",
    "        if age_overfit:\n",
    "            print(f\"  Age prediction: Train-Val RÂ² gap = {age_results[model]['train_val_gap']:.3f}\")\n",
    "\n",
    "if not any(r['Sex_Overfitting'] or r['Parkinson_Overfitting'] or r['Age_Overfitting'] for r in overfitting_report):\n",
    "    print(\"No significant overfitting detected\")\n",
    "\n",
    "pd.DataFrame(overfitting_report).to_csv('ppmi_overfitting_analysis.csv', index=False)\n",
    "print(\"\\nSaved: ppmi_overfitting_analysis.csv\\n\")\n",
    "\n",
    "# Saving Results\n",
    "print(\"=\"*80)\n",
    "print(\"Saving Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_models = ['DummyClassifier'] + list(features_dict.keys())\n",
    "\n",
    "pd.DataFrame([{'Model': m, 'Best_Alpha': sex_results[m]['best_alpha'], \n",
    "               'Test_Balanced_Accuracy': sex_results[m]['test_balanced_accuracy'], \n",
    "               'Test_AUC': sex_results[m]['test_auc'],\n",
    "               'CV_AUC_Mean': sex_results[m]['cv_auc_mean'],\n",
    "               'CV_AUC_Std': sex_results[m]['cv_auc_std']} \n",
    "              for m in all_models]).to_csv('ppmi_sex_classification_summary.csv', index=False)\n",
    "\n",
    "pd.DataFrame([{'Model': m, 'Best_Alpha': parkinson_results[m]['best_alpha'], \n",
    "               'Test_Balanced_Accuracy': parkinson_results[m]['test_balanced_accuracy'], \n",
    "               'Test_AUC': parkinson_results[m]['test_auc'],\n",
    "               'CV_AUC_Mean': parkinson_results[m]['cv_auc_mean'],\n",
    "               'CV_AUC_Std': parkinson_results[m]['cv_auc_std']} \n",
    "              for m in all_models]).to_csv('ppmi_parkinson_classification_summary.csv', index=False)\n",
    "\n",
    "all_models_reg = ['DummyRegressor'] + list(features_dict.keys())\n",
    "\n",
    "pd.DataFrame([{'Model': m, 'Best_Alpha': age_results[m]['best_alpha'], \n",
    "               'Test_R2': age_results[m]['test_r2'], \n",
    "               'Test_MAE': age_results[m]['test_mae'], \n",
    "               'Test_RMSE': age_results[m]['test_rmse'],\n",
    "               'CV_R2_Mean': age_results[m]['cv_r2_mean'],\n",
    "               'CV_R2_Std': age_results[m]['cv_r2_std'],\n",
    "               'CV_MAE_Mean': age_results[m]['cv_mae_mean'],\n",
    "               'CV_MAE_Std': age_results[m]['cv_mae_std']} \n",
    "              for m in all_models_reg]).to_csv('ppmi_age_prediction_summary.csv', index=False)\n",
    "\n",
    "all_results = [\n",
    "    {'Model': m, \n",
    "     'Sex_AUC': sex_results[m]['test_auc'], \n",
    "     'Sex_Bal_Acc': sex_results[m]['test_balanced_accuracy'],\n",
    "     'Parkinson_AUC': parkinson_results[m]['test_auc'], \n",
    "     'Parkinson_Bal_Acc': parkinson_results[m]['test_balanced_accuracy'],\n",
    "     'Age_R2': age_results[m]['test_r2'], \n",
    "     'Age_MAE': age_results[m]['test_mae'], \n",
    "     'Age_RMSE': age_results[m]['test_rmse']} \n",
    "    for m in list(features_dict.keys())\n",
    "]\n",
    "pd.DataFrame(all_results).to_csv('ppmi_detailed_comparison.csv', index=False)\n",
    "\n",
    "rankings = []\n",
    "for task in ['Sex_AUC', 'Parkinson_AUC', 'Age_R2']:\n",
    "    for rank, item in enumerate(sorted(all_results, key=lambda x: x[task], reverse=True), 1):\n",
    "        rankings.append({\n",
    "            'Task': task.replace('_', ' '), \n",
    "            'Rank': rank, \n",
    "            'Model': item['Model'], \n",
    "            'Score': item[task]\n",
    "        })\n",
    "pd.DataFrame(rankings).to_csv('ppmi_model_rankings.csv', index=False)\n",
    "\n",
    "print(\"Saved: ppmi_sex_classification_summary.csv\")\n",
    "print(\"Saved: ppmi_parkinson_classification_summary.csv\")\n",
    "print(\"Saved: ppmi_age_prediction_summary.csv\")\n",
    "print(\"Saved: ppmi_detailed_comparison.csv\")\n",
    "print(\"Saved: ppmi_model_rankings.csv\\n\")\n",
    "\n",
    "# Generate Learning Curve Visualizations\n",
    "print(\"=\"*80)\n",
    "print(\"Generating Learning Curves\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models = list(features_dict.keys())\n",
    "x_pos = np.arange(len(models))\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
    "\n",
    "n_cv = len(age_labels) - int(0.1 * len(age_labels))\n",
    "alphas = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "training_sizes = [int(alpha * n_cv * 0.8) for alpha in alphas]\n",
    "\n",
    "# Learning curves WITH SHADED ERROR REGIONS\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 6))\n",
    "\n",
    "# Sex classification learning curve\n",
    "for idx, model in enumerate(models):\n",
    "    acc_means = []\n",
    "    acc_stds = []\n",
    "    valid_sizes = []\n",
    "    \n",
    "    for i, alpha in enumerate(alphas):\n",
    "        if alpha in sex_results[model]['cv_results']['acc']:\n",
    "            acc_means.append(sex_results[model]['cv_results']['acc'][alpha])\n",
    "            acc_stds.append(sex_results[model]['cv_results']['acc_std'][alpha])\n",
    "            valid_sizes.append(training_sizes[i])\n",
    "    \n",
    "    acc_means = np.array(acc_means)\n",
    "    acc_stds = np.array(acc_stds)\n",
    "    valid_sizes = np.array(valid_sizes)\n",
    "    \n",
    "    ax1.plot(valid_sizes, acc_means, 'o-', linewidth=2.5, markersize=8, \n",
    "             label=model, color=colors[idx])\n",
    "    ax1.fill_between(valid_sizes, acc_means - acc_stds, acc_means + acc_stds, \n",
    "                     alpha=0.25, color=colors[idx])\n",
    "\n",
    "dummy_acc_cv = sex_results['DummyClassifier']['cv_acc_mean']\n",
    "ax1.axhline(y=dummy_acc_cv, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Dummy Classifier ({dummy_acc_cv:.3f})', alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Number of Training Subjects', fontsize=12, weight='bold')\n",
    "ax1.set_ylabel('Balanced Accuracy', fontsize=12, weight='bold')\n",
    "ax1.set_title('Sex Classification: CV Learning Curve', fontsize=14, weight='bold')\n",
    "ax1.legend(fontsize=10, loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Parkinson classification learning curve\n",
    "for idx, model in enumerate(models):\n",
    "    acc_means = []\n",
    "    acc_stds = []\n",
    "    valid_sizes = []\n",
    "    \n",
    "    for i, alpha in enumerate(alphas):\n",
    "        if alpha in parkinson_results[model]['cv_results']['acc']:\n",
    "            acc_means.append(parkinson_results[model]['cv_results']['acc'][alpha])\n",
    "            acc_stds.append(parkinson_results[model]['cv_results']['acc_std'][alpha])\n",
    "            valid_sizes.append(training_sizes[i])\n",
    "    \n",
    "    acc_means = np.array(acc_means)\n",
    "    acc_stds = np.array(acc_stds)\n",
    "    valid_sizes = np.array(valid_sizes)\n",
    "    \n",
    "    ax2.plot(valid_sizes, acc_means, 'o-', linewidth=2.5, markersize=8, \n",
    "             label=model, color=colors[idx])\n",
    "    ax2.fill_between(valid_sizes, acc_means - acc_stds, acc_means + acc_stds, \n",
    "                     alpha=0.25, color=colors[idx])\n",
    "\n",
    "dummy_acc_cv_pd = parkinson_results['DummyClassifier']['cv_acc_mean']\n",
    "ax2.axhline(y=dummy_acc_cv_pd, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Dummy Classifier ({dummy_acc_cv_pd:.3f})', alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('Number of Training Subjects', fontsize=12, weight='bold')\n",
    "ax2.set_ylabel('Balanced Accuracy', fontsize=12, weight='bold')\n",
    "ax2.set_title('Parkinson Classification: CV Learning Curve', fontsize=14, weight='bold')\n",
    "ax2.legend(fontsize=10, loc='lower right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "# Age prediction learning curve\n",
    "for idx, model in enumerate(models):\n",
    "    mae_means = []\n",
    "    mae_stds = []\n",
    "    valid_sizes = []\n",
    "    \n",
    "    for i, alpha in enumerate(alphas):\n",
    "        if alpha in age_results[model]['cv_results']['mae']:\n",
    "            mae_means.append(age_results[model]['cv_results']['mae'][alpha])\n",
    "            mae_stds.append(age_results[model]['cv_results']['mae_std'][alpha])\n",
    "            valid_sizes.append(training_sizes[i])\n",
    "    \n",
    "    mae_means = np.array(mae_means)\n",
    "    mae_stds = np.array(mae_stds)\n",
    "    valid_sizes = np.array(valid_sizes)\n",
    "    \n",
    "    ax3.plot(valid_sizes, mae_means, 'o-', linewidth=2.5, markersize=8, \n",
    "             label=model, color=colors[idx])\n",
    "    ax3.fill_between(valid_sizes, mae_means - mae_stds, mae_means + mae_stds, \n",
    "                     alpha=0.25, color=colors[idx])\n",
    "\n",
    "dummy_mae_cv = age_results['DummyRegressor']['cv_mae_mean']\n",
    "ax3.axhline(y=dummy_mae_cv, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Dummy Regressor ({dummy_mae_cv:.2f})', alpha=0.7)\n",
    "\n",
    "ax3.set_xlabel('Number of Training Subjects', fontsize=12, weight='bold')\n",
    "ax3.set_ylabel('MAE (years)', fontsize=12, weight='bold')\n",
    "ax3.set_title('Age Prediction: CV MAE Learning Curve', fontsize=14, weight='bold')\n",
    "ax3.legend(fontsize=10, loc='upper right')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ppmi_cv_learning_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: ppmi_cv_learning_curves.png\\n\")\n",
    "\n",
    "print(f\"Dataset: {len(common_subjects)} subjects\")\n",
    "print(f\"Age range: {age_labels.min():.1f} - {age_labels.max():.1f} years\")\n",
    "print(f\"Sex: {np.sum(sex_labels)} Female, {len(sex_labels) - np.sum(sex_labels)} Male\")\n",
    "print(f\"PD: {np.sum(parkinson_labels)} PD, {len(parkinson_labels) - np.sum(parkinson_labels)} HC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc4053-2a9c-4966-887a-aea3c11b52f1",
   "metadata": {},
   "source": [
    "## BrainIAC and CNN Preprocessing | HD-BET | HBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfebf287-f626-4640-967a-b16c3dc1333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch --array=1-100\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-glatard\n",
    "#SBATCH --time=12:00:00\n",
    "#SBATCH --mem=16G\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --job-name=brainiac_batch\n",
    "#SBATCH --output=logs/prep_batch_%a.out\n",
    "#SBATCH --error=logs/prep_batch_%a.err\n",
    "\n",
    "export BASE_DIR=\"/home/arelbaha/links/projects/rrg-glatard/arelbaha\"\n",
    "export RAW_DIR=\"${BASE_DIR}/brainiac_p_files\"\n",
    "export OUTPUT_DIR=\"${BASE_DIR}/brainiac_p_outputs\"\n",
    "\n",
    "module load python/3.11\n",
    "module load opencv\n",
    "source /home/arelbaha/.venvs/brainiac_env/bin/activate\n",
    "\n",
    "#Batch\n",
    "BATCH_NUM=$(printf \"%03d\" ${SLURM_ARRAY_TASK_ID})\n",
    "BATCH_DIR=\"${RAW_DIR}/batch_${BATCH_NUM}\"\n",
    "BATCH_OUTPUT=\"${OUTPUT_DIR}/batch_${BATCH_NUM}\"\n",
    "\n",
    "mkdir -p \"$BATCH_OUTPUT\"\n",
    "echo \"Processing batch ${BATCH_NUM} from ${BATCH_DIR}\"\n",
    "\n",
    "#Preprocessing\n",
    "python /home/arelbaha/.venvs/brainiac_env/lib/python3.11/site-packages/BrainIAC/src/preprocessing/mri_preprocess_3d_simple.py \\\n",
    "    --temp_img /home/arelbaha/.venvs/brainiac_env/lib/python3.11/site-packages/BrainIAC/src/preprocessing/atlases/temp_head.nii.gz \\\n",
    "    --input_dir \"$BATCH_DIR\" \\\n",
    "    --output_dir \"$BATCH_OUTPUT\"\n",
    "\n",
    "echo \"Completed batch ${BATCH_NUM}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb41b4-95b0-4a46-9c0b-74079bdb4e0b",
   "metadata": {},
   "source": [
    "## HBN Multi-task Evaluation | AnatCl vs BrainIAC vs CNN vs FreeSurfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97843354-48ff-48e4-9083-dbc9f8dd226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/arelbaha/links/projects/rrg-glatard/arelbaha/debug_hbn_multimodel_comparison.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/arelbaha/links/projects/rrg-glatard/arelbaha/debug_hbn_multimodel_comparison.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fnmatch\n",
    "from nilearn import plotting, datasets\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import nibabel as nib\n",
    "from anatcl import AnatCL\n",
    "from monai.networks.nets import ViT\n",
    "import torch.nn.functional as F\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.ndimage as ndi\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.set_num_threads(1)\n",
    "torch.set_num_interop_threads(1)\n",
    "\n",
    "# PATHS\n",
    "HBN_BIDS = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/HBN_BIDS\"\n",
    "HBN_BIDS_LOWER = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/hbn_bids\"\n",
    "BRAINIAC_OUT = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/brainiac_p_outputs\"\n",
    "FREESURFER_DIR = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/HBN_FreeSurfer/freesurfer\"\n",
    "DEMO_FILE = os.path.join(HBN_BIDS, \"final_preprocessed_subjects_with_demographics.tsv\")\n",
    "ANATCL_ENCODER_PATH = \"/home/arelbaha/.venvs/jupyter_py3/bin\"\n",
    "BRAINIAC_CKPT = \"/home/arelbaha/.venvs/jupyter_py3/bin/BrainIAC.ckpt\"\n",
    "DROPOUT_RATE = 0.3\n",
    "PARCELLATION = \"Schaefer2018_400Parcels_17Networks_order\"\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "#Loading Demographics\n",
    "print(\"Loading demographics\")\n",
    "\n",
    "demo_df = pd.read_csv(DEMO_FILE, sep='\\t')\n",
    "demo_df['participant_id'] = demo_df['participant_id'].astype(str)\n",
    "\n",
    "id_sex_dict = {}\n",
    "id_age_dict = {}\n",
    "\n",
    "for _, row in demo_df.iterrows():\n",
    "    subject_id = row['participant_id']\n",
    "    sex = row['sex'].strip()\n",
    "    if sex == 'Female':\n",
    "        id_sex_dict[subject_id] = 1\n",
    "    elif sex == 'Male':\n",
    "        id_sex_dict[subject_id] = 0\n",
    "    id_age_dict[subject_id] = row['age']\n",
    "\n",
    "print(f\"Loaded demographics for {len(id_sex_dict)} subjects\")\n",
    "print(f\"Sex distribution: {sum(id_sex_dict.values())} Female, {len(id_sex_dict) - sum(id_sex_dict.values())} Male\")\n",
    "print(f\"Age range: {min(id_age_dict.values()):.1f} - {max(id_age_dict.values()):.1f} years\\n\")\n",
    "\n",
    "#Finding CAT12 Files\n",
    "print(\"Finding CAT12 (s6mwp1) files\")\n",
    "\n",
    "cat12_data = {}\n",
    "\n",
    "print(f\"Searching in {HBN_BIDS}...\")\n",
    "for subject_id in id_sex_dict.keys():\n",
    "    pattern = os.path.join(HBN_BIDS, f\"sub-{subject_id}\", \"ses-*\", \"anat\", \"mri\", \"s6mwp1sub*.nii\")\n",
    "    files = glob.glob(pattern)\n",
    "    if files:\n",
    "        cat12_data[subject_id] = files[0]\n",
    "\n",
    "print(f\"Searching in {HBN_BIDS_LOWER}...\")\n",
    "for subject_id in id_sex_dict.keys():\n",
    "    if subject_id not in cat12_data:\n",
    "        pattern = os.path.join(HBN_BIDS_LOWER, f\"sub-{subject_id}\", \"ses-*\", \"anat\", \"mri\", \"s6mwp1sub*.nii\")\n",
    "        files = glob.glob(pattern)\n",
    "        if files:\n",
    "            cat12_data[subject_id] = files[0]\n",
    "\n",
    "print(f\"Found {len(cat12_data)} CAT12 files\\n\")\n",
    "\n",
    "#Finding BrainIAC Files\n",
    "print(\"Finding BrainIAC preprocessed files\")\n",
    "\n",
    "brainiac_data = {}\n",
    "batch_dirs = glob.glob(os.path.join(BRAINIAC_OUT, \"batch_*\"))\n",
    "print(f\"Searching in {len(batch_dirs)} batch directories...\")\n",
    "\n",
    "for batch_dir in sorted(batch_dirs):\n",
    "    files = glob.glob(os.path.join(batch_dir, \"sub-*_0000.nii.gz\"))\n",
    "    for f in files:\n",
    "        basename = os.path.basename(f)\n",
    "        subject_id = basename.split('_')[0].replace('sub-', '')\n",
    "        \n",
    "        if subject_id in id_sex_dict and subject_id not in brainiac_data:\n",
    "            brainiac_data[subject_id] = f\n",
    "\n",
    "print(f\"Found {len(brainiac_data)} BrainIAC files\\n\")\n",
    "\n",
    "#Extracting FreeSurfer Features\n",
    "print(\"Extracting FreeSurfer features\")\n",
    "\n",
    "fs_data = {}\n",
    "\n",
    "for subject_id in id_sex_dict.keys():\n",
    "    subject_dir = os.path.join(FREESURFER_DIR, f\"sub-{subject_id}\")\n",
    "    stats_file = os.path.join(subject_dir, f\"sub-{subject_id}_regionsurfacestats.tsv\")\n",
    "    \n",
    "    if os.path.exists(stats_file):\n",
    "        try:\n",
    "            df = pd.read_csv(stats_file, sep='\\t')\n",
    "            filtered_df = df[df[\"atlas\"] == PARCELLATION]\n",
    "            \n",
    "            if not filtered_df.empty:\n",
    "                filtered_df = filtered_df.sort_values(\"StructName\")\n",
    "                \n",
    "                if \"SurfArea\" in filtered_df.columns and \"ThickAvg\" in filtered_df.columns:\n",
    "                    surf_area = filtered_df[\"SurfArea\"].values[:400]\n",
    "                    thick_avg = filtered_df[\"ThickAvg\"].values[:400]\n",
    "                    combined = np.concatenate([surf_area, thick_avg])\n",
    "                    \n",
    "                    if not np.any(np.isnan(combined)):\n",
    "                        fs_data[subject_id] = combined\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {stats_file}: {e}\")\n",
    "\n",
    "print(f\"Found {len(fs_data)} FreeSurfer subjects with 800 features (400 SurfArea + 400 ThickAvg)\\n\")\n",
    "\n",
    "#Overall Common Subjects\n",
    "print(\"Overall common subjects across all modalities\")\n",
    "\n",
    "common_subjects = sorted(list(\n",
    "    set(cat12_data.keys()) & \n",
    "    set(brainiac_data.keys()) & \n",
    "    set(fs_data.keys())\n",
    "))\n",
    "\n",
    "print(f\"Subjects with CAT12: {len(cat12_data)}\")\n",
    "print(f\"Subjects with BrainIAC: {len(brainiac_data)}\")\n",
    "print(f\"Subjects with FreeSurfer: {len(fs_data)}\")\n",
    "print(f\"Common subjects (all modalities): {len(common_subjects)}\\n\")\n",
    "\n",
    "if len(common_subjects) == 0:\n",
    "    print(\"ERROR: No common subjects found across all modalities\")\n",
    "    exit(1)\n",
    "\n",
    "cat12_paths = [cat12_data[s] for s in common_subjects]\n",
    "brainiac_paths = [brainiac_data[s] for s in common_subjects]\n",
    "fs_features = np.array([fs_data[s] for s in common_subjects])\n",
    "sex_labels = np.array([id_sex_dict[s] for s in common_subjects])\n",
    "age_labels = np.array([id_age_dict[s] for s in common_subjects])\n",
    "\n",
    "print(f\"Final matched dataset: {len(common_subjects)} subjects\")\n",
    "print(f\"FreeSurfer features shape: {fs_features.shape}\")\n",
    "print(f\"Sex labels: {np.sum(sex_labels)} Female, {len(sex_labels) - np.sum(sex_labels)} Male\")\n",
    "print(f\"Age: mean={age_labels.mean():.1f}, range={age_labels.min():.1f}-{age_labels.max():.1f}\\n\")\n",
    "\n",
    "pd.DataFrame({\n",
    "    'subject_id': common_subjects,\n",
    "    'cat12_path': cat12_paths,\n",
    "    'brainiac_path': brainiac_paths,\n",
    "    'sex': sex_labels,\n",
    "    'age': age_labels\n",
    "}).to_csv('hbn_matched_subjects.csv', index=False)\n",
    "print(\"Saved: hbn_matched_subjects.csv\\n\")\n",
    "\n",
    "#Dataset Classes\n",
    "\n",
    "class CAT12VBMDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform):\n",
    "        self.data, self.labels, self.transform = data, labels, transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.transform(nib.load(self.data[idx]).get_fdata()).unsqueeze(0)\n",
    "        return img, torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "class BrainIACDataset(Dataset):\n",
    "    def __init__(self, paths, labels, transform):\n",
    "        self.paths, self.labels, self.transform = paths, labels, transform\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.paths[idx]).get_fdata().astype(np.float32)\n",
    "        if img.shape != (96, 96, 96):\n",
    "            img = F.interpolate(torch.from_numpy(img[None, None]), size=(96, 96, 96), \n",
    "                              mode='trilinear', align_corners=False).squeeze().numpy()\n",
    "        img_tensor = torch.from_numpy(img)\n",
    "        img_normalized = self.transform(img_tensor)\n",
    "        return img_normalized[None], torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "class CNNDataset(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.paths[idx]).get_fdata().astype(np.float32)\n",
    "        if img.shape != (92, 110, 92):\n",
    "            img = ndi.zoom(img, [92/img.shape[0], 110/img.shape[1], 92/img.shape[2]], order=1)\n",
    "        img = (img - img.mean()) / (img.std() + 1e-6)\n",
    "        return torch.from_numpy(img[None]).float()\n",
    "\n",
    "class CNN3D(nn.Module):\n",
    "    def __init__(self, dropout_rate=DROPOUT_RATE):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 64, 3, padding=1)\n",
    "        self.bn1, self.pool1, self.drop1 = nn.BatchNorm3d(64), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv2 = nn.Conv3d(64, 64, 3, padding=1)\n",
    "        self.bn2, self.pool2, self.drop2 = nn.BatchNorm3d(64), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv3 = nn.Conv3d(64, 128, 3, padding=1)\n",
    "        self.bn3, self.pool3, self.drop3 = nn.BatchNorm3d(128), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv4 = nn.Conv3d(128, 256, 3, padding=1)\n",
    "        self.bn4, self.pool4, self.drop4 = nn.BatchNorm3d(256), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc1, self.drop5 = nn.Linear(256, 512), nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop1(self.bn1(self.pool1(F.relu(self.conv1(x)))))\n",
    "        x = self.drop2(self.bn2(self.pool2(F.relu(self.conv2(x)))))\n",
    "        x = self.drop3(self.bn3(self.pool3(F.relu(self.conv3(x)))))\n",
    "        x = self.drop4(self.bn4(self.pool4(F.relu(self.conv4(x)))))\n",
    "        return self.drop5(F.relu(self.fc1(self.global_pool(x).view(x.size(0), -1))))\n",
    "\n",
    "def load_brainiac_vit(ckpt_path, device):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    vit = ViT(in_channels=1, img_size=(96, 96, 96), patch_size=(16, 16, 16),\n",
    "              hidden_size=768, mlp_dim=3072, num_layers=12, num_heads=12, classification=False)\n",
    "    pe_key = \"backbone.patch_embedding.position_embeddings\"\n",
    "    if pe_key in ckpt.get(\"state_dict\", ckpt):\n",
    "        vit.patch_embedding.position_embeddings = nn.Parameter(ckpt[\"state_dict\"][pe_key].clone())\n",
    "    backbone_state = {k.replace(\"backbone.\", \"\"): v for k, v in ckpt.get(\"state_dict\", ckpt).items() \n",
    "                     if k.startswith(\"backbone.\")}\n",
    "    vit.load_state_dict(backbone_state, strict=False)\n",
    "    return vit.to(device).eval()\n",
    "\n",
    "#Extracting AnatCL Features (All 5 folds, averaged)\n",
    "print(\"Extracting AnatCL features (averaging across 5 cross-validation folds)\")\n",
    "\n",
    "anatcl_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: torch.from_numpy(x.copy()).float()),\n",
    "    transforms.Normalize(mean=0.0, std=1.0)\n",
    "])\n",
    "\n",
    "num_folds = 5\n",
    "all_fold_features = []\n",
    "\n",
    "for fold_idx in range(num_folds):\n",
    "    path = os.path.join(ANATCL_ENCODER_PATH, f\"fold{fold_idx}.pth\")\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"ERROR: fold{fold_idx}.pth not found at {path}\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"Loading fold {fold_idx}...\")\n",
    "    encoder = AnatCL(descriptor=\"global\", fold=fold_idx, pretrained=False).to(device).eval()\n",
    "    encoder.backbone.load_state_dict(torch.load(path, map_location=device, weights_only=False)['model'])\n",
    "    \n",
    "    for p in encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    dl = DataLoader(CAT12VBMDataset(cat12_paths, age_labels, anatcl_transform), \n",
    "                    batch_size=32, num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fold_features = torch.cat([encoder(vol.to(device)).cpu() for vol, _ in dl]).numpy()\n",
    "    \n",
    "    all_fold_features.append(fold_features)\n",
    "    print(f\"  Fold {fold_idx} features: {fold_features.shape}\")\n",
    "    \n",
    "    del encoder\n",
    "\n",
    "anatcl_features = np.mean(all_fold_features, axis=0)\n",
    "\n",
    "print(f\"\\nAnatCL features (averaged across {num_folds} folds): {anatcl_features.shape}\\n\")\n",
    "\n",
    "#Extracting BrainIAC Features\n",
    "print(\"Extracting BrainIAC features\")\n",
    "\n",
    "brainiac_vit = load_brainiac_vit(BRAINIAC_CKPT, device)\n",
    "brainiac_transform = lambda x: (x - x.mean()) / (x.std() + 1e-6)\n",
    "dl = DataLoader(BrainIACDataset(brainiac_paths, age_labels, brainiac_transform), \n",
    "                batch_size=16, num_workers=0)\n",
    "\n",
    "brainiac_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, _ in dl:\n",
    "        out = brainiac_vit(x.to(device))\n",
    "        \n",
    "        #Handle tuple or tensor\n",
    "        if isinstance(out, tuple):\n",
    "            cls_token = out[0][:, 0]  # First element, CLS token\n",
    "        else:\n",
    "            cls_token = out[:, 0]  # CLS token directly\n",
    "        \n",
    "        brainiac_features.append(cls_token.cpu().numpy())\n",
    "\n",
    "brainiac_features = np.vstack(brainiac_features)\n",
    "\n",
    "print(f\"BrainIAC features: {brainiac_features.shape}\")\n",
    "del brainiac_vit\n",
    "\n",
    "#Extracting CNN Features\n",
    "print(\"Extracting CNN features\")\n",
    "\n",
    "cnn_model = CNN3D().to(device).eval()\n",
    "dl = DataLoader(CNNDataset(brainiac_paths), batch_size=8, num_workers=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    cnn_features = np.vstack([cnn_model(x.to(device)).cpu().numpy() for x in dl])\n",
    "\n",
    "print(f\"CNN features: {cnn_features.shape}\\n\")\n",
    "del cnn_model\n",
    "\n",
    "#Organize Features\n",
    "features_dict = {\n",
    "    'AnatCL': anatcl_features,\n",
    "    'BrainIAC': brainiac_features,\n",
    "    'CNN': cnn_features,\n",
    "    'FreeSurfer': fs_features\n",
    "}\n",
    "print(\"Feature extraction complete\")\n",
    "for name, feats in features_dict.items():\n",
    "    print(f\"{name}: {feats.shape}\")\n",
    "print()\n",
    "\n",
    "#Correlation Analysis\n",
    "print(\"Computing cross-model feature correlations\")\n",
    "\n",
    "def compute_correlation_within_models(features_dict):\n",
    "    all_features = []\n",
    "    boundaries = [0]\n",
    "    reordered_indices = {}\n",
    "\n",
    "    for model in ['AnatCL', 'BrainIAC', 'CNN', 'FreeSurfer']:\n",
    "        if model not in features_dict:\n",
    "            continue\n",
    "\n",
    "        feats = features_dict[model]\n",
    "        feats = StandardScaler().fit_transform(feats)\n",
    "\n",
    "        valid = ~np.isnan(feats).any(axis=0) & (np.std(feats, axis=0) > 1e-10)\n",
    "        feats = feats[:, valid]\n",
    "\n",
    "        if feats.shape[1] == 0:\n",
    "            continue\n",
    "\n",
    "        print(f\"  {model}: {feats.shape[1]} features\")\n",
    "\n",
    "        corr = np.corrcoef(feats.T)\n",
    "        corr = np.nan_to_num((corr + corr.T) / 2, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        np.fill_diagonal(corr, 1.0)\n",
    "\n",
    "        dist = np.abs(1 - np.abs(corr))\n",
    "        np.fill_diagonal(dist, 0)\n",
    "        condensed = np.nan_to_num(squareform((dist + dist.T) / 2, checks=False), \n",
    "                                  nan=0.0, posinf=1.0, neginf=0.0)\n",
    "\n",
    "        reorder_idx = dendrogram(linkage(condensed, method='ward'), no_plot=True)['leaves']\n",
    "        all_features.append(feats[:, reorder_idx])\n",
    "        boundaries.append(boundaries[-1] + feats.shape[1])\n",
    "        reordered_indices[model] = reorder_idx\n",
    "\n",
    "    combined = np.hstack(all_features)\n",
    "    corr = np.nan_to_num((np.corrcoef(combined.T) + np.corrcoef(combined.T).T) / 2, \n",
    "                         nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "\n",
    "    print(f\"\\nTotal features: {combined.shape[1]}\")\n",
    "    \n",
    "    return corr, boundaries, ['AnatCL', 'BrainIAC', 'CNN', 'FreeSurfer'], reordered_indices\n",
    "\n",
    "corr_matrix, boundaries, model_names, reorder_indices = compute_correlation_within_models(features_dict)\n",
    "pd.DataFrame(corr_matrix).to_csv('hbn_feature_correlation_matrix.csv', index=False)\n",
    "print(f\"Correlation matrix: {corr_matrix.shape}\")\n",
    "print(f\"Model boundaries: {boundaries}\\n\")\n",
    "\n",
    "# Plot correlation matrix (FULL SIZE)\n",
    "print(\"Plotting cross-model correlation matrix...\")\n",
    "\n",
    "plt.figure(figsize=(40, 38))\n",
    "sns.heatmap(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1, center=0, square=True, \n",
    "            linewidths=0, cbar_kws={\"shrink\": 0.3},\n",
    "            xticklabels=False, yticklabels=False)\n",
    "\n",
    "for b in boundaries[1:-1]:\n",
    "    plt.axhline(y=b, color='black', linewidth=4)\n",
    "    plt.axvline(x=b, color='black', linewidth=4)\n",
    "\n",
    "for i, (pos, name) in enumerate(zip([(boundaries[i] + boundaries[i+1]) / 2 \n",
    "                                      for i in range(len(boundaries)-1)], model_names)):\n",
    "    plt.text(pos, -30, name, ha='center', fontsize=24, weight='bold')\n",
    "    plt.text(-30, pos, name, ha='center', va='center', fontsize=24, weight='bold', rotation=90)\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.savefig('hbn_cross_model_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: hbn_cross_model_correlation.png\\n\")\n",
    "\n",
    "#Model Training Functions\n",
    "\n",
    "def run_classification(X_dict, y, test_size=0.1, task_name=\"Classification\"):\n",
    "    print(f\"Running {task_name}...\")\n",
    "    results = {}\n",
    "    cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)), \n",
    "                                        stratify=y, random_state=SEED)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    alphas = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "    print(\"  Training DummyClassifier baseline...\")\n",
    "    dummy_val_auc = []\n",
    "    dummy_val_acc = []\n",
    "    \n",
    "    for train_rel, val_rel in skf.split(cv_idx, y[cv_idx]):\n",
    "        train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "        dummy = DummyClassifier(strategy='stratified', random_state=SEED)\n",
    "        dummy.fit(np.zeros((len(train_idx), 1)), y[train_idx])\n",
    "        \n",
    "        pred = dummy.predict(np.zeros((len(val_idx), 1)))\n",
    "        pred_proba = dummy.predict_proba(np.zeros((len(val_idx), 1)))[:, 1]\n",
    "        \n",
    "        dummy_val_acc.append(balanced_accuracy_score(y[val_idx], pred))\n",
    "        dummy_val_auc.append(roc_auc_score(y[val_idx], pred_proba))\n",
    "    \n",
    "    dummy = DummyClassifier(strategy='stratified', random_state=SEED)\n",
    "    dummy.fit(np.zeros((len(cv_idx), 1)), y[cv_idx])\n",
    "    dummy_test_pred = dummy.predict(np.zeros((len(test_idx), 1)))\n",
    "    dummy_test_proba = dummy.predict_proba(np.zeros((len(test_idx), 1)))[:, 1]\n",
    "    \n",
    "    results['DummyClassifier'] = {\n",
    "        'best_alpha': 1.0,\n",
    "        'test_balanced_accuracy': balanced_accuracy_score(y[test_idx], dummy_test_pred),\n",
    "        'test_auc': roc_auc_score(y[test_idx], dummy_test_proba),\n",
    "        'cv_auc_mean': np.mean(dummy_val_auc),\n",
    "        'cv_auc_std': np.std(dummy_val_auc),\n",
    "        'cv_acc_mean': np.mean(dummy_val_acc),\n",
    "        'cv_acc_std': np.std(dummy_val_acc),\n",
    "        'fold_results': {'val_auc': dummy_val_auc, 'val_acc': dummy_val_acc}\n",
    "    }\n",
    "    print(f\"  DummyClassifier: AUC={results['DummyClassifier']['test_auc']:.4f}, Bal_Acc={results['DummyClassifier']['test_balanced_accuracy']:.4f}\")\n",
    "\n",
    "    for model, X in X_dict.items():\n",
    "        val_auc = {a: [] for a in alphas}\n",
    "        val_acc = {a: [] for a in alphas}\n",
    "        train_auc = {a: [] for a in alphas}\n",
    "        train_acc = {a: [] for a in alphas}\n",
    "\n",
    "        for train_rel, val_rel in skf.split(cv_idx, y[cv_idx]):\n",
    "            train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "\n",
    "            for alpha in alphas:\n",
    "                n = max(4, int(alpha * len(train_idx)))\n",
    "                try:\n",
    "                    tr_idx, _ = train_test_split(train_idx, train_size=n, stratify=y[train_idx], random_state=SEED)\n",
    "                except:\n",
    "                    tr_idx = np.random.choice(train_idx, n, replace=False)\n",
    "\n",
    "                if len(np.unique(y[tr_idx])) < 2:\n",
    "                    continue\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                rf = RandomForestClassifier(n_estimators=500, random_state=SEED, n_jobs=-1, \n",
    "                                           max_features='sqrt', class_weight='balanced')\n",
    "                rf.fit(scaler.fit_transform(X[tr_idx]), y[tr_idx])\n",
    "\n",
    "                pred_proba = rf.predict_proba(scaler.transform(X[val_idx]))[:, 1]\n",
    "                val_acc[alpha].append(balanced_accuracy_score(y[val_idx], rf.predict(scaler.transform(X[val_idx]))))\n",
    "                val_auc[alpha].append(roc_auc_score(y[val_idx], pred_proba))\n",
    "                \n",
    "                train_pred_proba = rf.predict_proba(scaler.transform(X[tr_idx]))[:, 1]\n",
    "                train_acc[alpha].append(balanced_accuracy_score(y[tr_idx], rf.predict(scaler.transform(X[tr_idx]))))\n",
    "                train_auc[alpha].append(roc_auc_score(y[tr_idx], train_pred_proba))\n",
    "\n",
    "        avg_auc = {a: np.mean(val_auc[a]) for a in alphas if val_auc[a]}\n",
    "        best_alpha = max(avg_auc, key=avg_auc.get)\n",
    "\n",
    "        try:\n",
    "            final_tr, _ = train_test_split(cv_idx, train_size=max(4, int(best_alpha * len(cv_idx))), \n",
    "                                          stratify=y[cv_idx], random_state=SEED)\n",
    "        except:\n",
    "            final_tr = cv_idx\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        rf = RandomForestClassifier(n_estimators=500, random_state=SEED, n_jobs=-1, \n",
    "                                    max_features='sqrt', class_weight='balanced')\n",
    "        rf.fit(scaler.fit_transform(X[final_tr]), y[final_tr])\n",
    "        test_pred_proba = rf.predict_proba(scaler.transform(X[test_idx]))[:, 1]\n",
    "\n",
    "        train_val_gap = np.mean(train_auc[best_alpha]) - np.mean(val_auc[best_alpha])\n",
    "        is_overfitting = train_val_gap > 0.25\n",
    "\n",
    "        results[model] = {\n",
    "            'best_alpha': best_alpha,\n",
    "            'test_balanced_accuracy': balanced_accuracy_score(y[test_idx], rf.predict(scaler.transform(X[test_idx]))),\n",
    "            'test_auc': roc_auc_score(y[test_idx], test_pred_proba),\n",
    "            'cv_auc_mean': np.mean(val_auc[best_alpha]),\n",
    "            'cv_auc_std': np.std(val_auc[best_alpha]),\n",
    "            'cv_acc_mean': np.mean(val_acc[best_alpha]),\n",
    "            'cv_acc_std': np.std(val_acc[best_alpha]),\n",
    "            'train_auc_mean': np.mean(train_auc[best_alpha]),\n",
    "            'train_val_gap': train_val_gap,\n",
    "            'is_overfitting': is_overfitting,\n",
    "            'fold_results': {\n",
    "                'val_auc': val_auc[best_alpha],\n",
    "                'val_acc': val_acc[best_alpha],\n",
    "                'train_auc': train_auc[best_alpha]\n",
    "            },\n",
    "            'cv_results': {\n",
    "                'alphas': alphas, \n",
    "                'acc': {a: np.mean(val_acc[a]) for a in alphas if val_acc[a]},\n",
    "                'auc': avg_auc,\n",
    "                'acc_std': {a: np.std(val_acc[a]) for a in alphas if val_acc[a]},\n",
    "                'auc_std': {a: np.std(val_auc[a]) for a in alphas if val_auc[a]}\n",
    "            }\n",
    "        }\n",
    "        print(f\"  {model}: AUC={results[model]['test_auc']:.4f}, Bal_Acc={results[model]['test_balanced_accuracy']:.4f}\" + \n",
    "              (f\" [OVERFITTING: train-val gap={train_val_gap:.3f}]\" if is_overfitting else \"\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_regression(X_dict, y, test_size=0.1):\n",
    "    print(\"Running age prediction...\")\n",
    "    results = {}\n",
    "    cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)), random_state=SEED)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    alphas = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "    print(\"  Training DummyRegressor baseline...\")\n",
    "    dummy_val_r2 = []\n",
    "    dummy_val_mae = []\n",
    "    \n",
    "    for train_rel, val_rel in kf.split(cv_idx):\n",
    "        train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "        dummy = DummyRegressor(strategy='mean')\n",
    "        dummy.fit(np.zeros((len(train_idx), 1)), y[train_idx])\n",
    "        \n",
    "        pred = dummy.predict(np.zeros((len(val_idx), 1)))\n",
    "        dummy_val_r2.append(r2_score(y[val_idx], pred))\n",
    "        dummy_val_mae.append(mean_absolute_error(y[val_idx], pred))\n",
    "    \n",
    "    dummy = DummyRegressor(strategy='mean')\n",
    "    dummy.fit(np.zeros((len(cv_idx), 1)), y[cv_idx])\n",
    "    dummy_test_pred = dummy.predict(np.zeros((len(test_idx), 1)))\n",
    "    \n",
    "    results['DummyRegressor'] = {\n",
    "        'best_alpha': 1.0,\n",
    "        'test_r2': r2_score(y[test_idx], dummy_test_pred),\n",
    "        'test_mae': mean_absolute_error(y[test_idx], dummy_test_pred),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y[test_idx], dummy_test_pred)),\n",
    "        'cv_r2_mean': np.mean(dummy_val_r2),\n",
    "        'cv_r2_std': np.std(dummy_val_r2),\n",
    "        'cv_mae_mean': np.mean(dummy_val_mae),\n",
    "        'cv_mae_std': np.std(dummy_val_mae),\n",
    "        'fold_results': {'val_r2': dummy_val_r2, 'val_mae': dummy_val_mae}\n",
    "    }\n",
    "    print(f\"  DummyRegressor: R2={results['DummyRegressor']['test_r2']:.4f}, MAE={results['DummyRegressor']['test_mae']:.2f}\")\n",
    "\n",
    "    for model, X in X_dict.items():\n",
    "        val_r2 = {a: [] for a in alphas}\n",
    "        val_mae = {a: [] for a in alphas}\n",
    "        train_r2 = {a: [] for a in alphas}\n",
    "        train_mae = {a: [] for a in alphas}\n",
    "\n",
    "        for train_rel, val_rel in kf.split(cv_idx):\n",
    "            train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "\n",
    "            for alpha in alphas:\n",
    "                n = max(40, int(alpha * len(train_idx)))\n",
    "                tr_idx, _ = train_test_split(train_idx, train_size=n, random_state=SEED) \\\n",
    "                           if n < len(train_idx) else (train_idx, None)\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                rf = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_split=5, \n",
    "                                          random_state=SEED, n_jobs=-1)\n",
    "                rf.fit(scaler.fit_transform(X[tr_idx]), y[tr_idx])\n",
    "\n",
    "                pred = rf.predict(scaler.transform(X[val_idx]))\n",
    "                val_r2[alpha].append(r2_score(y[val_idx], pred))\n",
    "                val_mae[alpha].append(mean_absolute_error(y[val_idx], pred))\n",
    "                \n",
    "                train_pred = rf.predict(scaler.transform(X[tr_idx]))\n",
    "                train_r2[alpha].append(r2_score(y[tr_idx], train_pred))\n",
    "                train_mae[alpha].append(mean_absolute_error(y[tr_idx], train_pred))\n",
    "\n",
    "        avg_r2 = {a: np.mean(val_r2[a]) for a in alphas if val_r2[a]}\n",
    "        best_alpha = max(avg_r2, key=avg_r2.get)\n",
    "\n",
    "        n = max(40, int(best_alpha * len(cv_idx)))\n",
    "        final_tr, _ = train_test_split(cv_idx, train_size=n, random_state=SEED) \\\n",
    "                     if n < len(cv_idx) else (cv_idx, None)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        rf = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_split=5, random_state=SEED, n_jobs=-1)\n",
    "        rf.fit(scaler.fit_transform(X[final_tr]), y[final_tr])\n",
    "        test_pred = rf.predict(scaler.transform(X[test_idx]))\n",
    "\n",
    "        train_val_gap = np.mean(train_r2[best_alpha]) - np.mean(val_r2[best_alpha])\n",
    "        is_overfitting = train_val_gap > 0.35\n",
    "\n",
    "        results[model] = {\n",
    "            'best_alpha': best_alpha,\n",
    "            'test_r2': r2_score(y[test_idx], test_pred),\n",
    "            'test_mae': mean_absolute_error(y[test_idx], test_pred),\n",
    "            'test_rmse': np.sqrt(mean_squared_error(y[test_idx], test_pred)),\n",
    "            'predictions': test_pred,\n",
    "            'actual': y[test_idx],\n",
    "            'cv_r2_mean': np.mean(val_r2[best_alpha]),\n",
    "            'cv_r2_std': np.std(val_r2[best_alpha]),\n",
    "            'cv_mae_mean': np.mean(val_mae[best_alpha]),\n",
    "            'cv_mae_std': np.std(val_mae[best_alpha]),\n",
    "            'train_r2_mean': np.mean(train_r2[best_alpha]),\n",
    "            'train_val_gap': train_val_gap,\n",
    "            'is_overfitting': is_overfitting,\n",
    "            'fold_results': {\n",
    "                'val_r2': val_r2[best_alpha],\n",
    "                'val_mae': val_mae[best_alpha],\n",
    "                'train_r2': train_r2[best_alpha]\n",
    "            },\n",
    "            'cv_results': {\n",
    "                'alphas': alphas, \n",
    "                'r2': avg_r2, \n",
    "                'mae': {a: np.mean(val_mae[a]) for a in alphas if val_mae[a]},\n",
    "                'r2_std': {a: np.std(val_r2[a]) for a in alphas if val_r2[a]},\n",
    "                'mae_std': {a: np.std(val_mae[a]) for a in alphas if val_mae[a]}\n",
    "            }\n",
    "        }\n",
    "        print(f\"  {model}: R2={results[model]['test_r2']:.4f}, MAE={results[model]['test_mae']:.2f}\" +\n",
    "              (f\" [OVERFITTING: train-val gap={train_val_gap:.3f}]\" if is_overfitting else \"\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "#Run Sex Classification\n",
    "print(\"Sex Classification\")\n",
    "\n",
    "sex_results = run_classification(features_dict, sex_labels, test_size=0.1, task_name=\"Sex classification\")\n",
    "print()\n",
    "\n",
    "#Run Age Prediction\n",
    "print(\"Age Prediction\")\n",
    "\n",
    "age_results = run_regression(features_dict, age_labels, test_size=0.1)\n",
    "print()\n",
    "\n",
    "#Overfitting Analysis\n",
    "print(\"Overfitting Analysis\")\n",
    "\n",
    "overfitting_report = []\n",
    "for model in features_dict.keys():\n",
    "    sex_overfit = sex_results[model]['is_overfitting']\n",
    "    age_overfit = age_results[model]['is_overfitting']\n",
    "    \n",
    "    overfitting_report.append({\n",
    "        'Model': model,\n",
    "        'Sex_Overfitting': sex_overfit,\n",
    "        'Sex_Train_Val_Gap': sex_results[model]['train_val_gap'],\n",
    "        'Age_Overfitting': age_overfit,\n",
    "        'Age_Train_Val_Gap': age_results[model]['train_val_gap']\n",
    "    })\n",
    "    \n",
    "    if sex_overfit or age_overfit:\n",
    "        print(f\"{model}:\")\n",
    "        if sex_overfit:\n",
    "            print(f\"  Sex classification: Train-Val AUC gap = {sex_results[model]['train_val_gap']:.3f}\")\n",
    "        if age_overfit:\n",
    "            print(f\"  Age prediction: Train-Val RÂ² gap = {age_results[model]['train_val_gap']:.3f}\")\n",
    "\n",
    "if not any(r['Sex_Overfitting'] or r['Age_Overfitting'] for r in overfitting_report):\n",
    "    print(\"No significant overfitting detected\")\n",
    "\n",
    "pd.DataFrame(overfitting_report).to_csv('hbn_overfitting_analysis.csv', index=False)\n",
    "print(\"\\nSaved: hbn_overfitting_analysis.csv\\n\")\n",
    "\n",
    "#Saving results\n",
    "print(\"=\"*80)\n",
    "print(\"Saving Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_models = ['DummyClassifier'] + list(features_dict.keys())\n",
    "\n",
    "pd.DataFrame([{'Model': m, 'Best_Alpha': sex_results[m]['best_alpha'], \n",
    "               'Test_Balanced_Accuracy': sex_results[m]['test_balanced_accuracy'], \n",
    "               'Test_AUC': sex_results[m]['test_auc'],\n",
    "               'CV_AUC_Mean': sex_results[m]['cv_auc_mean'],\n",
    "               'CV_AUC_Std': sex_results[m]['cv_auc_std']} \n",
    "              for m in all_models]).to_csv('hbn_sex_classification_summary.csv', index=False)\n",
    "\n",
    "all_models_reg = ['DummyRegressor'] + list(features_dict.keys())\n",
    "\n",
    "pd.DataFrame([{'Model': m, 'Best_Alpha': age_results[m]['best_alpha'], \n",
    "               'Test_R2': age_results[m]['test_r2'], \n",
    "               'Test_MAE': age_results[m]['test_mae'], \n",
    "               'Test_RMSE': age_results[m]['test_rmse'],\n",
    "               'CV_R2_Mean': age_results[m]['cv_r2_mean'],\n",
    "               'CV_R2_Std': age_results[m]['cv_r2_std'],\n",
    "               'CV_MAE_Mean': age_results[m]['cv_mae_mean'],\n",
    "               'CV_MAE_Std': age_results[m]['cv_mae_std']} \n",
    "              for m in all_models_reg]).to_csv('hbn_age_prediction_summary.csv', index=False)\n",
    "\n",
    "all_results = [\n",
    "    {'Model': m, \n",
    "     'Sex_AUC': sex_results[m]['test_auc'], \n",
    "     'Sex_Bal_Acc': sex_results[m]['test_balanced_accuracy'],\n",
    "     'Age_R2': age_results[m]['test_r2'], \n",
    "     'Age_MAE': age_results[m]['test_mae'], \n",
    "     'Age_RMSE': age_results[m]['test_rmse']} \n",
    "    for m in list(features_dict.keys())\n",
    "]\n",
    "pd.DataFrame(all_results).to_csv('hbn_detailed_comparison.csv', index=False)\n",
    "\n",
    "rankings = []\n",
    "for task in ['Sex_AUC', 'Age_R2']:\n",
    "    for rank, item in enumerate(sorted(all_results, key=lambda x: x[task], reverse=True), 1):\n",
    "        rankings.append({\n",
    "            'Task': task.replace('_', ' '), \n",
    "            'Rank': rank, \n",
    "            'Model': item['Model'], \n",
    "            'Score': item[task]\n",
    "        })\n",
    "pd.DataFrame(rankings).to_csv('hbn_model_rankings.csv', index=False)\n",
    "\n",
    "print(\"Saved: hbn_sex_classification_summary.csv\")\n",
    "print(\"Saved: hbn_age_prediction_summary.csv\")\n",
    "print(\"Saved: hbn_detailed_comparison.csv\")\n",
    "print(\"Saved: hbn_model_rankings.csv\\n\")\n",
    "\n",
    "#Generate Learning Curve Visualizations\n",
    "print(\"=\"*80)\n",
    "print(\"Generating Learning Curves\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models = list(features_dict.keys())\n",
    "x_pos = np.arange(len(models))\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
    "\n",
    "n_cv = len(age_labels) - int(0.1 * len(age_labels))\n",
    "alphas = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "training_sizes = [int(alpha * n_cv * 0.8) for alpha in alphas]\n",
    "\n",
    "# Learning curves WITH SHADED ERROR REGIONS\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Sex classification learning curve\n",
    "for idx, model in enumerate(models):\n",
    "    acc_means = []\n",
    "    acc_stds = []\n",
    "    valid_sizes = []\n",
    "    \n",
    "    for i, alpha in enumerate(alphas):\n",
    "        if alpha in sex_results[model]['cv_results']['acc']:\n",
    "            acc_means.append(sex_results[model]['cv_results']['acc'][alpha])\n",
    "            acc_stds.append(sex_results[model]['cv_results']['acc_std'][alpha])\n",
    "            valid_sizes.append(training_sizes[i])\n",
    "    \n",
    "    acc_means = np.array(acc_means)\n",
    "    acc_stds = np.array(acc_stds)\n",
    "    valid_sizes = np.array(valid_sizes)\n",
    "    \n",
    "    ax1.plot(valid_sizes, acc_means, 'o-', linewidth=2.5, markersize=8, \n",
    "             label=model, color=colors[idx])\n",
    "    ax1.fill_between(valid_sizes, acc_means - acc_stds, acc_means + acc_stds, \n",
    "                     alpha=0.25, color=colors[idx])\n",
    "\n",
    "dummy_acc_cv = sex_results['DummyClassifier']['cv_acc_mean']\n",
    "ax1.axhline(y=dummy_acc_cv, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Dummy Classifier ({dummy_acc_cv:.3f})', alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Number of Training Subjects', fontsize=12, weight='bold')\n",
    "ax1.set_ylabel('Balanced Accuracy', fontsize=12, weight='bold')\n",
    "ax1.set_title('Sex Classification: CV Learning Curve', fontsize=14, weight='bold')\n",
    "ax1.legend(fontsize=10, loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Age prediction learning curve\n",
    "for idx, model in enumerate(models):\n",
    "    mae_means = []\n",
    "    mae_stds = []\n",
    "    valid_sizes = []\n",
    "    \n",
    "    for i, alpha in enumerate(alphas):\n",
    "        if alpha in age_results[model]['cv_results']['mae']:\n",
    "            mae_means.append(age_results[model]['cv_results']['mae'][alpha])\n",
    "            mae_stds.append(age_results[model]['cv_results']['mae_std'][alpha])\n",
    "            valid_sizes.append(training_sizes[i])\n",
    "    \n",
    "    mae_means = np.array(mae_means)\n",
    "    mae_stds = np.array(mae_stds)\n",
    "    valid_sizes = np.array(valid_sizes)\n",
    "    \n",
    "    ax2.plot(valid_sizes, mae_means, 'o-', linewidth=2.5, markersize=8, \n",
    "             label=model, color=colors[idx])\n",
    "    ax2.fill_between(valid_sizes, mae_means - mae_stds, mae_means + mae_stds, \n",
    "                     alpha=0.25, color=colors[idx])\n",
    "\n",
    "dummy_mae_cv = age_results['DummyRegressor']['cv_mae_mean']\n",
    "ax2.axhline(y=dummy_mae_cv, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Dummy Regressor ({dummy_mae_cv:.2f})', alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('Number of Training Subjects', fontsize=12, weight='bold')\n",
    "ax2.set_ylabel('MAE (years)', fontsize=12, weight='bold')\n",
    "ax2.set_title('Age Prediction: CV MAE Learning Curve', fontsize=14, weight='bold')\n",
    "ax2.legend(fontsize=10, loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hbn_cv_learning_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: hbn_cv_learning_curves.png\\n\")\n",
    "\n",
    "\n",
    "print(f\"Dataset: {len(common_subjects)} subjects\")\n",
    "print(f\"Age range: {age_labels.min():.1f} - {age_labels.max():.1f} years\")\n",
    "print(f\"Sex: {np.sum(sex_labels)} Female, {len(sex_labels) - np.sum(sex_labels)} Male\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a31d65-ae1b-44b1-9d28-d44267569b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/arelbaha/links/projects/rrg-glatard/arelbaha/hbn_fs_save_freesurfer_data.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "FREESURFER_DIR = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/HBN_FreeSurfer/freesurfer\"\n",
    "DEMO_FILE = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/HBN_BIDS/final_preprocessed_subjects_with_demographics.tsv\"\n",
    "OUTPUT_DIR = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/freesurfer_importance_outputs\"\n",
    "PARCELLATION = \"Schaefer2018_400Parcels_17Networks_order\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Loading demographics...\")\n",
    "demo_df = pd.read_csv(DEMO_FILE, sep='\\t')\n",
    "demo_df['participant_id'] = demo_df['participant_id'].astype(str)\n",
    "\n",
    "id_sex_dict = {}\n",
    "id_age_dict = {}\n",
    "\n",
    "for _, row in demo_df.iterrows():\n",
    "    subject_id = row['participant_id']\n",
    "    sex = row['sex'].strip()\n",
    "    if sex == 'Female':\n",
    "        id_sex_dict[subject_id] = 1\n",
    "    elif sex == 'Male':\n",
    "        id_sex_dict[subject_id] = 0\n",
    "    id_age_dict[subject_id] = row['age']\n",
    "\n",
    "print(f\"Loaded demographics for {len(id_sex_dict)} subjects\")\n",
    "print(f\"Sex distribution: {sum(id_sex_dict.values())} Female, {len(id_sex_dict) - sum(id_sex_dict.values())} Male\")\n",
    "print(f\"Age range: {min(id_age_dict.values()):.1f} - {max(id_age_dict.values()):.1f} years\\n\")\n",
    "\n",
    "print(\"Extracting FreeSurfer features...\")\n",
    "fs_data = {}\n",
    "\n",
    "for subject_id in id_sex_dict.keys():\n",
    "    stats_file = os.path.join(FREESURFER_DIR, f\"sub-{subject_id}\", \n",
    "                              f\"sub-{subject_id}_regionsurfacestats.tsv\")\n",
    "    \n",
    "    if os.path.exists(stats_file):\n",
    "        try:\n",
    "            df = pd.read_csv(stats_file, sep='\\t')\n",
    "            filtered_df = df[df[\"atlas\"] == PARCELLATION].sort_values(\"StructName\")\n",
    "            \n",
    "            if not filtered_df.empty and \"SurfArea\" in filtered_df.columns and \"ThickAvg\" in filtered_df.columns:\n",
    "                surf_area = filtered_df[\"SurfArea\"].values[:400]\n",
    "                thick_avg = filtered_df[\"ThickAvg\"].values[:400]\n",
    "                \n",
    "                if len(surf_area) == 400 and len(thick_avg) == 400:\n",
    "                    combined = np.concatenate([surf_area, thick_avg])\n",
    "                    \n",
    "                    if not np.any(np.isnan(combined)):\n",
    "                        fs_data[subject_id] = combined\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {subject_id}: {e}\")\n",
    "\n",
    "print(f\"Found {len(fs_data)} subjects with FreeSurfer data\\n\")\n",
    "\n",
    "if len(fs_data) == 0:\n",
    "    print(\"ERROR: No FreeSurfer data found!\")\n",
    "    exit(1)\n",
    "\n",
    "common_subjects = sorted(list(fs_data.keys()))\n",
    "fs_features = np.array([fs_data[s] for s in common_subjects])\n",
    "sex_labels = np.array([id_sex_dict[s] for s in common_subjects])\n",
    "age_labels = np.array([id_age_dict[s] for s in common_subjects])\n",
    "\n",
    "print(f\"Final dataset: {len(common_subjects)} subjects\")\n",
    "print(f\"FreeSurfer features shape: {fs_features.shape}\")\n",
    "print(f\"Sex: {np.sum(sex_labels)} Female, {len(sex_labels) - np.sum(sex_labels)} Male\")\n",
    "print(f\"Age: mean={age_labels.mean():.1f}, range={age_labels.min():.1f}-{age_labels.max():.1f}\\n\")\n",
    "\n",
    "print(\"Loading region names...\")\n",
    "sample_subject = common_subjects[0]\n",
    "sample_file = os.path.join(FREESURFER_DIR, f'sub-{sample_subject}', \n",
    "                           f'sub-{sample_subject}_regionsurfacestats.tsv')\n",
    "region_df = pd.read_csv(sample_file, sep='\\t')\n",
    "region_df = region_df[region_df['atlas'] == PARCELLATION].sort_values('StructName')\n",
    "region_names = region_df['StructName'].values[:400]\n",
    "hemisphere = region_df['hemisphere'].values[:400]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPLITTING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_indices = np.arange(len(common_subjects))\n",
    "\n",
    "# 90/10 split for sex classification and age prediction\n",
    "cv_indices, test_indices = train_test_split(\n",
    "    all_indices, \n",
    "    test_size=0.1,\n",
    "    stratify=sex_labels,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"CV set: {len(cv_indices)} subjects\")\n",
    "print(f\"Test set: {len(test_indices)} subjects\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-VALIDATION WITH ALPHA SUBSAMPLING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "alphas = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "n_folds = 5\n",
    "\n",
    "cv_results = {\n",
    "    'sex': {'acc': {a: [] for a in alphas}, 'auc': {a: [] for a in alphas}},\n",
    "    'age': {'mae': {a: [] for a in alphas}, 'r2': {a: [] for a in alphas}}\n",
    "}\n",
    "\n",
    "# SEX CLASSIFICATION CV\n",
    "print(\"\\nSex classification cross-validation...\")\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold_idx, (train_rel, val_rel) in enumerate(skf.split(cv_indices, sex_labels[cv_indices])):\n",
    "    train_idx = cv_indices[train_rel]\n",
    "    val_idx = cv_indices[val_rel]\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        n_train = max(4, int(alpha * len(train_idx)))\n",
    "        try:\n",
    "            train_sub, _ = train_test_split(\n",
    "                train_idx,\n",
    "                train_size=n_train,\n",
    "                stratify=sex_labels[train_idx],\n",
    "                random_state=SEED\n",
    "            )\n",
    "        except:\n",
    "            train_sub = np.random.choice(train_idx, n_train, replace=False)\n",
    "        \n",
    "        if len(np.unique(sex_labels[train_sub])) < 2:\n",
    "            continue\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(fs_features[train_sub])\n",
    "        X_val = scaler.transform(fs_features[val_idx])\n",
    "        \n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=500,\n",
    "            max_depth=8,\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=10,\n",
    "            random_state=SEED,\n",
    "            max_features='sqrt',\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train, sex_labels[train_sub])\n",
    "        \n",
    "        val_pred = rf.predict(X_val)\n",
    "        val_proba = rf.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        cv_results['sex']['acc'][alpha].append(\n",
    "            balanced_accuracy_score(sex_labels[val_idx], val_pred)\n",
    "        )\n",
    "        cv_results['sex']['auc'][alpha].append(\n",
    "            roc_auc_score(sex_labels[val_idx], val_proba)\n",
    "        )\n",
    "    \n",
    "    print(f\"  Fold {fold_idx+1}/{n_folds} complete\")\n",
    "\n",
    "# AGE PREDICTION CV\n",
    "print(\"\\nAge prediction cross-validation...\")\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold_idx, (train_rel, val_rel) in enumerate(kf.split(cv_indices)):\n",
    "    train_idx = cv_indices[train_rel]\n",
    "    val_idx = cv_indices[val_rel]\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        n_train = max(40, int(alpha * len(train_idx)))\n",
    "        if n_train < len(train_idx):\n",
    "            train_sub, _ = train_test_split(train_idx, train_size=n_train, random_state=SEED)\n",
    "        else:\n",
    "            train_sub = train_idx\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(fs_features[train_sub])\n",
    "        X_val = scaler.transform(fs_features[val_idx])\n",
    "        \n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=8,\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=10,\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train, age_labels[train_sub])\n",
    "        \n",
    "        val_pred = rf.predict(X_val)\n",
    "        \n",
    "        cv_results['age']['mae'][alpha].append(\n",
    "            mean_absolute_error(age_labels[val_idx], val_pred)\n",
    "        )\n",
    "        cv_results['age']['r2'][alpha].append(\n",
    "            r2_score(age_labels[val_idx], val_pred)\n",
    "        )\n",
    "    \n",
    "    print(f\"  Fold {fold_idx+1}/{n_folds} complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SELECTING BEST ALPHA FROM CV\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "avg_auc_sex = {a: np.mean(cv_results['sex']['auc'][a]) if cv_results['sex']['auc'][a] else 0 for a in alphas}\n",
    "avg_r2_age = {a: np.mean(cv_results['age']['r2'][a]) if cv_results['age']['r2'][a] else 0 for a in alphas}\n",
    "\n",
    "best_alpha_sex = max(avg_auc_sex, key=avg_auc_sex.get)\n",
    "best_alpha_age = max(avg_r2_age, key=avg_r2_age.get)\n",
    "\n",
    "print(f\"Best alpha - Sex: {best_alpha_sex} (CV AUC: {avg_auc_sex[best_alpha_sex]:.4f})\")\n",
    "print(f\"Best alpha - Age: {best_alpha_age} (CV RÂ²: {avg_r2_age[best_alpha_age]:.4f})\")\n",
    "\n",
    "print(\"FINAL TEST SET EVALUATION & PERMUTATION IMPORTANCE EXTRACTION\")\n",
    "\n",
    "# SEX CLASSIFICATION\n",
    "print(\"\\nSex classification final model...\")\n",
    "n_train_sex = int(best_alpha_sex * len(cv_indices))\n",
    "try:\n",
    "    final_train_sex, _ = train_test_split(\n",
    "        cv_indices,\n",
    "        train_size=n_train_sex,\n",
    "        stratify=sex_labels[cv_indices],\n",
    "        random_state=SEED\n",
    "    )\n",
    "except:\n",
    "    final_train_sex = cv_indices\n",
    "\n",
    "scaler_sex = StandardScaler()\n",
    "X_train_sex = scaler_sex.fit_transform(fs_features[final_train_sex])\n",
    "X_test_sex = scaler_sex.transform(fs_features[test_indices])\n",
    "\n",
    "rf_sex = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=8,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=SEED,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_sex.fit(X_train_sex, sex_labels[final_train_sex])\n",
    "\n",
    "test_pred_sex = rf_sex.predict(X_test_sex)\n",
    "test_proba_sex = rf_sex.predict_proba(X_test_sex)[:, 1]\n",
    "test_acc_sex = balanced_accuracy_score(sex_labels[test_indices], test_pred_sex)\n",
    "test_auc_sex = roc_auc_score(sex_labels[test_indices], test_proba_sex)\n",
    "\n",
    "# Permutation importance on TEST SET\n",
    "print(\"  Computing permutation importance on test set...\")\n",
    "perm_sex = permutation_importance(\n",
    "    rf_sex, X_test_sex, sex_labels[test_indices],\n",
    "    n_repeats=10,\n",
    "    random_state=SEED,\n",
    "    scoring='balanced_accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "sex_importance = perm_sex.importances_mean\n",
    "\n",
    "print(f\"  Test Balanced Accuracy: {test_acc_sex:.4f}\")\n",
    "print(f\"  Test AUC: {test_auc_sex:.4f}\")\n",
    "\n",
    "# AGE PREDICTION\n",
    "print(\"\\nAge prediction final model...\")\n",
    "n_train_age = int(best_alpha_age * len(cv_indices))\n",
    "if n_train_age < len(cv_indices):\n",
    "    final_train_age, _ = train_test_split(cv_indices, train_size=n_train_age, random_state=SEED)\n",
    "else:\n",
    "    final_train_age = cv_indices\n",
    "\n",
    "scaler_age = StandardScaler()\n",
    "X_train_age = scaler_age.fit_transform(fs_features[final_train_age])\n",
    "X_test_age = scaler_age.transform(fs_features[test_indices])\n",
    "\n",
    "rf_age = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_age.fit(X_train_age, age_labels[final_train_age])\n",
    "\n",
    "test_pred_age = rf_age.predict(X_test_age)\n",
    "test_r2_age = r2_score(age_labels[test_indices], test_pred_age)\n",
    "test_mae_age = mean_absolute_error(age_labels[test_indices], test_pred_age)\n",
    "\n",
    "# Permutation importance on TEST SET\n",
    "print(\"  Computing permutation importance on test set...\")\n",
    "perm_age = permutation_importance(\n",
    "    rf_age, X_test_age, age_labels[test_indices],\n",
    "    n_repeats=10,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "age_importance = perm_age.importances_mean\n",
    "\n",
    "print(f\"  Test RÂ²: {test_r2_age:.4f}\")\n",
    "print(f\"  Test MAE: {test_mae_age:.2f} years\")\n",
    "\n",
    "age_importance_sa = age_importance[:400]\n",
    "age_importance_thick = age_importance[400:]\n",
    "sex_importance_sa = sex_importance[:400]\n",
    "sex_importance_thick = sex_importance[400:]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERMUTATION IMPORTANCE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nAge prediction:\")\n",
    "print(f\"  Surface area: {age_importance_sa.sum():.4f} ({age_importance_sa.sum()*100:.1f}%)\")\n",
    "print(f\"  Thickness: {age_importance_thick.sum():.4f} ({age_importance_thick.sum()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nSex classification:\")\n",
    "print(f\"  Surface area: {sex_importance_sa.sum():.4f} ({sex_importance_sa.sum()*100:.1f}%)\")\n",
    "print(f\"  Thickness: {sex_importance_thick.sum():.4f} ({sex_importance_thick.sum()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 10 MOST IMPORTANT REGIONS (PERMUTATION IMPORTANCE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nAge prediction - Surface area:\")\n",
    "top_age_sa = np.argsort(age_importance_sa)[-10:][::-1]\n",
    "for rank, idx in enumerate(top_age_sa, 1):\n",
    "    print(f\"  {rank:2d}. {region_names[idx]:50s} = {age_importance_sa[idx]:.6f}\")\n",
    "\n",
    "print(\"\\nAge prediction - Thickness:\")\n",
    "top_age_thick = np.argsort(age_importance_thick)[-10:][::-1]\n",
    "for rank, idx in enumerate(top_age_thick, 1):\n",
    "    print(f\"  {rank:2d}. {region_names[idx]:50s} = {age_importance_thick[idx]:.6f}\")\n",
    "\n",
    "print(\"\\nSex classification - Surface area:\")\n",
    "top_sex_sa = np.argsort(sex_importance_sa)[-10:][::-1]\n",
    "for rank, idx in enumerate(top_sex_sa, 1):\n",
    "    print(f\"  {rank:2d}. {region_names[idx]:50s} = {sex_importance_sa[idx]:.6f}\")\n",
    "\n",
    "print(\"\\nSex classification - Thickness:\")\n",
    "top_sex_thick = np.argsort(sex_importance_thick)[-10:][::-1]\n",
    "for rank, idx in enumerate(top_sex_thick, 1):\n",
    "    print(f\"  {rank:2d}. {region_names[idx]:50s} = {sex_importance_thick[idx]:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Saving results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fs_importance_data = {\n",
    "    'age_importance_surface_area': age_importance_sa,\n",
    "    'age_importance_thickness': age_importance_thick,\n",
    "    'sex_importance_surface_area': sex_importance_sa,\n",
    "    'sex_importance_thickness': sex_importance_thick,\n",
    "    'region_names': region_names,\n",
    "    'hemisphere': hemisphere,\n",
    "    'n_subjects': len(common_subjects),\n",
    "    'test_results': {\n",
    "        'sex': {'accuracy': float(test_acc_sex), 'auc': float(test_auc_sex), 'alpha': best_alpha_sex},\n",
    "        'age': {'r2': float(test_r2_age), 'mae': float(test_mae_age), 'alpha': best_alpha_age}\n",
    "    },\n",
    "    'cv_results': cv_results\n",
    "}\n",
    "\n",
    "output_file = os.path.join(OUTPUT_DIR, 'freesurfer_feature_importance.pkl')\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(fs_importance_data, f)\n",
    "print(f\"Saved: {output_file}\")\n",
    "\n",
    "# Save as CSV\n",
    "all_features = list(range(400))\n",
    "all_importance_age = np.concatenate([age_importance_sa, age_importance_thick])\n",
    "all_importance_sex = np.concatenate([sex_importance_sa, sex_importance_thick])\n",
    "all_types = ['Surface Area'] * 400 + ['Thickness'] * 400\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'ROI_Index': all_features + all_features,\n",
    "    'Region_Name': list(region_names) + list(region_names),\n",
    "    'Hemisphere': list(hemisphere) + list(hemisphere),\n",
    "    'Feature_Type': all_types,\n",
    "    'Age_Permutation_Importance': all_importance_age,\n",
    "    'Sex_Permutation_Importance': all_importance_sex,\n",
    "})\n",
    "\n",
    "for task, sort_col in [('age', 'Age_Permutation_Importance'), ('sex', 'Sex_Permutation_Importance')]:\n",
    "    df_sorted = importance_df.sort_values(sort_col, ascending=False)\n",
    "    csv_file = os.path.join(OUTPUT_DIR, f'freesurfer_permutation_importance_{task}_sorted.csv')\n",
    "    df_sorted.to_csv(csv_file, index=False)\n",
    "    print(f\"Saved: {csv_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE!\")\n",
    "print(f\"Analyzed {len(common_subjects)} subjects\")\n",
    "print(f\"90/10 split: {len(cv_indices)} CV / {len(test_indices)} Test\")\n",
    "print(f\"Permutation importance extracted on test set\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d0ae9-1cb4-455c-b7d2-19131c97ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/arelbaha/links/projects/rrg-glatard/arelbaha/ppmi_fs_save_freesurfer_data.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "CAT12_BASE_DIR = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data/inputs\"\n",
    "DATA_DIR = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data\"\n",
    "LABELS_PATH = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data/processed_cohort_with_mri.csv\"\n",
    "BRAINIAC_MAPPING_CSV = os.path.join(DATA_DIR, \"processed_files_mapping.csv\")\n",
    "OUTPUT_DIR = \"/home/arelbaha/links/projects/def-glatard/arelbaha/ppmi_freesurfer_importance_outputs\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "#Loading Demographics\n",
    "print(\"Loading demographics...\")\n",
    "labels_df = pd.read_csv(LABELS_PATH)\n",
    "\n",
    "id_sex_dict = {}\n",
    "id_parkinson_dict = {}\n",
    "id_age_dict = {}\n",
    "\n",
    "for _, row in labels_df.iterrows():\n",
    "    patno = str(int(row['PATNO']))\n",
    "    sex = row['Sex'].strip().upper()\n",
    "    if sex == 'F':\n",
    "        id_sex_dict[patno] = 1\n",
    "    elif sex == 'M':\n",
    "        id_sex_dict[patno] = 0\n",
    "    \n",
    "    group = row['Group'].strip()\n",
    "    if group == 'PD':\n",
    "        id_parkinson_dict[patno] = 1\n",
    "    elif group == 'HC':\n",
    "        id_parkinson_dict[patno] = 0\n",
    "    \n",
    "    id_age_dict[patno] = row['Age']\n",
    "\n",
    "print(f\"Loaded demographics for {len(id_sex_dict)} subjects\")\n",
    "\n",
    "def extract_patno_from_path(filepath):\n",
    "    for part in filepath.split(os.sep):\n",
    "        if part.startswith('sub-'):\n",
    "            return part[4:]\n",
    "    return None\n",
    "\n",
    "print(\"\\nLoading CAT12 data\")\n",
    "cat12_files = glob.glob(os.path.join(CAT12_BASE_DIR, \"**\", \"*s6mwp1*.nii*\"), recursive=True)\n",
    "cat12_data = {}\n",
    "for f in cat12_files:\n",
    "    if not os.path.isfile(f):\n",
    "        continue\n",
    "    patno = extract_patno_from_path(f)\n",
    "    if patno and patno in id_sex_dict and patno in id_parkinson_dict and patno in id_age_dict:\n",
    "        cat12_data[patno] = f\n",
    "\n",
    "print(f\"Found {len(cat12_data)} CAT12 subjects\")\n",
    "\n",
    "print(\"Loading BrainIAC data\")\n",
    "brainiac_df = pd.read_csv(BRAINIAC_MAPPING_CSV).dropna(subset=[\"processed_file\", \"Age\", \"subject_id\"])\n",
    "brainiac_data = {}\n",
    "for _, row in brainiac_df.iterrows():\n",
    "    patno = str(row['subject_id'])\n",
    "    if patno in id_sex_dict and patno in id_parkinson_dict and patno in id_age_dict:\n",
    "        if os.path.exists(row['processed_file']):\n",
    "            brainiac_data[patno] = row['processed_file']\n",
    "\n",
    "print(f\"Found {len(brainiac_data)} BrainIAC subjects\")\n",
    "\n",
    "print(\"Loading FreeSurfer data\")\n",
    "fs_cth_df = pd.read_csv(os.path.join(CAT12_BASE_DIR, \"FS7_APARC_CTH_23Oct2025.csv\"))\n",
    "fs_sa_df = pd.read_csv(os.path.join(CAT12_BASE_DIR, \"FS7_APARC_SA_23Oct2025.csv\"))\n",
    "\n",
    "# Filter to baseline only\n",
    "fs_cth_df = fs_cth_df[fs_cth_df['EVENT_ID'] == 'BL'].copy()\n",
    "fs_sa_df = fs_sa_df[fs_sa_df['EVENT_ID'] == 'BL'].copy()\n",
    "\n",
    "fs_cth_df['PATNO'] = fs_cth_df['PATNO'].astype(str)\n",
    "fs_sa_df['PATNO'] = fs_sa_df['PATNO'].astype(str)\n",
    "\n",
    "cth_features = [c for c in fs_cth_df.columns if c not in ['PATNO', 'EVENT_ID']]\n",
    "sa_features = [c for c in fs_sa_df.columns if c not in ['PATNO', 'EVENT_ID']]\n",
    "\n",
    "print(f\"CTH features: {len(cth_features)}\")\n",
    "print(f\"SA features: {len(sa_features)}\")\n",
    "\n",
    "print(\"Finding common subjects across all modalities\")\n",
    "\n",
    "# First get subjects with CAT12 and BrainIAC\n",
    "common_subjects = sorted(list(set(cat12_data.keys()) & set(brainiac_data.keys())))\n",
    "print(f\"Subjects with CAT12 + BrainIAC: {len(common_subjects)}\")\n",
    "\n",
    "# Now filter for FreeSurfer\n",
    "fs_data = {}\n",
    "for patno in common_subjects:\n",
    "    cth_row = fs_cth_df[fs_cth_df['PATNO'] == patno]\n",
    "    sa_row = fs_sa_df[fs_sa_df['PATNO'] == patno]\n",
    "    \n",
    "    if len(cth_row) > 0 and len(sa_row) > 0:\n",
    "        combined = np.concatenate([\n",
    "            cth_row[cth_features].values.flatten(), \n",
    "            sa_row[sa_features].values.flatten()\n",
    "        ])\n",
    "        \n",
    "        if not np.any(np.isnan(combined)):\n",
    "            fs_data[patno] = combined\n",
    "\n",
    "# Final common subjects with ALL modalities\n",
    "common_subjects = sorted(list(set(cat12_data.keys()) & set(brainiac_data.keys()) & set(fs_data.keys())))\n",
    "\n",
    "print(f\"Subjects with CAT12 + BrainIAC + FreeSurfer: {len(common_subjects)}\")\n",
    "print(f\"\\nThese {len(common_subjects)} subjects have ALL modalities and will be used for analysis\")\n",
    "\n",
    "fs_features = np.array([fs_data[p] for p in common_subjects])\n",
    "sex_labels = np.array([id_sex_dict[p] for p in common_subjects])\n",
    "parkinson_labels = np.array([id_parkinson_dict[p] for p in common_subjects])\n",
    "age_labels = np.array([id_age_dict[p] for p in common_subjects])\n",
    "\n",
    "print(f\"\\nFinal dataset: {len(common_subjects)} subjects\")\n",
    "print(f\"FreeSurfer features shape: {fs_features.shape}\")\n",
    "print(f\"Sex: {np.sum(sex_labels)} Female, {len(sex_labels) - np.sum(sex_labels)} Male\")\n",
    "print(f\"PD: {np.sum(parkinson_labels)} PD, {len(parkinson_labels) - np.sum(parkinson_labels)} HC\")\n",
    "print(f\"Age: mean={age_labels.mean():.1f}, range={age_labels.min():.1f}-{age_labels.max():.1f}\\n\")\n",
    "\n",
    "print(\"Parsing region names and hemispheres\")\n",
    "\n",
    "cth_region_names = []\n",
    "cth_hemispheres = []\n",
    "for feat in cth_features:\n",
    "    if feat.startswith('lh_'):\n",
    "        hemi = 'L'\n",
    "        region = feat[3:].replace('_thickness', '')\n",
    "    elif feat.startswith('rh_'):\n",
    "        hemi = 'R'\n",
    "        region = feat[3:].replace('_thickness', '')\n",
    "    else:\n",
    "        hemi = 'L'\n",
    "        region = feat\n",
    "    cth_region_names.append(region)\n",
    "    cth_hemispheres.append(hemi)\n",
    "\n",
    "sa_region_names = []\n",
    "sa_hemispheres = []\n",
    "for feat in sa_features:\n",
    "    if feat.startswith('lh_'):\n",
    "        hemi = 'L'\n",
    "        region = feat[3:].replace('_area', '')\n",
    "    elif feat.startswith('rh_'):\n",
    "        hemi = 'R'\n",
    "        region = feat[3:].replace('_area', '')\n",
    "    else:\n",
    "        hemi = 'L'\n",
    "        region = feat\n",
    "    sa_region_names.append(region)\n",
    "    sa_hemispheres.append(hemi)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPLITTING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_indices = np.arange(len(common_subjects))\n",
    "\n",
    "# 90/10 split for sex classification and age prediction\n",
    "cv_indices, test_indices = train_test_split(\n",
    "    all_indices, \n",
    "    test_size=0.1,  # 10% for test\n",
    "    stratify=sex_labels,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# 80/20 split for Parkinson's (needs more test samples)\n",
    "cv_indices_pd, test_indices_pd = train_test_split(\n",
    "    all_indices,\n",
    "    test_size=0.2,  # 20% for test\n",
    "    stratify=parkinson_labels,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Sex/Age CV set: {len(cv_indices)} subjects, Test set: {len(test_indices)} subjects\")\n",
    "print(f\"PD CV set: {len(cv_indices_pd)} subjects, Test set: {len(test_indices_pd)} subjects\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-VALIDATION WITH ALPHA SUBSAMPLING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "alphas = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "n_folds = 5\n",
    "\n",
    "cv_results = {\n",
    "    'sex': {'acc': {a: [] for a in alphas}, 'auc': {a: [] for a in alphas}},\n",
    "    'pd': {'acc': {a: [] for a in alphas}, 'auc': {a: [] for a in alphas}},\n",
    "    'age': {'mae': {a: [] for a in alphas}, 'r2': {a: [] for a in alphas}}\n",
    "}\n",
    "\n",
    "# SEX CLASSIFICATION CV\n",
    "print(\"\\nSex classification cross-validation...\")\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold_idx, (train_rel, val_rel) in enumerate(skf.split(cv_indices, sex_labels[cv_indices])):\n",
    "    train_idx = cv_indices[train_rel]\n",
    "    val_idx = cv_indices[val_rel]\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        n_train = max(4, int(alpha * len(train_idx)))\n",
    "        try:\n",
    "            train_sub, _ = train_test_split(\n",
    "                train_idx,\n",
    "                train_size=n_train,\n",
    "                stratify=sex_labels[train_idx],\n",
    "                random_state=SEED\n",
    "            )\n",
    "        except:\n",
    "            train_sub = np.random.choice(train_idx, n_train, replace=False)\n",
    "        \n",
    "        if len(np.unique(sex_labels[train_sub])) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Train model\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(fs_features[train_sub])\n",
    "        X_val = scaler.transform(fs_features[val_idx])\n",
    "        \n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=500, \n",
    "            random_state=SEED,\n",
    "            max_features='sqrt',\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train, sex_labels[train_sub])\n",
    "        \n",
    "        # Evaluate\n",
    "        val_pred = rf.predict(X_val)\n",
    "        val_proba = rf.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        cv_results['sex']['acc'][alpha].append(\n",
    "            balanced_accuracy_score(sex_labels[val_idx], val_pred)\n",
    "        )\n",
    "        cv_results['sex']['auc'][alpha].append(\n",
    "            roc_auc_score(sex_labels[val_idx], val_proba)\n",
    "        )\n",
    "    \n",
    "    print(f\"  Fold {fold_idx+1}/{n_folds} complete\")\n",
    "\n",
    "# PARKINSON'S CLASSIFICATION CV\n",
    "print(\"\\nParkinson's classification cross-validation...\")\n",
    "skf_pd = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold_idx, (train_rel, val_rel) in enumerate(skf_pd.split(cv_indices_pd, parkinson_labels[cv_indices_pd])):\n",
    "    train_idx = cv_indices_pd[train_rel]\n",
    "    val_idx = cv_indices_pd[val_rel]\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        n_train = max(4, int(alpha * len(train_idx)))\n",
    "        try:\n",
    "            train_sub, _ = train_test_split(\n",
    "                train_idx,\n",
    "                train_size=n_train,\n",
    "                stratify=parkinson_labels[train_idx],\n",
    "                random_state=SEED\n",
    "            )\n",
    "        except:\n",
    "            train_sub = np.random.choice(train_idx, n_train, replace=False)\n",
    "        \n",
    "        if len(np.unique(parkinson_labels[train_sub])) < 2:\n",
    "            continue\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(fs_features[train_sub])\n",
    "        X_val = scaler.transform(fs_features[val_idx])\n",
    "        \n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=500,\n",
    "            random_state=SEED,\n",
    "            max_features='sqrt',\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train, parkinson_labels[train_sub])\n",
    "        \n",
    "        val_pred = rf.predict(X_val)\n",
    "        val_proba = rf.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        cv_results['pd']['acc'][alpha].append(\n",
    "            balanced_accuracy_score(parkinson_labels[val_idx], val_pred)\n",
    "        )\n",
    "        cv_results['pd']['auc'][alpha].append(\n",
    "            roc_auc_score(parkinson_labels[val_idx], val_proba)\n",
    "        )\n",
    "    \n",
    "    print(f\"  Fold {fold_idx+1}/{n_folds} complete\")\n",
    "\n",
    "# AGE PREDICTION CV\n",
    "print(\"\\nAge prediction cross-validation...\")\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold_idx, (train_rel, val_rel) in enumerate(kf.split(cv_indices)):\n",
    "    train_idx = cv_indices[train_rel]\n",
    "    val_idx = cv_indices[val_rel]\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        n_train = max(40, int(alpha * len(train_idx)))\n",
    "        if n_train < len(train_idx):\n",
    "            train_sub, _ = train_test_split(train_idx, train_size=n_train, random_state=SEED)\n",
    "        else:\n",
    "            train_sub = train_idx\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(fs_features[train_sub])\n",
    "        X_val = scaler.transform(fs_features[val_idx])\n",
    "        \n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train, age_labels[train_sub])\n",
    "        \n",
    "        val_pred = rf.predict(X_val)\n",
    "        \n",
    "        cv_results['age']['mae'][alpha].append(\n",
    "            mean_absolute_error(age_labels[val_idx], val_pred)\n",
    "        )\n",
    "        cv_results['age']['r2'][alpha].append(\n",
    "            r2_score(age_labels[val_idx], val_pred)\n",
    "        )\n",
    "    \n",
    "    print(f\"  Fold {fold_idx+1}/{n_folds} complete\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SELECTING BEST ALPHA FROM CV\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "avg_auc_sex = {a: np.mean(cv_results['sex']['auc'][a]) if cv_results['sex']['auc'][a] else 0 for a in alphas}\n",
    "avg_auc_pd = {a: np.mean(cv_results['pd']['auc'][a]) if cv_results['pd']['auc'][a] else 0 for a in alphas}\n",
    "avg_r2_age = {a: np.mean(cv_results['age']['r2'][a]) if cv_results['age']['r2'][a] else 0 for a in alphas}\n",
    "\n",
    "best_alpha_sex = max(avg_auc_sex, key=avg_auc_sex.get)\n",
    "best_alpha_pd = max(avg_auc_pd, key=avg_auc_pd.get)\n",
    "best_alpha_age = max(avg_r2_age, key=avg_r2_age.get)\n",
    "\n",
    "print(f\"Best alpha - Sex: {best_alpha_sex} (CV AUC: {avg_auc_sex[best_alpha_sex]:.4f})\")\n",
    "print(f\"Best alpha - PD: {best_alpha_pd} (CV AUC: {avg_auc_pd[best_alpha_pd]:.4f})\")\n",
    "print(f\"Best alpha - Age: {best_alpha_age} (CV RÂ²: {avg_r2_age[best_alpha_age]:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL TEST SET EVALUATION & FEATURE IMPORTANCE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# SEX CLASSIFICATION\n",
    "print(\"\\nSex classification final model...\")\n",
    "n_train_sex = int(best_alpha_sex * len(cv_indices))\n",
    "try:\n",
    "    final_train_sex, _ = train_test_split(\n",
    "        cv_indices,\n",
    "        train_size=n_train_sex,\n",
    "        stratify=sex_labels[cv_indices],\n",
    "        random_state=SEED\n",
    "    )\n",
    "except:\n",
    "    final_train_sex = cv_indices\n",
    "\n",
    "scaler_sex = StandardScaler()\n",
    "X_train_sex = scaler_sex.fit_transform(fs_features[final_train_sex])\n",
    "X_test_sex = scaler_sex.transform(fs_features[test_indices])\n",
    "\n",
    "rf_sex = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    random_state=SEED,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_sex.fit(X_train_sex, sex_labels[final_train_sex])\n",
    "\n",
    "test_pred_sex = rf_sex.predict(X_test_sex)\n",
    "test_proba_sex = rf_sex.predict_proba(X_test_sex)[:, 1]\n",
    "test_acc_sex = balanced_accuracy_score(sex_labels[test_indices], test_pred_sex)\n",
    "test_auc_sex = roc_auc_score(sex_labels[test_indices], test_proba_sex)\n",
    "\n",
    "sex_importance = rf_sex.feature_importances_\n",
    "\n",
    "print(f\"  Test Balanced Accuracy: {test_acc_sex:.4f}\")\n",
    "print(f\"  Test AUC: {test_auc_sex:.4f}\")\n",
    "\n",
    "# PARKINSON'S CLASSIFICATION\n",
    "print(\"\\nParkinson's classification final model...\")\n",
    "n_train_pd = int(best_alpha_pd * len(cv_indices_pd))\n",
    "try:\n",
    "    final_train_pd, _ = train_test_split(\n",
    "        cv_indices_pd,\n",
    "        train_size=n_train_pd,\n",
    "        stratify=parkinson_labels[cv_indices_pd],\n",
    "        random_state=SEED\n",
    "    )\n",
    "except:\n",
    "    final_train_pd = cv_indices_pd\n",
    "\n",
    "scaler_pd = StandardScaler()\n",
    "X_train_pd = scaler_pd.fit_transform(fs_features[final_train_pd])\n",
    "X_test_pd = scaler_pd.transform(fs_features[test_indices_pd])\n",
    "\n",
    "rf_pd = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    random_state=SEED,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_pd.fit(X_train_pd, parkinson_labels[final_train_pd])\n",
    "\n",
    "test_pred_pd = rf_pd.predict(X_test_pd)\n",
    "test_proba_pd = rf_pd.predict_proba(X_test_pd)[:, 1]\n",
    "test_acc_pd = balanced_accuracy_score(parkinson_labels[test_indices_pd], test_pred_pd)\n",
    "test_auc_pd = roc_auc_score(parkinson_labels[test_indices_pd], test_proba_pd)\n",
    "\n",
    "pd_importance = rf_pd.feature_importances_\n",
    "\n",
    "print(f\"  Test Balanced Accuracy: {test_acc_pd:.4f}\")\n",
    "print(f\"  Test AUC: {test_auc_pd:.4f}\")\n",
    "\n",
    "# AGE PREDICTION\n",
    "print(\"\\nAge prediction final model...\")\n",
    "n_train_age = int(best_alpha_age * len(cv_indices))\n",
    "if n_train_age < len(cv_indices):\n",
    "    final_train_age, _ = train_test_split(cv_indices, train_size=n_train_age, random_state=SEED)\n",
    "else:\n",
    "    final_train_age = cv_indices\n",
    "\n",
    "scaler_age = StandardScaler()\n",
    "X_train_age = scaler_age.fit_transform(fs_features[final_train_age])\n",
    "X_test_age = scaler_age.transform(fs_features[test_indices])\n",
    "\n",
    "rf_age = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_age.fit(X_train_age, age_labels[final_train_age])\n",
    "\n",
    "test_pred_age = rf_age.predict(X_test_age)\n",
    "test_r2_age = r2_score(age_labels[test_indices], test_pred_age)\n",
    "test_mae_age = mean_absolute_error(age_labels[test_indices], test_pred_age)\n",
    "\n",
    "age_importance = rf_age.feature_importances_\n",
    "\n",
    "print(f\"  Test RÂ²: {test_r2_age:.4f}\")\n",
    "print(f\"  Test MAE: {test_mae_age:.2f} years\")\n",
    "\n",
    "n_cth = len(cth_features)\n",
    "n_sa = len(sa_features)\n",
    "\n",
    "age_importance_cth = age_importance[:n_cth]\n",
    "age_importance_sa = age_importance[n_cth:]\n",
    "sex_importance_cth = sex_importance[:n_cth]\n",
    "sex_importance_sa = sex_importance[n_cth:]\n",
    "pd_importance_cth = pd_importance[:n_cth]\n",
    "pd_importance_sa = pd_importance[n_cth:]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nAge prediction:\")\n",
    "print(f\"  CTH: {age_importance_cth.sum():.4f} ({age_importance_cth.sum()*100:.1f}%)\")\n",
    "print(f\"  SA: {age_importance_sa.sum():.4f} ({age_importance_sa.sum()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nSex classification:\")\n",
    "print(f\"  CTH: {sex_importance_cth.sum():.4f} ({sex_importance_cth.sum()*100:.1f}%)\")\n",
    "print(f\"  SA: {sex_importance_sa.sum():.4f} ({sex_importance_sa.sum()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nParkinson's disease:\")\n",
    "print(f\"  CTH: {pd_importance_cth.sum():.4f} ({pd_importance_cth.sum()*100:.1f}%)\")\n",
    "print(f\"  SA: {pd_importance_sa.sum():.4f} ({pd_importance_sa.sum()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 10 REGIONS PER TASK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nAge - Thickness:\")\n",
    "top_age_cth = np.argsort(age_importance_cth)[-10:][::-1]\n",
    "for rank, idx in enumerate(top_age_cth, 1):\n",
    "    print(f\"  {rank:2d}. {cth_region_names[idx]:30s} ({cth_hemispheres[idx]}) = {age_importance_cth[idx]:.6f} ({age_importance_cth[idx]*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nAge - Surface Area:\")\n",
    "top_age_sa = np.argsort(age_importance_sa)[-10:][::-1]\n",
    "for rank, idx in enumerate(top_age_sa, 1):\n",
    "    print(f\"  {rank:2d}. {sa_region_names[idx]:30s} ({sa_hemispheres[idx]}) = {age_importance_sa[idx]:.6f} ({age_importance_sa[idx]*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nSex - Surface Area:\")\n",
    "top_sex_sa = np.argsort(sex_importance_sa)[-10:][::-1]\n",
    "for rank, idx in enumerate(top_sex_sa, 1):\n",
    "    print(f\"  {rank:2d}. {sa_region_names[idx]:30s} ({sa_hemispheres[idx]}) = {sex_importance_sa[idx]:.6f} ({sex_importance_sa[idx]*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nParkinson's - Thickness:\")\n",
    "top_pd_cth = np.argsort(pd_importance_cth)[-10:][::-1]\n",
    "for rank, idx in enumerate(top_pd_cth, 1):\n",
    "    print(f\"  {rank:2d}. {cth_region_names[idx]:30s} ({cth_hemispheres[idx]}) = {pd_importance_cth[idx]:.6f} ({pd_importance_cth[idx]*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Saving results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ppmi_importance_data = {\n",
    "    'age_importance_cth': age_importance_cth,\n",
    "    'age_importance_sa': age_importance_sa,\n",
    "    'sex_importance_cth': sex_importance_cth,\n",
    "    'sex_importance_sa': sex_importance_sa,\n",
    "    'pd_importance_cth': pd_importance_cth,\n",
    "    'pd_importance_sa': pd_importance_sa,\n",
    "    'cth_region_names': cth_region_names,\n",
    "    'cth_hemispheres': cth_hemispheres,\n",
    "    'sa_region_names': sa_region_names,\n",
    "    'sa_hemispheres': sa_hemispheres,\n",
    "    'n_subjects': len(common_subjects),\n",
    "    'n_pd': int(np.sum(parkinson_labels)),\n",
    "    'n_hc': int(len(parkinson_labels) - np.sum(parkinson_labels)),\n",
    "    'age_mean': float(age_labels.mean()),\n",
    "    'age_range': (float(age_labels.min()), float(age_labels.max())),\n",
    "    # Add test set performance\n",
    "    'test_results': {\n",
    "        'sex': {'accuracy': float(test_acc_sex), 'auc': float(test_auc_sex), 'alpha': best_alpha_sex},\n",
    "        'pd': {'accuracy': float(test_acc_pd), 'auc': float(test_auc_pd), 'alpha': best_alpha_pd},\n",
    "        'age': {'r2': float(test_r2_age), 'mae': float(test_mae_age), 'alpha': best_alpha_age}\n",
    "    },\n",
    "    # Add CV results for visualization\n",
    "    'cv_results': cv_results\n",
    "}\n",
    "\n",
    "output_file = os.path.join(OUTPUT_DIR, 'ppmi_freesurfer_feature_importance.pkl')\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(ppmi_importance_data, f)\n",
    "print(f\"Saved: {output_file}\")\n",
    "\n",
    "# Save CSVs\n",
    "all_features = cth_features + sa_features\n",
    "all_importance_age = np.concatenate([age_importance_cth, age_importance_sa])\n",
    "all_importance_sex = np.concatenate([sex_importance_cth, sex_importance_sa])\n",
    "all_importance_pd = np.concatenate([pd_importance_cth, pd_importance_sa])\n",
    "all_types = ['CTH'] * n_cth + ['SA'] * n_sa\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': all_features,\n",
    "    'Type': all_types,\n",
    "    'Age_Importance': all_importance_age,\n",
    "    'Age_Percent': all_importance_age * 100,\n",
    "    'Sex_Importance': all_importance_sex,\n",
    "    'Sex_Percent': all_importance_sex * 100,\n",
    "    'PD_Importance': all_importance_pd,\n",
    "    'PD_Percent': all_importance_pd * 100\n",
    "})\n",
    "\n",
    "for task, sort_col in [('age', 'Age_Importance'), ('sex', 'Sex_Importance'), ('pd', 'PD_Importance')]:\n",
    "    df_sorted = importance_df.sort_values(sort_col, ascending=False)\n",
    "    csv_file = os.path.join(OUTPUT_DIR, f'ppmi_feature_importance_{task}_sorted.csv')\n",
    "    df_sorted.to_csv(csv_file, index=False)\n",
    "    print(f\"Saved: {csv_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE!\")\n",
    "print(f\"Analyzed {len(common_subjects)} subjects with ALL modalities\")\n",
    "print(f\"Feature importance extracted from models trained on CV set (best alpha)\")\n",
    "print(f\"Test set performance evaluated on held-out {len(test_indices)} subjects\")\n",
    "print(\"Download .pkl file for visualization\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
