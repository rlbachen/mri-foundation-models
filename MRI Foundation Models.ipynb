{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b6b822-71f3-49ce-aeed-22c8b4febb7f",
   "metadata": {},
   "source": [
    "# Structural MRI Foundation Models: Systematic Benchmarking and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab81ad-45cd-46cb-ad84-c740ab64cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext slurm_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653b3cf-1728-4a6d-b2aa-f01ea0a5dadb",
   "metadata": {},
   "source": [
    "## CAT12 Preprocessing | AnatCL | HBN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c2d56-25ab-4b65-a537-a8f2288012f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/links/projects/rrg-glatard/arelbaha/HBN_BIDS/new_cat12_preprocessing.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import boutiques\n",
    "from boutiques import bosh\n",
    "from boutiques.descriptor2func import function\n",
    "\n",
    "boutiques_descriptor = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/descriptors/cat12_prepro.json\"\n",
    "base_dir = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/HBN_BIDS\"\n",
    "output_dir = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/preprocessed_output\"\n",
    "\n",
    "bosh([\"exec\", \n",
    "      \"prepare\", \n",
    "      boutiques_descriptor, \n",
    "      \"--imagepath\", \n",
    "      \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif\"\n",
    "])\n",
    "\n",
    "cat12 = boutiques.descriptor2func.function(boutiques_descriptor)\n",
    "\n",
    "t1_nii_files = glob.glob(os.path.join(base_dir, \"sub-*\", \"ses-*\", \"anat\", \"sub-*_T1w.nii\"))\n",
    "print(f\"Found {len(t1_nii_files)} T1w files.\")\n",
    "\n",
    "task_id = int(os.environ[\"SLURM_ARRAY_TASK_ID\"])\n",
    "\n",
    "if task_id >= len(t1_nii_files) or task_id < 0:\n",
    "    print(f\"SLURM_ARRAY_TASK_ID={task_id} out of range\")\n",
    "    exit(1)\n",
    "\n",
    "input_file = t1_nii_files[task_id]\n",
    "\n",
    "path_parts = input_file.split(os.sep)\n",
    "subject_id = next((part for part in path_parts if part.startswith(\"sub-\")), None)\n",
    "\n",
    "if subject_id is None:\n",
    "        print(f\"No subject ID in path: {input_file}\")\n",
    "        exit(1)\n",
    "\n",
    "filename = os.path.basename(input_file)\n",
    "\n",
    "subject_output_dir = os.path.join(output_dir, subject_id)\n",
    "os.makedirs(subject_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Processing={task_id} -> file: {filename} (subject: {subject_id})\")\n",
    "\n",
    "result = cat12('--imagepath=/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif', \n",
    "                   input_file=input_file, \n",
    "                   output_dir=subject_output_dir)\n",
    "\n",
    "result_dict = vars(result)\n",
    "print(\"Available result keys:\", result_dict.keys())\n",
    "print(\"\\nExit code:\", result_dict.get(\"exit_code\"))\n",
    "print(\"\\nStdout:\\n\", result_dict.get (\"stdout\" ))\n",
    "print(\"\\nStderr:\\n\", result_dict.get (\"stderr\"))\n",
    "print(\"\\nOutput files:\\n\", result_dict.get(\"output_files\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d9624-6f3c-41b0-9fdc-54d3878e50ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch --array=500-520\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=CAT12_preproc\n",
    "#SBATCH --account=rrg-glatard\n",
    "#SBATCH --mem=12G\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --output=CAT12_hbn_preproc_%A_%a.out\n",
    "#SBATCH --error=CAT12_hbn_preproc_%A_%a.err\n",
    "#SBATCH --time=1:0:0\n",
    "\n",
    "source ~/.venvs/jupyter_py3/bin/activate\n",
    "\n",
    "module load apptainer\n",
    "\n",
    "cd ~/links/projects/rrg-glatard/arelbaha/HBN_BIDS #Location of preprocessing data and script\n",
    "\n",
    "echo \"Running task ID: $SLURM_ARRAY_TASK_ID\"\n",
    "\n",
    "python new_cat12_preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56370de7-6f6c-47d8-bf17-ad17b7dbfabb",
   "metadata": {},
   "source": [
    "## CAT12 Preprocessing | AnatCL | PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dcbe07-4e28-43bc-b009-ae1d394712a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/links/projects/def-glatard/arelbaha/data/inputs/ppmicat12_preprocessing.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import boutiques\n",
    "from boutiques import bosh\n",
    "from boutiques.descriptor2func import function\n",
    "\n",
    "#Downloading Container Part + Paths\n",
    "boutiques_descriptor = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/descriptors/cat12_prepro.json\"\n",
    "base_dir = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data/inputs\"\n",
    "output_dir = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/preprocessed_output\"\n",
    "\n",
    "bosh([\"exec\", \"prepare\", boutiques_descriptor, \"--imagepath\", \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif\"])\n",
    "cat12 = boutiques.descriptor2func.function(boutiques_descriptor)\n",
    "\n",
    "data = glob.glob(os.path.join(base_dir, \"sub-*\", \"ses-*\", \"anat\", \"*.nii\"))\n",
    "print(f\"Found {len(data)}\")\n",
    "\n",
    "task_id = int(os.environ[\"SLURM_ARRAY_TASK_ID\"])\n",
    "\n",
    "if task_id >= len(data) or task_id < 0:\n",
    "    print(f\"SLURM_ARRAY_TASK_ID={task_id} out of range\")\n",
    "    exit(1)\n",
    "\n",
    "input_file = data[task_id]\n",
    "\n",
    "path_parts = input_file.split(os.sep)\n",
    "subject_id = next((part for part in path_parts if part.startswith(\"sub-\")), None)\n",
    "\n",
    "if subject_id is None:\n",
    "        print(f\"Could not find subject ID in path: {input_file}\")\n",
    "        exit(1)\n",
    "\n",
    "filename = os.path.basename(input_file)\n",
    "\n",
    "subject_output_dir = os.path.join(output_dir, subject_id)\n",
    "os.makedirs(subject_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Processing SLURM_ARRAY_TASK_ID ={task_id} -> file: {filename} (subject: {subject_id})\")\n",
    "\n",
    "result = cat12('--imagepath=/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif', \n",
    "                   input_file=input_file, \n",
    "                   output_dir=subject_output_dir)\n",
    "\n",
    "result_dict = vars(result)\n",
    "print(\"Available result keys:\", result_dict.keys())\n",
    "print(\"\\nExit code:\", result_dict.get(\"exit_code\"))\n",
    "print(\"\\nStdout:\\n\", result_dict.get (\"stdout\" ))\n",
    "print(\"\\nStderr:\\n\", result_dict.get (\"stderr\"))\n",
    "print(\"\\nOutput files:\\n\", result_dict.get(\"output_files\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472cc4a-c309-45a3-bc27-2ff39eed37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch --array=1040-1040\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=CAT12_parkinson_preproc\n",
    "#SBATCH --account=rrg-glatard\n",
    "#SBATCH --mem=12G\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --output=cat12_ppmi_preproc_%A_%a.out\n",
    "#SBATCH --error=cat12_ppmi_preproc_%A_%a.err\n",
    "#SBATCH --time=1:0:0\n",
    "\n",
    "source ~/.venvs/jupyter_py3/bin/activate\n",
    "\n",
    "module load apptainer\n",
    "\n",
    "cd ~/links/projects/def-glatard/arelbaha/data/inputs #Location of preprocessing data and script\n",
    "\n",
    "echo \"Running task ID: $SLURM_ARRAY_TASK_ID\"\n",
    "\n",
    "python classification_parkinson.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67ad396-5c11-4ff3-a9c3-951911764d61",
   "metadata": {},
   "source": [
    "## CAT12 Preprocessing | AnatCL | NKI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b757d-331d-43f8-bb53-0898400ad9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/arelbaha/links/scratch/NKI/cat12_script/nki_cat12_preprocessing.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import boutiques\n",
    "from boutiques import bosh\n",
    "from boutiques.descriptor2func import function\n",
    "\n",
    "boutiques_descriptor = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/descriptors/cat12_prepro.json\"\n",
    "base_dir = \"/home/arelbaha/links/scratch/NKI/NKI_BIDS\"\n",
    "output_dir = \"/home/arelbaha/links/scratch/cat12_NKI\"\n",
    "\n",
    "bosh([\"exec\", \n",
    "      \"prepare\", \n",
    "      boutiques_descriptor, \n",
    "      \"--imagepath\", \n",
    "      \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif\"\n",
    "])\n",
    "\n",
    "cat12 = boutiques.descriptor2func.function(boutiques_descriptor)\n",
    "\n",
    "with open(\"/home/arelbaha/links/scratch/NKI/final_subjects.txt\") as f:\n",
    "    subjects = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "t1_nii_files = []\n",
    "for subj in subjects:\n",
    "    files = glob.glob(os.path.join(base_dir, f\"sub-{subj}\", \"ses-BAS1\", \"anat\", \"*T1w.nii\"))\n",
    "    if files:\n",
    "        t1_nii_files.append(files[0])\n",
    "\n",
    "print(f\"Found {len(t1_nii_files)} T1w files from {len(subjects)} subjects.\")\n",
    "\n",
    "task_id = int(os.environ[\"SLURM_ARRAY_TASK_ID\"])\n",
    "if task_id >= len(t1_nii_files) or task_id < 0:\n",
    "    print(f\"SLURM_ARRAY_TASK_ID={task_id} out of range\")\n",
    "    exit(1)\n",
    "\n",
    "input_file = t1_nii_files[task_id]\n",
    "path_parts = input_file.split(os.sep)\n",
    "subject_id = next((part for part in path_parts if part.startswith(\"sub-\")), None)\n",
    "\n",
    "if subject_id is None:\n",
    "    print(f\"No subject ID in path: {input_file}\")\n",
    "    exit(1)\n",
    "\n",
    "filename = os.path.basename(input_file)\n",
    "subject_output_dir = os.path.join(output_dir, subject_id)\n",
    "os.makedirs(subject_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Processing={task_id} -> file: {filename} (subject: {subject_id})\")\n",
    "\n",
    "result = cat12('--imagepath=/home/arelbaha/links/projects/rrg-glatard/arelbaha/containers/cat12_prepro.sif', \n",
    "                   input_file=input_file, \n",
    "                   output_dir=subject_output_dir)\n",
    "\n",
    "result_dict = vars(result)\n",
    "print(\"Available result keys:\", result_dict.keys())\n",
    "print(\"\\nExit code:\", result_dict.get(\"exit_code\"))\n",
    "print(\"\\nStdout:\\n\", result_dict.get(\"stdout\"))\n",
    "print(\"\\nStderr:\\n\", result_dict.get(\"stderr\"))\n",
    "print(\"\\nOutput files:\\n\", result_dict.get(\"output_files\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479f1f5-5cde-488f-a137-a3ef7093ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch\n",
    "#!/bin/bash\n",
    "#SBATCH --array=21-957\n",
    "#SBATCH --job-name=cat12_nki\n",
    "#SBATCH --time=4:00:00\n",
    "#SBATCH --account=rrg-glatard\n",
    "#SBATCH --mem=32G\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --output=/home/arelbaha/links/scratch/NKI/cat12_script/logs/cat12_nki_%A_%a.out\n",
    "#SBATCH --error=/home/arelbaha/links/scratch/NKI/cat12_script/logs/cat12_nki_%A_%a.err\n",
    "\n",
    "source ~/.venvs/jupyter_py3/bin/activate\n",
    "module load apptainer\n",
    "cd /home/arelbaha/links/scratch/NKI/cat12_script\n",
    "\n",
    "echo \"Running task ID: $SLURM_ARRAY_TASK_ID\"\n",
    "python nki_cat12_preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d4893-626f-4d73-b08c-95ada3ca9022",
   "metadata": {},
   "source": [
    "## Turboprep Processing | BrainIAC, SwinBrain, 3D-Neuro-SimCLR | HBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2674e5-634e-43cc-afdf-8884cd0ea326",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch --array=75\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-glatard\n",
    "#SBATCH --time=4:00:00\n",
    "#SBATCH --mem=32G\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --job-name=turboprep\n",
    "#SBATCH --output=logs/turboprep_batch_%a.out\n",
    "#SBATCH --error=logs/turboprep_batch_%a.err\n",
    "\n",
    "\n",
    "INPUT_BASE=\"/home/arelbaha/links/projects/rrg-glatard/arelbaha/brainiac_p_files\"\n",
    "OUTPUT_BASE=\"/home/arelbaha/links/scratch/turboprep_output\"\n",
    "TEMPLATE=\"/home/arelbaha/links/scratch/t1_template/mni_icbm152_nlin_sym_09c/mni_icbm152_t1_tal_nlin_sym_09c.nii\"\n",
    "CONTAINER=\"/home/arelbaha/links/scratch/turboprep_container/turboprep.sif\"\n",
    "\n",
    "\n",
    "BATCH_NUM=$(printf \"%03d\" $SLURM_ARRAY_TASK_ID)\n",
    "BATCH_DIR=\"${INPUT_BASE}/batch_${BATCH_NUM}\"\n",
    "\n",
    "echo \"Processing batch ${BATCH_NUM}\"\n",
    "\n",
    "module load apptainer\n",
    "\n",
    "for T1_FILE in \"$BATCH_DIR\"/*.nii.gz; do\n",
    "    SUBJ_ID=$(basename \"$T1_FILE\" | sed 's/sub-\\([^_]*\\)_.*/\\1/')\n",
    "    \n",
    "    SUBJ_OUTPUT=\"${OUTPUT_BASE}/${SUBJ_ID}\"\n",
    "\n",
    "    mkdir -p \"$SUBJ_OUTPUT\"\n",
    "    \n",
    "    echo \"Processing $SUBJ_ID\"\n",
    "    \n",
    "    apptainer run \"$CONTAINER\" \"$T1_FILE\" \"$SUBJ_OUTPUT\" \"$TEMPLATE\" -m t1 -r r\n",
    "    \n",
    "done\n",
    "\n",
    "echo \"Completed batch ${BATCH_NUM}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5968d704-37a3-4914-bf91-62fdfcd39335",
   "metadata": {},
   "source": [
    "## Turboprep Processing | BrainIAC, SwinBrain, 3D-Neuro-SimCLR | NKI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff2b8f9-196e-4bc3-a409-4d5e1438cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch --array=21-958\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-glatard\n",
    "#SBATCH --time=2:00:00\n",
    "#SBATCH --mem=32G\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --job-name=turboprep_nki\n",
    "#SBATCH --output=logs/turboprep_nki_%a.out\n",
    "#SBATCH --error=logs/turboprep_nki_%a.err\n",
    "\n",
    "SUBJ_LIST=\"/home/arelbaha/links/scratch/NKI/final_subjects.txt\"\n",
    "BIDS_DIR=\"/home/arelbaha/links/scratch/NKI/NKI_BIDS\"\n",
    "OUTPUT_BASE=\"/home/arelbaha/links/scratch/turboprep_NKI\"\n",
    "TEMPLATE=\"/home/arelbaha/links/scratch/t1_template/mni_icbm152_nlin_sym_09c/mni_icbm152_t1_tal_nlin_sym_09c.nii\"\n",
    "CONTAINER=\"/home/arelbaha/links/scratch/turboprep_container/turboprep.sif\"\n",
    "\n",
    "SUBJ_ID=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" \"$SUBJ_LIST\")\n",
    "echo \"Processing sub-${SUBJ_ID}\"\n",
    "\n",
    "T1_FILE=$(find -L \"${BIDS_DIR}/sub-${SUBJ_ID}/ses-BAS1/anat\" -name \"*T1w.nii.gz\" 2>/dev/null | head -1)\n",
    "\n",
    "if [ -z \"$T1_FILE\" ]; then\n",
    "    echo \"ERROR: No downloaded T1w found for sub-${SUBJ_ID}, skipping\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"Using: $T1_FILE\"\n",
    "\n",
    "SUBJ_OUTPUT=\"${OUTPUT_BASE}/${SUBJ_ID}\"\n",
    "mkdir -p \"$SUBJ_OUTPUT\"\n",
    "\n",
    "module load apptainer\n",
    "apptainer run \"$CONTAINER\" \"$T1_FILE\" \"$SUBJ_OUTPUT\" \"$TEMPLATE\" -m t1 -r r\n",
    "\n",
    "echo \"Completed sub-${SUBJ_ID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6154db85-cdd5-49db-8a59-7e41cbc2bb70",
   "metadata": {},
   "source": [
    "## Turboprep Preprocessing | BrainIAC, SwinBrain, 3D-Neuro-SimCLR | PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef9999c-6cb7-48ee-92d4-50185663aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sbatch --array=10\n",
    "#!/bin/bash\n",
    "#SBATCH --account=def-glatard\n",
    "#SBATCH --time=8:00:00\n",
    "#SBATCH --mem=32G\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --job-name=turboprep_ppmi\n",
    "#SBATCH --output=logs/turboprep_ppmi_batch_%a.out\n",
    "#SBATCH --error=logs/turboprep_ppmi_batch_%a.err\n",
    "\n",
    "INPUT_BASE=\"/home/arelbaha/links/projects/def-glatard/arelbaha/data/raw_files_brainiac\"\n",
    "OUTPUT_BASE=\"/home/arelbaha/links/scratch/turboprep_output_ppmi\"\n",
    "TEMPLATE=\"/home/arelbaha/links/scratch/t1_template/mni_icbm152_nlin_sym_09c/mni_icbm152_t1_tal_nlin_sym_09c.nii\"\n",
    "CONTAINER=\"/home/arelbaha/links/scratch/turboprep_container/turboprep.sif\"\n",
    "\n",
    "BATCH_DIR=\"${INPUT_BASE}/batch_${SLURM_ARRAY_TASK_ID}\"\n",
    "\n",
    "echo \"Processing batch ${SLURM_ARRAY_TASK_ID}\"\n",
    "\n",
    "module load apptainer\n",
    "\n",
    "for T1_FILE in \"$BATCH_DIR\"/*.nii.gz; do\n",
    "    SUBJ_ID=$(basename \"$T1_FILE\" | sed 's/PPMI_\\([0-9]*\\)_.*/\\1/')\n",
    "    \n",
    "    SUBJ_OUTPUT=\"${OUTPUT_BASE}/${SUBJ_ID}\"\n",
    "    mkdir -p \"$SUBJ_OUTPUT\"\n",
    "    \n",
    "    echo \"Processing $SUBJ_ID\"\n",
    "    \n",
    "    apptainer run \"$CONTAINER\" \"$T1_FILE\" \"$SUBJ_OUTPUT\" \"$TEMPLATE\" -m t1 -r r\n",
    "    \n",
    "done\n",
    "echo \"Completed batch ${SLURM_ARRAY_TASK_ID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb41b4-95b0-4a46-9c0b-74079bdb4e0b",
   "metadata": {},
   "source": [
    "## Multi-Task Evaluation of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e688283-7df6-4c55-9586-6371f7928e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/arelbaha/links/projects/rrg-glatard/arelbaha/all_multimodel_comparison.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import nibabel as nib\n",
    "import sys\n",
    "import types\n",
    "import anatcl.models as _anatcl_models\n",
    "\n",
    "class _AgeEstimator:\n",
    "    pass\n",
    "\n",
    "_estimators_mod = types.ModuleType('models.estimators')\n",
    "_estimators_mod.AgeEstimator = _AgeEstimator\n",
    "sys.modules['models'] = _anatcl_models\n",
    "sys.modules['models.estimators'] = _estimators_mod\n",
    "\n",
    "from anatcl import AnatCL\n",
    "from monai.networks.nets import ViT, SwinUNETR, resnet18\n",
    "import torch.nn.functional as F\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import permutation_test\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "SEEDS = [0, 1, 2, 3, 42]\n",
    "\n",
    "CNN_SEED_CONFIGS = {\n",
    "    'CNN_0':  [0],\n",
    "    'CNN_42': [42],\n",
    "}\n",
    "\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.set_num_threads(1)\n",
    "torch.set_num_interop_threads(1)\n",
    "\n",
    "device = \"cpu\"\n",
    "DROPOUT_RATE = 0.3\n",
    "ANATCL_ENCODER_PATH = \"/home/arelbaha/.venvs/jupyter_py3/bin\"\n",
    "BRAINIAC_CKPT = \"/home/arelbaha/.venvs/jupyter_py3/bin/BrainIAC.ckpt\"\n",
    "SWINBRAIN_CKPT = \"/home/arelbaha/.venvs/jupyter_py3/bin/Brain_Swin_UNETR.pth\"\n",
    "SIMCLR_CKPT = \"/home/arelbaha/.venvs/jupyter_py3/bin/simclr_3d_brain_foundation.tar\"\n",
    "\n",
    "class CAT12VBMDataset(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data, self.transform = data, transform\n",
    "        self.target_shape = (121, 128, 121)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.data[idx]).get_fdata().astype(np.float32)\n",
    "        if img.shape != self.target_shape:\n",
    "            img = F.interpolate(torch.from_numpy(img[None, None]), size=self.target_shape,\n",
    "                               mode='trilinear', align_corners=False).squeeze().numpy()\n",
    "        img = self.transform(img).unsqueeze(0)\n",
    "        return img\n",
    "\n",
    "class BrainIACDataset(Dataset):\n",
    "    def __init__(self, paths, transform):\n",
    "        self.paths, self.transform = paths, transform\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.paths[idx]).get_fdata().astype(np.float32)\n",
    "        if img.shape != (96, 96, 96):\n",
    "            img = F.interpolate(torch.from_numpy(img[None, None]), size=(96, 96, 96),\n",
    "                              mode='trilinear', align_corners=False).squeeze().numpy()\n",
    "        img_tensor = torch.from_numpy(img)\n",
    "        img_normalized = self.transform(img_tensor)\n",
    "        return img_normalized[None]\n",
    "\n",
    "class SwinBrainDataset(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.paths[idx]).get_fdata().astype(np.float32)\n",
    "        img = F.interpolate(torch.from_numpy(img[None, None]), size=(128, 128, 64),\n",
    "                           mode='trilinear', align_corners=False).squeeze(0)\n",
    "        img = (img - img.mean()) / (img.std() + 1e-6)\n",
    "        img = img.repeat(3, 1, 1, 1)\n",
    "        return img\n",
    "\n",
    "class CNNDataset(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.paths[idx]).get_fdata().astype(np.float32)\n",
    "        if img.shape != (91, 109, 91):\n",
    "            img = F.interpolate(torch.from_numpy(img[None, None]), size=(91, 109, 91),\n",
    "                               mode='trilinear', align_corners=False).squeeze().numpy()\n",
    "        img = (img - img.mean()) / (img.std() + 1e-6)\n",
    "        return torch.from_numpy(img[None]).float()\n",
    "\n",
    "class TurboPrepDataset(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "        self.target_shape = (150, 192, 192)\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def _center_crop(self, img, target_shape):\n",
    "        starts = [(s - t) // 2 for s, t in zip(img.shape, target_shape)]\n",
    "        return img[starts[0]:starts[0]+target_shape[0],\n",
    "                   starts[1]:starts[1]+target_shape[1],\n",
    "                   starts[2]:starts[2]+target_shape[2]]\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.paths[idx]).get_fdata().astype(np.float32)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        img = self._center_crop(img, self.target_shape)\n",
    "        img = (img - img.mean()) / (img.std() + 1e-6)\n",
    "        return torch.from_numpy(img[None]).float()\n",
    "\n",
    "# Models\n",
    "\n",
    "class CNN3D(nn.Module):\n",
    "    def __init__(self, dropout_rate=DROPOUT_RATE):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 64, 3, padding=1)\n",
    "        self.bn1, self.pool1, self.drop1 = nn.BatchNorm3d(64), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv2 = nn.Conv3d(64, 64, 3, padding=1)\n",
    "        self.bn2, self.pool2, self.drop2 = nn.BatchNorm3d(64), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv3 = nn.Conv3d(64, 128, 3, padding=1)\n",
    "        self.bn3, self.pool3, self.drop3 = nn.BatchNorm3d(128), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.conv4 = nn.Conv3d(128, 256, 3, padding=1)\n",
    "        self.bn4, self.pool4, self.drop4 = nn.BatchNorm3d(256), nn.MaxPool3d(2), nn.Dropout3d(dropout_rate)\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc1, self.drop5 = nn.Linear(256, 512), nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop1(self.bn1(self.pool1(F.relu(self.conv1(x)))))\n",
    "        x = self.drop2(self.bn2(self.pool2(F.relu(self.conv2(x)))))\n",
    "        x = self.drop3(self.bn3(self.pool3(F.relu(self.conv3(x)))))\n",
    "        x = self.drop4(self.bn4(self.pool4(F.relu(self.conv4(x)))))\n",
    "        return self.drop5(F.relu(self.fc1(self.global_pool(x).view(x.size(0), -1))))\n",
    "\n",
    "def load_brainiac_vit(ckpt_path, device):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    vit = ViT(in_channels=1, img_size=(96, 96, 96), patch_size=(16, 16, 16),\n",
    "              hidden_size=768, mlp_dim=3072, num_layers=12, num_heads=12, save_attn=True)\n",
    "    pe_key = \"backbone.patch_embedding.position_embeddings\"\n",
    "    if pe_key in ckpt.get(\"state_dict\", ckpt):\n",
    "        vit.patch_embedding.position_embeddings = nn.Parameter(ckpt[\"state_dict\"][pe_key].clone())\n",
    "    backbone_state = {k.replace(\"backbone.\", \"\"): v for k, v in ckpt.get(\"state_dict\", ckpt).items()\n",
    "                     if k.startswith(\"backbone.\")}\n",
    "    vit.load_state_dict(backbone_state, strict=False)\n",
    "    return vit.to(device).eval()\n",
    "\n",
    "def load_swinbrain(ckpt_path, device):\n",
    "    model = SwinUNETR(img_size=(128, 128, 64), in_channels=3, out_channels=3, spatial_dims=3,\n",
    "                      feature_size=24, drop_rate=0.0, attn_drop_rate=0.0)\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    clean_state = {k.replace(\"module.\", \"\"): v for k, v in ckpt.items()}\n",
    "    model.load_state_dict(clean_state, strict=True)\n",
    "    return model.to(device).eval()\n",
    "\n",
    "def load_simclr_encoder(ckpt_path, device):\n",
    "    encoder = resnet18(spatial_dims=3, n_input_channels=1, num_classes=0)\n",
    "    encoder.fc = nn.Identity()\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    state_dict = {k.replace(\"module.encoder.\", \"\"): v\n",
    "                  for k, v in ckpt['model_state_dict'].items()\n",
    "                  if \"encoder\" in k and \"projector\" not in k}\n",
    "    encoder.load_state_dict(state_dict, strict=True)\n",
    "    return encoder.to(device).eval()\n",
    "\n",
    "def extract_cnn_features_multiseed(turboprep_paths, seeds, device):\n",
    "    dl = DataLoader(CNNDataset(turboprep_paths), batch_size=8, num_workers=0)\n",
    "    per_seed = []\n",
    "    for seed in seeds:\n",
    "        torch.manual_seed(seed)\n",
    "        model = CNN3D().to(device).eval()\n",
    "        with torch.no_grad():\n",
    "            feats = np.vstack([model(x.to(device)).cpu().numpy() for x in dl])\n",
    "        per_seed.append(feats)\n",
    "        del model\n",
    "    return np.hstack(per_seed)\n",
    "    \n",
    "# Downstream Analysis\n",
    "\n",
    "def run_classification(X_dict, y, test_size=0.1, task_name=\"Classification\"):\n",
    "    print(f\"\\n{task_name}\")\n",
    "    results = {}\n",
    "    alphas = [0.01, 0.05, 0.1, 0.15, 0.25, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "    dummy_cv_acc, dummy_cv_auc, dummy_test_acc, dummy_test_auc = [], [], [], []\n",
    "    for seed in SEEDS:\n",
    "        cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)),\n",
    "                                            stratify=y, random_state=seed)\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        for train_rel, val_rel in skf.split(cv_idx, y[cv_idx]):\n",
    "            train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "            dummy = DummyClassifier(strategy='stratified', random_state=seed)\n",
    "            dummy.fit(np.zeros((len(train_idx), 1)), y[train_idx])\n",
    "            dummy_cv_acc.append(balanced_accuracy_score(y[val_idx], dummy.predict(np.zeros((len(val_idx), 1)))))\n",
    "            dummy_cv_auc.append(roc_auc_score(y[val_idx], dummy.predict_proba(np.zeros((len(val_idx), 1)))[:, 1]))\n",
    "        dummy = DummyClassifier(strategy='stratified', random_state=seed)\n",
    "        dummy.fit(np.zeros((len(cv_idx), 1)), y[cv_idx])\n",
    "        dummy_test_acc.append(balanced_accuracy_score(y[test_idx], dummy.predict(np.zeros((len(test_idx), 1)))))\n",
    "        dummy_test_auc.append(roc_auc_score(y[test_idx], dummy.predict_proba(np.zeros((len(test_idx), 1)))[:, 1]))\n",
    "\n",
    "    results['DummyClassifier'] = {\n",
    "        'best_alpha': 1.0,\n",
    "        'test_acc_mean': np.mean(dummy_test_acc), 'test_acc_std': np.std(dummy_test_acc),\n",
    "        'test_auc_mean': np.mean(dummy_test_auc), 'test_auc_std': np.std(dummy_test_auc),\n",
    "        'cv_auc_mean': np.mean(dummy_cv_auc), 'cv_auc_std': np.std(dummy_cv_auc),\n",
    "        'cv_acc_mean': np.mean(dummy_cv_acc), 'cv_acc_std': np.std(dummy_cv_acc),\n",
    "        'fold_results': {'val_auc': dummy_cv_auc, 'val_acc': dummy_cv_acc},\n",
    "        'test_seed_results': {'test_acc': dummy_test_acc, 'test_auc': dummy_test_auc},\n",
    "    }\n",
    "    print(f\"Dummy: AUC={np.mean(dummy_test_auc):.3f}+/-{np.std(dummy_test_auc):.3f}\")\n",
    "\n",
    "    for model, X in X_dict.items():\n",
    "        all_val_auc = {a: [] for a in alphas}\n",
    "        all_val_acc = {a: [] for a in alphas}\n",
    "        all_train_auc = {a: [] for a in alphas}\n",
    "        all_train_acc = {a: [] for a in alphas}\n",
    "        all_test_acc, all_test_auc = [], []\n",
    "\n",
    "        for seed in SEEDS:\n",
    "            cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)),\n",
    "                                                stratify=y, random_state=seed)\n",
    "            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "            for train_rel, val_rel in skf.split(cv_idx, y[cv_idx]):\n",
    "                train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "                for alpha in alphas:\n",
    "                    n = int(alpha * len(train_idx))\n",
    "                    if n >= len(train_idx):\n",
    "                        tr_idx = train_idx\n",
    "                    else:\n",
    "                        tr_idx, _ = train_test_split(train_idx, train_size=n, stratify=y[train_idx], random_state=seed)\n",
    "                    if len(np.unique(y[tr_idx])) < 2:\n",
    "                        continue\n",
    "                    scaler = StandardScaler()\n",
    "                    rf = RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_split=5,\n",
    "                                               random_state=seed, n_jobs=1, max_features='sqrt', class_weight='balanced')\n",
    "                    rf.fit(scaler.fit_transform(X[tr_idx]), y[tr_idx])\n",
    "                    val_pred_proba = rf.predict_proba(scaler.transform(X[val_idx]))[:, 1]\n",
    "                    all_val_acc[alpha].append(balanced_accuracy_score(y[val_idx], rf.predict(scaler.transform(X[val_idx]))))\n",
    "                    all_val_auc[alpha].append(roc_auc_score(y[val_idx], val_pred_proba))\n",
    "                    all_train_acc[alpha].append(balanced_accuracy_score(y[tr_idx], rf.predict(scaler.transform(X[tr_idx]))))\n",
    "                    all_train_auc[alpha].append(roc_auc_score(y[tr_idx], rf.predict_proba(scaler.transform(X[tr_idx]))[:, 1]))\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            rf = RandomForestClassifier(n_estimators=200, max_depth=6, min_samples_split=5,\n",
    "                                        random_state=seed, n_jobs=1, max_features='sqrt', class_weight='balanced')\n",
    "            rf.fit(scaler.fit_transform(X[cv_idx]), y[cv_idx])\n",
    "            test_pred_proba = rf.predict_proba(scaler.transform(X[test_idx]))[:, 1]\n",
    "            all_test_acc.append(balanced_accuracy_score(y[test_idx], rf.predict(scaler.transform(X[test_idx]))))\n",
    "            all_test_auc.append(roc_auc_score(y[test_idx], test_pred_proba))\n",
    "\n",
    "        avg_auc = {a: np.mean(all_val_auc[a]) for a in alphas if all_val_auc[a]}\n",
    "        best_alpha = max(avg_auc, key=avg_auc.get)\n",
    "        train_val_gaps = {a: np.mean(all_train_auc[a]) - np.mean(all_val_auc[a]) for a in alphas if all_train_auc[a] and all_val_auc[a]}\n",
    "\n",
    "        results[model] = {\n",
    "            'best_alpha': best_alpha,\n",
    "            'test_acc_mean': np.mean(all_test_acc), 'test_acc_std': np.std(all_test_acc),\n",
    "            'test_auc_mean': np.mean(all_test_auc), 'test_auc_std': np.std(all_test_auc),\n",
    "            'cv_auc_mean': np.mean(all_val_auc[best_alpha]), 'cv_auc_std': np.std(all_val_auc[best_alpha]),\n",
    "            'cv_acc_mean': np.mean(all_val_acc[best_alpha]), 'cv_acc_std': np.std(all_val_acc[best_alpha]),\n",
    "            'train_val_gap': train_val_gaps.get(best_alpha, 0),\n",
    "            'is_overfitting': train_val_gaps.get(best_alpha, 0) > 0.25,\n",
    "            'fold_results': {'val_auc': all_val_auc[best_alpha], 'val_acc': all_val_acc[best_alpha]},\n",
    "            'fold_results_alpha1': {'val_acc': all_val_acc[1.0], 'val_auc': all_val_auc[1.0]},\n",
    "            'test_seed_results': {'test_acc': all_test_acc, 'test_auc': all_test_auc},\n",
    "            'cv_results': {\n",
    "                'alphas': alphas,\n",
    "                'acc': {a: np.mean(all_val_acc[a]) for a in alphas if all_val_acc[a]},\n",
    "                'auc': avg_auc,\n",
    "                'acc_std': {a: np.std(all_val_acc[a]) for a in alphas if all_val_acc[a]},\n",
    "                'auc_std': {a: np.std(all_val_auc[a]) for a in alphas if all_val_auc[a]},\n",
    "                'train_acc': {a: np.mean(all_train_acc[a]) for a in alphas if all_train_acc[a]},\n",
    "                'train_acc_std': {a: np.std(all_train_acc[a]) for a in alphas if all_train_acc[a]},\n",
    "                'train_auc': {a: np.mean(all_train_auc[a]) for a in alphas if all_train_auc[a]},\n",
    "                'train_auc_std': {a: np.std(all_train_auc[a]) for a in alphas if all_train_auc[a]},\n",
    "            }\n",
    "        }\n",
    "        print(f\"{model}: AUC={np.mean(all_test_auc):.3f}+/-{np.std(all_test_auc):.3f}, Acc={np.mean(all_test_acc):.3f}+/-{np.std(all_test_acc):.3f}\")\n",
    "    return results\n",
    "\n",
    "def run_regression(X_dict, y, test_size=0.1, task_name=\"Regression\"):\n",
    "    print(f\"\\n{task_name}\")\n",
    "    results = {}\n",
    "    alphas = [0.01, 0.05, 0.1, 0.15, 0.25, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "    dummy_cv_r2, dummy_cv_mae, dummy_test_r2, dummy_test_mae = [], [], [], []\n",
    "    for seed in SEEDS:\n",
    "        cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)), random_state=seed)\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        for train_rel, val_rel in kf.split(cv_idx):\n",
    "            train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "            dummy = DummyRegressor(strategy='mean')\n",
    "            dummy.fit(np.zeros((len(train_idx), 1)), y[train_idx])\n",
    "            pred = dummy.predict(np.zeros((len(val_idx), 1)))\n",
    "            dummy_cv_r2.append(r2_score(y[val_idx], pred))\n",
    "            dummy_cv_mae.append(mean_absolute_error(y[val_idx], pred))\n",
    "        dummy = DummyRegressor(strategy='mean')\n",
    "        dummy.fit(np.zeros((len(cv_idx), 1)), y[cv_idx])\n",
    "        test_pred = dummy.predict(np.zeros((len(test_idx), 1)))\n",
    "        dummy_test_r2.append(r2_score(y[test_idx], test_pred))\n",
    "        dummy_test_mae.append(mean_absolute_error(y[test_idx], test_pred))\n",
    "\n",
    "    results['DummyRegressor'] = {\n",
    "        'best_alpha': 1.0,\n",
    "        'test_r2_mean': np.mean(dummy_test_r2), 'test_r2_std': np.std(dummy_test_r2),\n",
    "        'test_mae_mean': np.mean(dummy_test_mae), 'test_mae_std': np.std(dummy_test_mae),\n",
    "        'cv_r2_mean': np.mean(dummy_cv_r2), 'cv_r2_std': np.std(dummy_cv_r2),\n",
    "        'cv_mae_mean': np.mean(dummy_cv_mae), 'cv_mae_std': np.std(dummy_cv_mae),\n",
    "        'fold_results': {'val_r2': dummy_cv_r2, 'val_mae': dummy_cv_mae},\n",
    "        'test_seed_results': {'test_mae': dummy_test_mae, 'test_r2': dummy_test_r2},\n",
    "    }\n",
    "    print(f\"Dummy: MAE={np.mean(dummy_test_mae):.2f}+/-{np.std(dummy_test_mae):.2f}\")\n",
    "\n",
    "    for model, X in X_dict.items():\n",
    "        all_val_r2 = {a: [] for a in alphas}\n",
    "        all_val_mae = {a: [] for a in alphas}\n",
    "        all_train_r2 = {a: [] for a in alphas}\n",
    "        all_train_mae = {a: [] for a in alphas}\n",
    "        all_test_r2, all_test_mae = [], []\n",
    "\n",
    "        for seed in SEEDS:\n",
    "            cv_idx, test_idx = train_test_split(np.arange(len(y)), test_size=int(test_size * len(y)), random_state=seed)\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "            for train_rel, val_rel in kf.split(cv_idx):\n",
    "                train_idx, val_idx = cv_idx[train_rel], cv_idx[val_rel]\n",
    "                for alpha in alphas:\n",
    "                    n = int(alpha * len(train_idx))\n",
    "                    tr_idx, _ = train_test_split(train_idx, train_size=n, random_state=seed) if n < len(train_idx) else (train_idx, None)\n",
    "                    scaler = StandardScaler()\n",
    "                    rf = RandomForestRegressor(n_estimators=200, max_depth=6, min_samples_split=5,\n",
    "                                               random_state=seed, n_jobs=1)\n",
    "                    rf.fit(scaler.fit_transform(X[tr_idx]), y[tr_idx])\n",
    "                    pred = rf.predict(scaler.transform(X[val_idx]))\n",
    "                    train_pred = rf.predict(scaler.transform(X[tr_idx]))\n",
    "                    all_val_r2[alpha].append(r2_score(y[val_idx], pred))\n",
    "                    all_val_mae[alpha].append(mean_absolute_error(y[val_idx], pred))\n",
    "                    all_train_r2[alpha].append(r2_score(y[tr_idx], train_pred))\n",
    "                    all_train_mae[alpha].append(mean_absolute_error(y[tr_idx], train_pred))\n",
    "\n",
    "            # Held-out test evaluation: train on ALL cv_idx, predict test_idx once per seed\n",
    "            scaler = StandardScaler()\n",
    "            rf = RandomForestRegressor(n_estimators=200, max_depth=6, min_samples_split=5,\n",
    "                                        random_state=seed, n_jobs=1)\n",
    "            rf.fit(scaler.fit_transform(X[cv_idx]), y[cv_idx])\n",
    "            test_pred = rf.predict(scaler.transform(X[test_idx]))\n",
    "            all_test_r2.append(r2_score(y[test_idx], test_pred))\n",
    "            all_test_mae.append(mean_absolute_error(y[test_idx], test_pred))\n",
    "\n",
    "        avg_r2 = {a: np.mean(all_val_r2[a]) for a in alphas if all_val_r2[a]}\n",
    "        best_alpha = max(avg_r2, key=avg_r2.get)\n",
    "        train_val_gaps = {a: np.mean(all_train_r2[a]) - np.mean(all_val_r2[a]) for a in alphas if all_train_r2[a] and all_val_r2[a]}\n",
    "\n",
    "        results[model] = {\n",
    "            'best_alpha': best_alpha,\n",
    "            'test_r2_mean': np.mean(all_test_r2), 'test_r2_std': np.std(all_test_r2),\n",
    "            'test_mae_mean': np.mean(all_test_mae), 'test_mae_std': np.std(all_test_mae),\n",
    "            'cv_r2_mean': np.mean(all_val_r2[best_alpha]), 'cv_r2_std': np.std(all_val_r2[best_alpha]),\n",
    "            'cv_mae_mean': np.mean(all_val_mae[best_alpha]), 'cv_mae_std': np.std(all_val_mae[best_alpha]),\n",
    "            'train_val_gap': train_val_gaps.get(best_alpha, 0),\n",
    "            'is_overfitting': train_val_gaps.get(best_alpha, 0) > 0.35,\n",
    "            'fold_results': {'val_r2': all_val_r2[best_alpha], 'val_mae': all_val_mae[best_alpha]},\n",
    "            'fold_results_alpha1': {'val_mae': all_val_mae[1.0], 'val_r2': all_val_r2[1.0]},\n",
    "            'test_seed_results': {'test_mae': all_test_mae, 'test_r2': all_test_r2},\n",
    "            'cv_results': {\n",
    "                'alphas': alphas,\n",
    "                'r2': avg_r2,\n",
    "                'mae': {a: np.mean(all_val_mae[a]) for a in alphas if all_val_mae[a]},\n",
    "                'r2_std': {a: np.std(all_val_r2[a]) for a in alphas if all_val_r2[a]},\n",
    "                'mae_std': {a: np.std(all_val_mae[a]) for a in alphas if all_val_mae[a]},\n",
    "                'train_r2': {a: np.mean(all_train_r2[a]) for a in alphas if all_train_r2[a]},\n",
    "                'train_r2_std': {a: np.std(all_train_r2[a]) for a in alphas if all_train_r2[a]},\n",
    "                'train_mae': {a: np.mean(all_train_mae[a]) for a in alphas if all_train_mae[a]},\n",
    "                'train_mae_std': {a: np.std(all_train_mae[a]) for a in alphas if all_train_mae[a]},\n",
    "            }\n",
    "        }\n",
    "        print(f\"{model}: MAE={np.mean(all_test_mae):.2f}+/-{np.std(all_test_mae):.2f}, R2={np.mean(all_test_r2):.3f}+/-{np.std(all_test_r2):.3f}\")\n",
    "    return results\n",
    "\n",
    "# Correlation Analysis\n",
    "\n",
    "def compute_pairwise_correlations(features_dict, dataset_name):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MEAN ABSOLUTE CORRELATIONS - {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    model_names = list(features_dict.keys())\n",
    "    scaled_features = {}\n",
    "    for model in model_names:\n",
    "        X = StandardScaler().fit_transform(features_dict[model])\n",
    "        valid = ~np.isnan(X).any(axis=0) & (np.std(X, axis=0) > 1e-10)\n",
    "        scaled_features[model] = X[:, valid]\n",
    "    corr_results = {}\n",
    "    for i, model_i in enumerate(model_names):\n",
    "        for j, model_j in enumerate(model_names):\n",
    "            if j <= i:\n",
    "                continue\n",
    "            X_i, X_j = scaled_features[model_i], scaled_features[model_j]\n",
    "            cross_corr = np.corrcoef(X_i.T, X_j.T)[:X_i.shape[1], X_i.shape[1]:]\n",
    "            corr_results[(model_i, model_j)] = np.mean(np.abs(cross_corr))\n",
    "    fs_keys = [k for k in model_names if 'FreeSurfer' in k or 'FS_' in k or k == 'FreeSurfer']\n",
    "    for fs_key in fs_keys:\n",
    "        print(f\"\\nvs {fs_key}:\")\n",
    "        for model in model_names:\n",
    "            if model == fs_key:\n",
    "                continue\n",
    "            key = (model, fs_key) if (model, fs_key) in corr_results else (fs_key, model)\n",
    "            print(f\"  {model} vs {fs_key}: {corr_results[key]:.4f}\")\n",
    "    print(f\"\\nAll pairwise (sorted by correlation):\")\n",
    "    for (m1, m2), corr in sorted(corr_results.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {m1} vs {m2}: {corr:.4f}\")\n",
    "\n",
    "def compute_correlation_with_cluster_labels(features_dict, fs_feature_infos, fs_keys, dataset_name):\n",
    "    all_features, boundaries, reordered_indices, model_names_used, cluster_dfs = [], [0], {}, [], {}\n",
    "    cnn_keys = list(CNN_SEED_CONFIGS.keys())\n",
    "    model_order = ['AnatCL_Global', 'AnatCL_Local', 'BrainIAC', 'SwinBrain'] + \\\n",
    "                  cnn_keys + ['3D-Neuro-SimCLR'] + fs_keys\n",
    "\n",
    "    for model in model_order:\n",
    "        if model not in features_dict:\n",
    "            continue\n",
    "        feats = StandardScaler().fit_transform(features_dict[model])\n",
    "        valid = ~np.isnan(feats).any(axis=0) & (np.std(feats, axis=0) > 1e-10)\n",
    "        feats = feats[:, valid]\n",
    "        if feats.shape[1] == 0:\n",
    "            continue\n",
    "        print(f\"  {model}: {feats.shape[1]} features\")\n",
    "        corr = np.corrcoef(feats.T)\n",
    "        corr = np.nan_to_num((corr + corr.T) / 2, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        np.fill_diagonal(corr, 1.0)\n",
    "        dist = np.abs(1 - np.abs(corr))\n",
    "        np.fill_diagonal(dist, 0)\n",
    "        condensed = np.nan_to_num(squareform((dist + dist.T) / 2, checks=False), nan=0.0, posinf=1.0, neginf=0.0)\n",
    "        reorder_idx = dendrogram(linkage(condensed, method='ward'), no_plot=True)['leaves']\n",
    "        if model in fs_keys and model in fs_feature_infos:\n",
    "            fs_info = fs_feature_infos[model]\n",
    "            thickness_indices = [i for i in reorder_idx if fs_info[i]['feature_type'] == 'Thickness']\n",
    "            surfarea_indices  = [i for i in reorder_idx if fs_info[i]['feature_type'] == 'Surface_Area']\n",
    "            reorder_idx = thickness_indices + surfarea_indices\n",
    "        all_features.append(feats[:, reorder_idx])\n",
    "        boundaries.append(boundaries[-1] + feats.shape[1])\n",
    "        reordered_indices[model] = reorder_idx\n",
    "        model_names_used.append(model)\n",
    "\n",
    "    combined = np.hstack(all_features)\n",
    "    corr = np.nan_to_num((np.corrcoef(combined.T) + np.corrcoef(combined.T).T) / 2, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "    for fs_key in fs_keys:\n",
    "        if fs_key in reordered_indices:\n",
    "            fs_info = fs_feature_infos[fs_key]\n",
    "            cluster_df = pd.DataFrame([{**fs_info[idx], 'original_index': idx, 'reordered_position': pos}\n",
    "                                       for pos, idx in enumerate(reordered_indices[fs_key])])\n",
    "            cluster_dfs[fs_key] = cluster_df\n",
    "    return corr, boundaries, model_names_used, reordered_indices, cluster_dfs\n",
    "\n",
    "def plot_correlation_with_labels(corr_matrix, boundaries, model_names, cluster_dfs, fs_keys, dataset_name):\n",
    "    fig, ax = plt.subplots(figsize=(32, 30))\n",
    "    im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1, aspect='equal', interpolation='nearest')\n",
    "    plt.colorbar(im, ax=ax, shrink=0.3)\n",
    "    for b in boundaries[1:-1]:\n",
    "        ax.axhline(y=b-0.5, color='black', linewidth=4)\n",
    "        ax.axvline(x=b-0.5, color='black', linewidth=4)\n",
    "    bar_width = 8\n",
    "    colors = {'Surface_Area': '#2ECC71', 'Thickness': '#3498DB'}\n",
    "    for fs_key in fs_keys:\n",
    "        if fs_key not in cluster_dfs:\n",
    "            continue\n",
    "        cluster_df = cluster_dfs[fs_key]\n",
    "        fs_idx = model_names.index(fs_key)\n",
    "        fs_start = boundaries[fs_idx]\n",
    "        for i, ft in enumerate(cluster_df['feature_type'].values):\n",
    "            color = colors.get(ft, '#95A5A6')\n",
    "            ax.add_patch(plt.Rectangle((boundaries[-1] + 3, fs_start + i - 0.5), bar_width, 1, facecolor=color, edgecolor='none'))\n",
    "            ax.add_patch(plt.Rectangle((-bar_width - 5, fs_start + i - 0.5), bar_width, 1, facecolor=color, edgecolor='none'))\n",
    "    display_names = {'FS_Schaefer': 'FS: Schaefer', 'FS_aparc': 'FS: aparc', 'FreeSurfer': 'FS'}\n",
    "    label_fontsize = 18 if dataset_name.lower() == 'ppmi' else 10\n",
    "    for i in range(len(boundaries) - 1):\n",
    "        pos = (boundaries[i] + boundaries[i+1]) / 2\n",
    "        label = display_names.get(model_names[i], model_names[i])\n",
    "        ax.text(pos, -30, label, ha='center', fontsize=label_fontsize, weight='bold')\n",
    "        ax.text(-30, pos, label, ha='right', va='center', fontsize=label_fontsize, weight='bold', rotation=90)\n",
    "    ax.set_xlim(-bar_width - 60, boundaries[-1] + bar_width + 50)\n",
    "    ax.set_ylim(boundaries[-1] + 20, -60)\n",
    "    ax.set_xticks([]); ax.set_yticks([]); ax.axis('off')\n",
    "    ax.legend(handles=[Patch(facecolor='#2ECC71', edgecolor='black', label='Surface Area'),\n",
    "                       Patch(facecolor='#3498DB', edgecolor='black', label='Thickness')],\n",
    "              loc='upper left', fontsize=14, bbox_to_anchor=(1.02, 1.0), frameon=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{dataset_name.lower()}_cross_model_correlation.pdf', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {dataset_name.lower()}_cross_model_correlation.pdf\")\n",
    "\n",
    "def export_correlation_data(corr_matrix, boundaries, model_names, cluster_dfs, dataset_name, output_dir=''):\n",
    "    prefix = os.path.join(output_dir, dataset_name.lower())\n",
    "    np.save(f'{prefix}_corr_matrix.npy', corr_matrix)\n",
    "    meta_rows = []\n",
    "    for i, model in enumerate(model_names):\n",
    "        meta_rows.append({'dataset': dataset_name, 'model': model,\n",
    "                          'boundary_start': boundaries[i], 'boundary_end': boundaries[i+1],\n",
    "                          'n_features': boundaries[i+1] - boundaries[i]})\n",
    "    pd.DataFrame(meta_rows).to_csv(f'{prefix}_corr_metadata.csv', index=False)\n",
    "    print(f\"Saved: {prefix}_corr_matrix.npy ({corr_matrix.shape})\")\n",
    "    print(f\"Saved: {prefix}_corr_metadata.csv\")\n",
    "\n",
    "# Feature Extraction\n",
    "\n",
    "def extract_features_ppmi(cat12_paths, turboprep_paths, device):\n",
    "    anatcl_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda x: torch.from_numpy(x.copy()).float()),\n",
    "        transforms.Normalize(mean=0.0, std=1.0)\n",
    "    ])\n",
    "\n",
    "    print(\"Extracting AnatCL (global)\")\n",
    "    all_fold_features = []\n",
    "    for fold_idx in range(5):\n",
    "        path = os.path.join(ANATCL_ENCODER_PATH, f\"weights_global_fold{fold_idx}.pth\")\n",
    "        encoder = AnatCL(descriptor=\"global\", fold=fold_idx, pretrained=False).to(device).eval()\n",
    "        encoder.backbone.load_state_dict(torch.load(path, map_location=device, weights_only=False)['model'])\n",
    "        for p in encoder.parameters(): p.requires_grad = False\n",
    "        dl = DataLoader(CAT12VBMDataset(cat12_paths, anatcl_transform), batch_size=32, num_workers=0)\n",
    "        with torch.no_grad():\n",
    "            all_fold_features.append(torch.cat([encoder(v.to(device)).cpu() for v in dl]).numpy())\n",
    "        del encoder\n",
    "    anatcl_features = np.mean(all_fold_features, axis=0)\n",
    "    print(f\"  AnatCL (global): {anatcl_features.shape}\")\n",
    "\n",
    "    print(\"Extracting AnatCL (local)\")\n",
    "    all_fold_features_local = []\n",
    "    for fold_idx in range(5):\n",
    "        path = os.path.join(ANATCL_ENCODER_PATH, f\"weights_local_fold{fold_idx}.pth\")\n",
    "        encoder = AnatCL(descriptor=\"local\", fold=fold_idx, pretrained=False).to(device).eval()\n",
    "        encoder.backbone.load_state_dict(torch.load(path, map_location=device, weights_only=False)['model'])\n",
    "        for p in encoder.parameters(): p.requires_grad = False\n",
    "        dl = DataLoader(CAT12VBMDataset(cat12_paths, anatcl_transform), batch_size=32, num_workers=0)\n",
    "        with torch.no_grad():\n",
    "            all_fold_features_local.append(torch.cat([encoder(v.to(device)).cpu() for v in dl]).numpy())\n",
    "        del encoder\n",
    "    anatcl_local_features = np.mean(all_fold_features_local, axis=0)\n",
    "    print(f\"  AnatCL (local): {anatcl_local_features.shape}\")\n",
    "\n",
    "    print(\"Extracting SwinBrain\")\n",
    "    swinbrain_model = load_swinbrain(SWINBRAIN_CKPT, device)\n",
    "    swinbrain_hook_output = [None]\n",
    "    def swinbrain_hook(module, input, output): swinbrain_hook_output[0] = output\n",
    "    hook_handle = swinbrain_model.encoder10.register_forward_hook(swinbrain_hook)\n",
    "    pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "    dl = DataLoader(SwinBrainDataset(turboprep_paths), batch_size=8, num_workers=0)\n",
    "    swinbrain_list = []\n",
    "    with torch.no_grad():\n",
    "        for x in dl:\n",
    "            _ = swinbrain_model(x.to(device))\n",
    "            pooled = pool(swinbrain_hook_output[0])\n",
    "            swinbrain_list.append(pooled.view(pooled.size(0), -1).cpu().numpy())\n",
    "    hook_handle.remove()\n",
    "    swinbrain_features = np.vstack(swinbrain_list)\n",
    "    print(f\"  SwinBrain: {swinbrain_features.shape}\")\n",
    "    del swinbrain_model\n",
    "\n",
    "    cnn_features_dict = {}\n",
    "    for tag, seeds in CNN_SEED_CONFIGS.items():\n",
    "        print(f\"Extracting {tag} (seeds={seeds})\")\n",
    "        cnn_features_dict[tag] = extract_cnn_features_multiseed(turboprep_paths, seeds, device)\n",
    "        print(f\"  {tag}: {cnn_features_dict[tag].shape}\")\n",
    "\n",
    "    return anatcl_features, anatcl_local_features, swinbrain_features, cnn_features_dict\n",
    "\n",
    "def extract_features_full(cat12_paths, turboprep_paths, device):\n",
    "    anatcl_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda x: torch.from_numpy(x.copy()).float()),\n",
    "        transforms.Normalize(mean=0.0, std=1.0)\n",
    "    ])\n",
    "\n",
    "    print(\"Extracting AnatCL (global)\")\n",
    "    all_fold_features = []\n",
    "    for fold_idx in range(5):\n",
    "        path = os.path.join(ANATCL_ENCODER_PATH, f\"weights_global_fold{fold_idx}.pth\")\n",
    "        encoder = AnatCL(descriptor=\"global\", fold=fold_idx, pretrained=False).to(device).eval()\n",
    "        encoder.backbone.load_state_dict(torch.load(path, map_location=device, weights_only=False)['model'])\n",
    "        for p in encoder.parameters(): p.requires_grad = False\n",
    "        dl = DataLoader(CAT12VBMDataset(cat12_paths, anatcl_transform), batch_size=32, num_workers=0)\n",
    "        with torch.no_grad():\n",
    "            all_fold_features.append(torch.cat([encoder(v.to(device)).cpu() for v in dl]).numpy())\n",
    "        del encoder\n",
    "    anatcl_features = np.mean(all_fold_features, axis=0)\n",
    "    print(f\"  AnatCL (global): {anatcl_features.shape}\")\n",
    "\n",
    "    print(\"Extracting AnatCL (local)\")\n",
    "    all_fold_features_local = []\n",
    "    for fold_idx in range(5):\n",
    "        path = os.path.join(ANATCL_ENCODER_PATH, f\"weights_local_fold{fold_idx}.pth\")\n",
    "        encoder = AnatCL(descriptor=\"local\", fold=fold_idx, pretrained=False).to(device).eval()\n",
    "        encoder.backbone.load_state_dict(torch.load(path, map_location=device, weights_only=False)['model'])\n",
    "        for p in encoder.parameters(): p.requires_grad = False\n",
    "        dl = DataLoader(CAT12VBMDataset(cat12_paths, anatcl_transform), batch_size=32, num_workers=0)\n",
    "        with torch.no_grad():\n",
    "            all_fold_features_local.append(torch.cat([encoder(v.to(device)).cpu() for v in dl]).numpy())\n",
    "        del encoder\n",
    "    anatcl_local_features = np.mean(all_fold_features_local, axis=0)\n",
    "    print(f\"  AnatCL (local): {anatcl_local_features.shape}\")\n",
    "\n",
    "    print(\"Extracting BrainIAC\")\n",
    "    brainiac_vit = load_brainiac_vit(BRAINIAC_CKPT, device)\n",
    "    brainiac_transform = lambda x: (x - x.mean()) / (x.std() + 1e-6)\n",
    "    dl = DataLoader(BrainIACDataset(turboprep_paths, brainiac_transform), batch_size=16, num_workers=0)\n",
    "    brainiac_list = []\n",
    "    with torch.no_grad():\n",
    "        for x in dl:\n",
    "            out = brainiac_vit(x.to(device))\n",
    "            cls_token = out[0][:, 0] if isinstance(out, tuple) else out[:, 0]\n",
    "            brainiac_list.append(cls_token.cpu().numpy())\n",
    "    brainiac_features = np.vstack(brainiac_list)\n",
    "    print(f\"  BrainIAC: {brainiac_features.shape}\")\n",
    "    del brainiac_vit\n",
    "\n",
    "    print(\"Extracting SwinBrain\")\n",
    "    swinbrain_model = load_swinbrain(SWINBRAIN_CKPT, device)\n",
    "    swinbrain_hook_output = [None]\n",
    "    def swinbrain_hook(module, input, output): swinbrain_hook_output[0] = output\n",
    "    hook_handle = swinbrain_model.encoder10.register_forward_hook(swinbrain_hook)\n",
    "    pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "    dl = DataLoader(SwinBrainDataset(turboprep_paths), batch_size=8, num_workers=0)\n",
    "    swinbrain_list = []\n",
    "    with torch.no_grad():\n",
    "        for x in dl:\n",
    "            _ = swinbrain_model(x.to(device))\n",
    "            pooled = pool(swinbrain_hook_output[0])\n",
    "            swinbrain_list.append(pooled.view(pooled.size(0), -1).cpu().numpy())\n",
    "    hook_handle.remove()\n",
    "    swinbrain_features = np.vstack(swinbrain_list)\n",
    "    print(f\"  SwinBrain: {swinbrain_features.shape}\")\n",
    "    del swinbrain_model\n",
    "\n",
    "    cnn_features_dict = {}\n",
    "    for tag, seeds in CNN_SEED_CONFIGS.items():\n",
    "        print(f\"Extracting {tag} (seeds={seeds})\")\n",
    "        cnn_features_dict[tag] = extract_cnn_features_multiseed(turboprep_paths, seeds, device)\n",
    "        print(f\"  {tag}: {cnn_features_dict[tag].shape}\")\n",
    "\n",
    "    print(\"Extracting 3D-Neuro-SimCLR\")\n",
    "    simclr_encoder = load_simclr_encoder(SIMCLR_CKPT, device)\n",
    "    dl = DataLoader(TurboPrepDataset(turboprep_paths), batch_size=8, num_workers=0)\n",
    "    simclr_list = []\n",
    "    with torch.no_grad():\n",
    "        for x in dl:\n",
    "            simclr_list.append(simclr_encoder(x.to(device)).cpu().numpy())\n",
    "    simclr_features = np.vstack(simclr_list)\n",
    "    print(f\"  3D-Neuro-SimCLR: {simclr_features.shape}\")\n",
    "    del simclr_encoder\n",
    "\n",
    "    return anatcl_features, anatcl_local_features, brainiac_features, swinbrain_features, cnn_features_dict, simclr_features\n",
    "\n",
    "def mean_statistic(sample, axis):\n",
    "    return np.mean(sample, axis=axis)\n",
    "\n",
    "def run_permutation_test_classification(results, fs_key, models_to_test, n_resamples=9999):\n",
    "    perm_results = {}\n",
    "    if fs_key not in results:\n",
    "        return perm_results\n",
    "    fs_folds = np.array(results[fs_key]['fold_results_alpha1']['val_acc'])\n",
    "    fs_mean = np.mean(fs_folds)\n",
    "    models_beating_fs = [m for m in models_to_test if m != fs_key and m in results and 'Dummy' not in m\n",
    "                         and np.mean(results[m]['fold_results_alpha1']['val_acc']) > fs_mean]\n",
    "    n_tests = len(models_beating_fs)\n",
    "    if n_tests == 0:\n",
    "        return perm_results\n",
    "    for model in models_beating_fs:\n",
    "        model_folds = np.array(results[model]['fold_results_alpha1']['val_acc'])\n",
    "        diff = model_folds - fs_folds\n",
    "        res = permutation_test((diff,), mean_statistic, permutation_type='samples', n_resamples=n_resamples, vectorized=True, axis=0, alternative='greater')\n",
    "        p_corr = min(res.pvalue * n_tests, 1.0)\n",
    "        perm_results[model] = {'p_raw': res.pvalue, 'p_corr': p_corr, 'n_tests': n_tests, 'diff': np.mean(diff), 'significant': p_corr < 0.05}\n",
    "        print(f\"    {model}: diff={np.mean(diff):.4f}, p_raw={res.pvalue:.4f}, p_corr={p_corr:.4f}\")\n",
    "    return perm_results\n",
    "\n",
    "def run_permutation_test_regression(results, fs_key, models_to_test, n_resamples=9999):\n",
    "    perm_results = {}\n",
    "    if fs_key not in results:\n",
    "        return perm_results\n",
    "    fs_folds = np.array(results[fs_key]['fold_results_alpha1']['val_mae'])\n",
    "    fs_mean = np.mean(fs_folds)\n",
    "    models_beating_fs = [m for m in models_to_test if m != fs_key and m in results and 'Dummy' not in m\n",
    "                         and np.mean(results[m]['fold_results_alpha1']['val_mae']) < fs_mean]\n",
    "    n_tests = len(models_beating_fs)\n",
    "    if n_tests == 0:\n",
    "        return perm_results\n",
    "    for model in models_beating_fs:\n",
    "        model_folds = np.array(results[model]['fold_results_alpha1']['val_mae'])\n",
    "        diff = model_folds - fs_folds\n",
    "        res = permutation_test((diff,), mean_statistic, permutation_type='samples', n_resamples=n_resamples, vectorized=True, axis=0, alternative='less')\n",
    "        p_corr = min(res.pvalue * n_tests, 1.0)\n",
    "        perm_results[model] = {'p_raw': res.pvalue, 'p_corr': p_corr, 'n_tests': n_tests, 'diff': np.mean(diff), 'significant': p_corr < 0.05}\n",
    "        print(f\"    {model}: diff={np.mean(diff):.4f}, p_raw={res.pvalue:.4f}, p_corr={p_corr:.4f}\")\n",
    "    return perm_results\n",
    "\n",
    "alphas = [0.01, 0.05, 0.1, 0.15, 0.25, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "def get_training_sizes(n_total, alphas, test_size=0.1, n_folds=5):\n",
    "    n_test = int(test_size * n_total)\n",
    "    n_cv = n_total - n_test\n",
    "    n_train_fold = n_cv - (n_cv // n_folds)\n",
    "    return [int(a * n_train_fold) for a in alphas]\n",
    "\n",
    "def export_cv_results_classification(results, training_sizes, dataset_name, task_name, models_list):\n",
    "    rows = []\n",
    "    for model in models_list:\n",
    "        if model not in results:\n",
    "            continue\n",
    "        cv_res = results[model].get('cv_results')\n",
    "        for i, alpha in enumerate(alphas):\n",
    "            if cv_res and alpha in cv_res.get('acc', {}):\n",
    "                rows.append({\n",
    "                    'dataset': dataset_name, 'task': task_name, 'model': model, 'alpha': alpha,\n",
    "                    'n_train': training_sizes[i],\n",
    "                    'val_acc_mean': cv_res['acc'][alpha], 'val_acc_std': cv_res['acc_std'][alpha],\n",
    "                    'val_auc_mean': cv_res['auc'].get(alpha, np.nan), 'val_auc_std': cv_res['auc_std'].get(alpha, np.nan),\n",
    "                    'train_acc_mean': cv_res.get('train_acc', {}).get(alpha, np.nan),\n",
    "                    'train_acc_std': cv_res.get('train_acc_std', {}).get(alpha, np.nan),\n",
    "                    'train_auc_mean': cv_res.get('train_auc', {}).get(alpha, np.nan),\n",
    "                    'train_auc_std': cv_res.get('train_auc_std', {}).get(alpha, np.nan),\n",
    "                    'test_acc_mean': results[model]['test_acc_mean'], 'test_acc_std': results[model]['test_acc_std'],\n",
    "                    'test_auc_mean': results[model]['test_auc_mean'], 'test_auc_std': results[model]['test_auc_std'],\n",
    "                })\n",
    "    if 'DummyClassifier' in results:\n",
    "        dummy = results['DummyClassifier']\n",
    "        for i, alpha in enumerate(alphas):\n",
    "            rows.append({\n",
    "                'dataset': dataset_name, 'task': task_name, 'model': 'Dummy', 'alpha': alpha,\n",
    "                'n_train': training_sizes[i],\n",
    "                'val_acc_mean': dummy['cv_acc_mean'], 'val_acc_std': dummy['cv_acc_std'],\n",
    "                'val_auc_mean': dummy['cv_auc_mean'], 'val_auc_std': dummy['cv_auc_std'],\n",
    "                'train_acc_mean': np.nan, 'train_acc_std': np.nan,\n",
    "                'train_auc_mean': np.nan, 'train_auc_std': np.nan,\n",
    "                'test_acc_mean': dummy['test_acc_mean'], 'test_acc_std': dummy['test_acc_std'],\n",
    "                'test_auc_mean': dummy['test_auc_mean'], 'test_auc_std': dummy['test_auc_std'],\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def export_cv_results_regression(results, training_sizes, dataset_name, task_name, models_list):\n",
    "    rows = []\n",
    "    for model in models_list:\n",
    "        if model not in results:\n",
    "            continue\n",
    "        cv_res = results[model].get('cv_results')\n",
    "        for i, alpha in enumerate(alphas):\n",
    "            if cv_res and alpha in cv_res.get('mae', {}):\n",
    "                rows.append({\n",
    "                    'dataset': dataset_name, 'task': task_name, 'model': model, 'alpha': alpha,\n",
    "                    'n_train': training_sizes[i],\n",
    "                    'val_mae_mean': cv_res['mae'][alpha], 'val_mae_std': cv_res['mae_std'][alpha],\n",
    "                    'val_r2_mean': cv_res['r2'].get(alpha, np.nan), 'val_r2_std': cv_res['r2_std'].get(alpha, np.nan),\n",
    "                    'train_mae_mean': cv_res.get('train_mae', {}).get(alpha, np.nan),\n",
    "                    'train_mae_std': cv_res.get('train_mae_std', {}).get(alpha, np.nan),\n",
    "                    'train_r2_mean': cv_res.get('train_r2', {}).get(alpha, np.nan),\n",
    "                    'train_r2_std': cv_res.get('train_r2_std', {}).get(alpha, np.nan),\n",
    "                    'test_mae_mean': results[model]['test_mae_mean'], 'test_mae_std': results[model]['test_mae_std'],\n",
    "                    'test_r2_mean': results[model]['test_r2_mean'], 'test_r2_std': results[model]['test_r2_std'],\n",
    "                })\n",
    "    if 'DummyRegressor' in results:\n",
    "        dummy = results['DummyRegressor']\n",
    "        for i, alpha in enumerate(alphas):\n",
    "            rows.append({\n",
    "                'dataset': dataset_name, 'task': task_name, 'model': 'Dummy', 'alpha': alpha,\n",
    "                'n_train': training_sizes[i],\n",
    "                'val_mae_mean': dummy['cv_mae_mean'], 'val_mae_std': dummy['cv_mae_std'],\n",
    "                'val_r2_mean': dummy['cv_r2_mean'], 'val_r2_std': dummy['cv_r2_std'],\n",
    "                'train_mae_mean': np.nan, 'train_mae_std': np.nan,\n",
    "                'train_r2_mean': np.nan, 'train_r2_std': np.nan,\n",
    "                'test_mae_mean': dummy['test_mae_mean'], 'test_mae_std': dummy['test_mae_std'],\n",
    "                'test_r2_mean': dummy['test_r2_mean'], 'test_r2_std': dummy['test_r2_std'],\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# PPMI Dataset\n",
    "print(\"PPMI Dataset\")\n",
    "\n",
    "PPMI_CAT12_BASE_DIR = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data/inputs\"\n",
    "PPMI_DATA_DIR = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data\"\n",
    "PPMI_LABELS_PATH = \"/home/arelbaha/links/projects/def-glatard/arelbaha/data/processed_cohort_with_mri.csv\"\n",
    "PPMI_TURBOPREP_DIR = \"/home/arelbaha/links/scratch/turboprep_output_ppmi\"\n",
    "\n",
    "print(\"Loading PPMI demographics...\")\n",
    "ppmi_labels_df = pd.read_csv(PPMI_LABELS_PATH)\n",
    "ppmi_id_sex_dict, ppmi_id_parkinson_dict, ppmi_id_age_dict = {}, {}, {}\n",
    "for _, row in ppmi_labels_df.iterrows():\n",
    "    patno = str(int(row['PATNO']))\n",
    "    ppmi_id_sex_dict[patno] = 1 if row['Sex'].strip().upper() == 'F' else 0\n",
    "    ppmi_id_parkinson_dict[patno] = 1 if row['Group'].strip() == 'PD' else 0\n",
    "    ppmi_id_age_dict[patno] = row['Age']\n",
    "print(f\"  {len(ppmi_id_sex_dict)} subjects\")\n",
    "\n",
    "def extract_patno_from_path(filepath):\n",
    "    for part in filepath.split(os.sep):\n",
    "        if part.startswith('sub-'):\n",
    "            return part[4:]\n",
    "    return None\n",
    "\n",
    "print(\"Finding PPMI files\")\n",
    "ppmi_cat12_files = glob.glob(os.path.join(PPMI_CAT12_BASE_DIR, \"**\", \"mwp1*.nii*\"), recursive=True)\n",
    "ppmi_cat12_data = {}\n",
    "for f in ppmi_cat12_files:\n",
    "    if os.path.isfile(f):\n",
    "        patno = extract_patno_from_path(f)\n",
    "        if patno and patno in ppmi_id_sex_dict:\n",
    "            ppmi_cat12_data[patno] = f\n",
    "\n",
    "print(\"Finding PPMI TurboPrep files\")\n",
    "ppmi_turboprep_data = {}\n",
    "for folder in os.listdir(PPMI_TURBOPREP_DIR):\n",
    "    folder_path = os.path.join(PPMI_TURBOPREP_DIR, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        normalized_file = os.path.join(folder_path, \"normalized.nii.gz\")\n",
    "        if os.path.exists(normalized_file) and folder in ppmi_id_sex_dict:\n",
    "            ppmi_turboprep_data[folder] = normalized_file\n",
    "print(f\"  Found {len(ppmi_turboprep_data)} TurboPrep subjects\")\n",
    "\n",
    "print(\"Loading PPMI FreeSurfer\")\n",
    "ppmi_fs_cth_df = pd.read_csv(os.path.join(PPMI_CAT12_BASE_DIR, \"FS7_APARC_CTH_23Oct2025.csv\"))\n",
    "ppmi_fs_sa_df  = pd.read_csv(os.path.join(PPMI_CAT12_BASE_DIR, \"FS7_APARC_SA_23Oct2025.csv\"))\n",
    "ppmi_fs_cth_df = ppmi_fs_cth_df[ppmi_fs_cth_df['EVENT_ID'] == 'BL'].copy()\n",
    "ppmi_fs_sa_df  = ppmi_fs_sa_df[ppmi_fs_sa_df['EVENT_ID'] == 'BL'].copy()\n",
    "ppmi_fs_cth_df['PATNO'] = ppmi_fs_cth_df['PATNO'].astype(str)\n",
    "ppmi_fs_sa_df['PATNO']  = ppmi_fs_sa_df['PATNO'].astype(str)\n",
    "ppmi_cth_features = [c for c in ppmi_fs_cth_df.columns if c not in ['PATNO', 'EVENT_ID']]\n",
    "ppmi_sa_features  = [c for c in ppmi_fs_sa_df.columns  if c not in ['PATNO', 'EVENT_ID']]\n",
    "\n",
    "ppmi_fs_feature_info = ([{'feature_name': f, 'feature_type': 'Thickness'}    for f in ppmi_cth_features] +\n",
    "                         [{'feature_name': f, 'feature_type': 'Surface_Area'} for f in ppmi_sa_features])\n",
    "\n",
    "ppmi_fs_data = {}\n",
    "for patno in set(ppmi_cat12_data.keys()) & set(ppmi_turboprep_data.keys()):\n",
    "    cth_row = ppmi_fs_cth_df[ppmi_fs_cth_df['PATNO'] == patno]\n",
    "    sa_row  = ppmi_fs_sa_df[ppmi_fs_sa_df['PATNO'] == patno]\n",
    "    if len(cth_row) > 0 and len(sa_row) > 0:\n",
    "        combined = np.concatenate([cth_row[ppmi_cth_features].values.flatten(),\n",
    "                                   sa_row[ppmi_sa_features].values.flatten()])\n",
    "        if not np.any(np.isnan(combined)):\n",
    "            ppmi_fs_data[patno] = combined\n",
    "\n",
    "ppmi_common_subjects = sorted(list(set(ppmi_cat12_data.keys()) & set(ppmi_turboprep_data.keys()) & set(ppmi_fs_data.keys())))\n",
    "print(f\"  Common subjects: {len(ppmi_common_subjects)}\")\n",
    "\n",
    "ppmi_cat12_paths     = [ppmi_cat12_data[p]    for p in ppmi_common_subjects]\n",
    "ppmi_turboprep_paths = [ppmi_turboprep_data[p] for p in ppmi_common_subjects]\n",
    "ppmi_fs_features     = np.array([ppmi_fs_data[p]        for p in ppmi_common_subjects])\n",
    "ppmi_sex_labels      = np.array([ppmi_id_sex_dict[p]    for p in ppmi_common_subjects])\n",
    "ppmi_parkinson_labels= np.array([ppmi_id_parkinson_dict[p] for p in ppmi_common_subjects])\n",
    "ppmi_age_labels      = np.array([ppmi_id_age_dict[p]    for p in ppmi_common_subjects])\n",
    "\n",
    "ppmi_anatcl_features, ppmi_anatcl_local_features, ppmi_swinbrain_features, ppmi_cnn_features_dict = \\\n",
    "    extract_features_ppmi(ppmi_cat12_paths, ppmi_turboprep_paths, device)\n",
    "\n",
    "ppmi_features_dict = {\n",
    "    'AnatCL_Global': ppmi_anatcl_features,\n",
    "    'AnatCL_Local':  ppmi_anatcl_local_features,\n",
    "    'SwinBrain':     ppmi_swinbrain_features,\n",
    "    **ppmi_cnn_features_dict,\n",
    "    'FreeSurfer':    ppmi_fs_features,\n",
    "}\n",
    "\n",
    "compute_pairwise_correlations(ppmi_features_dict, \"PPMI\")\n",
    "\n",
    "print(\"\\nComputing PPMI correlation matrix...\")\n",
    "ppmi_corr_matrix, ppmi_boundaries, ppmi_model_names, ppmi_reorder_indices, ppmi_cluster_dfs = \\\n",
    "    compute_correlation_with_cluster_labels(ppmi_features_dict, {'FreeSurfer': ppmi_fs_feature_info}, ['FreeSurfer'], \"PPMI\")\n",
    "\n",
    "ppmi_models = list(ppmi_features_dict.keys())\n",
    "ppmi_sex_results       = run_classification(ppmi_features_dict, ppmi_sex_labels,       task_name=\"PPMI Sex Classification\")\n",
    "ppmi_parkinson_results = run_classification(ppmi_features_dict, ppmi_parkinson_labels,  task_name=\"PPMI Parkinson Classification\")\n",
    "\n",
    "ppmi_male_indices   = np.where(ppmi_sex_labels == 0)[0]\n",
    "ppmi_male_features_dict  = {k: v[ppmi_male_indices]   for k, v in ppmi_features_dict.items()}\n",
    "ppmi_male_age_labels     = ppmi_age_labels[ppmi_male_indices]\n",
    "print(f\"\\nPPMI Male: {len(ppmi_male_indices)} subjects\")\n",
    "ppmi_male_age_results = run_regression(ppmi_male_features_dict, ppmi_male_age_labels, task_name=\"PPMI Male Age\")\n",
    "\n",
    "ppmi_female_indices = np.where(ppmi_sex_labels == 1)[0]\n",
    "ppmi_female_features_dict = {k: v[ppmi_female_indices] for k, v in ppmi_features_dict.items()}\n",
    "ppmi_female_age_labels    = ppmi_age_labels[ppmi_female_indices]\n",
    "print(f\"\\nPPMI Female: {len(ppmi_female_indices)} subjects\")\n",
    "ppmi_female_age_results = run_regression(ppmi_female_features_dict, ppmi_female_age_labels, task_name=\"PPMI Female Age\")\n",
    "\n",
    "# HBN Dataset\n",
    "print(\"HBN Dataset\")\n",
    "\n",
    "HBN_BIDS          = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/HBN_BIDS\"\n",
    "HBN_BIDS_LOWER    = \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/hbn_bids\"\n",
    "HBN_FREESURFER_DIR= \"/home/arelbaha/links/projects/rrg-glatard/arelbaha/HBN_FreeSurfer/freesurfer\"\n",
    "HBN_TURBOPREP_DIR = \"/home/arelbaha/links/scratch/turboprep_output\"\n",
    "HBN_DEMO_FILE     = os.path.join(HBN_BIDS, \"final_preprocessed_subjects_with_demographics.tsv\")\n",
    "HBN_PARCELLATION  = \"Schaefer2018_400Parcels_17Networks_order\"\n",
    "\n",
    "print(\"Loading HBN demographics\")\n",
    "hbn_demo_df = pd.read_csv(HBN_DEMO_FILE, sep='\\t')\n",
    "hbn_demo_df['participant_id'] = hbn_demo_df['participant_id'].astype(str)\n",
    "hbn_id_sex_dict, hbn_id_age_dict = {}, {}\n",
    "for _, row in hbn_demo_df.iterrows():\n",
    "    hbn_id_sex_dict[row['participant_id']] = 1 if row['sex'].strip() == 'Female' else 0\n",
    "    hbn_id_age_dict[row['participant_id']] = row['age']\n",
    "print(f\"  {len(hbn_id_sex_dict)} subjects\")\n",
    "\n",
    "print(\"Finding HBN files\")\n",
    "hbn_cat12_data = {}\n",
    "for subject_id in hbn_id_sex_dict.keys():\n",
    "    for base in [HBN_BIDS, HBN_BIDS_LOWER]:\n",
    "        files = glob.glob(os.path.join(base, f\"sub-{subject_id}\", \"ses-*\", \"anat\", \"mri\", \"mwp1sub*.nii\"))\n",
    "        if files:\n",
    "            hbn_cat12_data[subject_id] = files[0]\n",
    "            break\n",
    "\n",
    "print(\"Finding HBN TurboPrep files\")\n",
    "hbn_turboprep_data = {}\n",
    "for folder in os.listdir(HBN_TURBOPREP_DIR):\n",
    "    folder_path = os.path.join(HBN_TURBOPREP_DIR, folder)\n",
    "    if os.path.isdir(folder_path) and folder.startswith('NDAR'):\n",
    "        normalized_file = os.path.join(folder_path, \"normalized.nii.gz\")\n",
    "        if os.path.exists(normalized_file) and folder in hbn_id_sex_dict:\n",
    "            hbn_turboprep_data[folder] = normalized_file\n",
    "print(f\"  Found {len(hbn_turboprep_data)} TurboPrep subjects\")\n",
    "\n",
    "print(\"Loading HBN FreeSurfer Schaefer\")\n",
    "hbn_fs_data, hbn_fs_region_info = {}, {}\n",
    "for subject_id in hbn_id_sex_dict.keys():\n",
    "    stats_file = os.path.join(HBN_FREESURFER_DIR, f\"sub-{subject_id}\", f\"sub-{subject_id}_regionsurfacestats.tsv\")\n",
    "    if os.path.exists(stats_file):\n",
    "        df = pd.read_csv(stats_file, sep='\\t')\n",
    "        fdf = df[df[\"atlas\"] == HBN_PARCELLATION].sort_values(\"StructName\")\n",
    "        if not fdf.empty and \"SurfArea\" in fdf.columns and \"ThickAvg\" in fdf.columns:\n",
    "            sa = fdf[\"SurfArea\"].values[:400]; th = fdf[\"ThickAvg\"].values[:400]\n",
    "            combined = np.concatenate([sa, th])\n",
    "            if not np.any(np.isnan(combined)) and len(sa) == 400 and len(th) == 400:\n",
    "                hbn_fs_data[subject_id] = combined\n",
    "                if not hbn_fs_region_info:\n",
    "                    hbn_fs_region_info = {'region_names': fdf['StructName'].values[:400].tolist(),\n",
    "                                          'hemisphere': fdf['hemisphere'].values[:400].tolist()}\n",
    "\n",
    "hbn_fs_feature_info = []\n",
    "if hbn_fs_region_info:\n",
    "    for i in range(400):\n",
    "        hbn_fs_feature_info.append({'feature_name': hbn_fs_region_info['region_names'][i], 'feature_type': 'Surface_Area', 'hemisphere': hbn_fs_region_info['hemisphere'][i]})\n",
    "    for i in range(400):\n",
    "        hbn_fs_feature_info.append({'feature_name': hbn_fs_region_info['region_names'][i], 'feature_type': 'Thickness', 'hemisphere': hbn_fs_region_info['hemisphere'][i]})\n",
    "\n",
    "print(\"Loading HBN FreeSurfer aparc\")\n",
    "hbn_aparc_fs_data, hbn_aparc_region_info = {}, {}\n",
    "for subject_id in hbn_id_sex_dict.keys():\n",
    "    stats_file = os.path.join(HBN_FREESURFER_DIR, f\"sub-{subject_id}\", f\"sub-{subject_id}_regionsurfacestats.tsv\")\n",
    "    if os.path.exists(stats_file):\n",
    "        df = pd.read_csv(stats_file, sep='\\t')\n",
    "        fdf = df[df[\"atlas\"] == \"aparc\"].sort_values(\"StructName\")\n",
    "        if not fdf.empty and \"SurfArea\" in fdf.columns and \"ThickAvg\" in fdf.columns:\n",
    "            sa = fdf[\"SurfArea\"].values; th = fdf[\"ThickAvg\"].values\n",
    "            combined = np.concatenate([th, sa])\n",
    "            if not np.any(np.isnan(combined)) and len(sa) == 68 and len(th) == 68:\n",
    "                hbn_aparc_fs_data[subject_id] = combined\n",
    "                if not hbn_aparc_region_info:\n",
    "                    hbn_aparc_region_info = {'region_names': fdf['StructName'].values.tolist(),\n",
    "                                             'hemisphere': fdf['hemisphere'].values.tolist()}\n",
    "\n",
    "hbn_aparc_fs_feature_info = []\n",
    "if hbn_aparc_region_info:\n",
    "    for i in range(68):\n",
    "        hbn_aparc_fs_feature_info.append({'feature_name': hbn_aparc_region_info['region_names'][i], 'feature_type': 'Thickness', 'hemisphere': hbn_aparc_region_info['hemisphere'][i]})\n",
    "    for i in range(68):\n",
    "        hbn_aparc_fs_feature_info.append({'feature_name': hbn_aparc_region_info['region_names'][i], 'feature_type': 'Surface_Area', 'hemisphere': hbn_aparc_region_info['hemisphere'][i]})\n",
    "\n",
    "hbn_common_subjects = sorted(list(set(hbn_cat12_data.keys()) & set(hbn_turboprep_data.keys()) &\n",
    "                                   set(hbn_fs_data.keys()) & set(hbn_aparc_fs_data.keys())))\n",
    "print(f\"Common subjects: {len(hbn_common_subjects)}\")\n",
    "\n",
    "hbn_cat12_paths      = [hbn_cat12_data[s]      for s in hbn_common_subjects]\n",
    "hbn_turboprep_paths  = [hbn_turboprep_data[s]  for s in hbn_common_subjects]\n",
    "hbn_fs_features      = np.array([hbn_fs_data[s]       for s in hbn_common_subjects])\n",
    "hbn_aparc_fs_features= np.array([hbn_aparc_fs_data[s] for s in hbn_common_subjects])\n",
    "hbn_sex_labels       = np.array([hbn_id_sex_dict[s]   for s in hbn_common_subjects])\n",
    "hbn_age_labels       = np.array([hbn_id_age_dict[s]   for s in hbn_common_subjects])\n",
    "\n",
    "hbn_anatcl_features, hbn_anatcl_local_features, hbn_brainiac_features, hbn_swinbrain_features, hbn_cnn_features_dict, hbn_simclr_features = \\\n",
    "    extract_features_full(hbn_cat12_paths, hbn_turboprep_paths, device)\n",
    "\n",
    "hbn_features_dict = {\n",
    "    'AnatCL_Global':    hbn_anatcl_features,\n",
    "    'AnatCL_Local':     hbn_anatcl_local_features,\n",
    "    'BrainIAC':         hbn_brainiac_features,\n",
    "    'SwinBrain':        hbn_swinbrain_features,\n",
    "    **hbn_cnn_features_dict,\n",
    "    '3D-Neuro-SimCLR':  hbn_simclr_features,\n",
    "    'FS_Schaefer':      hbn_fs_features,\n",
    "    'FS_aparc':         hbn_aparc_fs_features,\n",
    "}\n",
    "\n",
    "compute_pairwise_correlations(hbn_features_dict, \"HBN\")\n",
    "\n",
    "print(\"\\nComputing HBN correlation matrix...\")\n",
    "hbn_corr_matrix, hbn_boundaries, hbn_model_names, hbn_reorder_indices, hbn_cluster_dfs = \\\n",
    "    compute_correlation_with_cluster_labels(hbn_features_dict,\n",
    "        {'FS_Schaefer': hbn_fs_feature_info, 'FS_aparc': hbn_aparc_fs_feature_info},\n",
    "        ['FS_Schaefer', 'FS_aparc'], \"HBN\")\n",
    "\n",
    "hbn_models = list(hbn_features_dict.keys())\n",
    "hbn_sex_results = run_classification(hbn_features_dict, hbn_sex_labels, task_name=\"HBN Sex Classification\")\n",
    "\n",
    "hbn_male_indices  = np.where(hbn_sex_labels == 0)[0]\n",
    "hbn_male_features_dict  = {k: v[hbn_male_indices]  for k, v in hbn_features_dict.items()}\n",
    "hbn_male_age_labels     = hbn_age_labels[hbn_male_indices]\n",
    "print(f\"\\nHBN Male: {len(hbn_male_indices)} subjects\")\n",
    "hbn_male_age_results = run_regression(hbn_male_features_dict, hbn_male_age_labels, task_name=\"HBN Male Age\")\n",
    "\n",
    "hbn_female_indices = np.where(hbn_sex_labels == 1)[0]\n",
    "hbn_female_features_dict  = {k: v[hbn_female_indices] for k, v in hbn_features_dict.items()}\n",
    "hbn_female_age_labels     = hbn_age_labels[hbn_female_indices]\n",
    "print(f\"\\nHBN Female: {len(hbn_female_indices)} subjects\")\n",
    "hbn_female_age_results = run_regression(hbn_female_features_dict, hbn_female_age_labels, task_name=\"HBN Female Age\")\n",
    "\n",
    "# NKI Dataset\n",
    "print(\"NKI Dataset\")\n",
    "\n",
    "NKI_BASE          = \"/home/arelbaha/links/scratch/NKI\"\n",
    "NKI_BIDS_DIR      = os.path.join(NKI_BASE, \"NKI_BIDS\")\n",
    "NKI_TURBOPREP_DIR = \"/home/arelbaha/links/scratch/turboprep_NKI\"\n",
    "NKI_FREESURFER_DIR= os.path.join(NKI_BASE, \"NKI_FreeSurfer\", \"freesurfer\")\n",
    "NKI_DEMO_FILE     = os.path.join(NKI_BASE, \"final_demographics.txt\")\n",
    "NKI_SUBJECTS_FILE = os.path.join(NKI_BASE, \"final_subjects.txt\")\n",
    "NKI_PARCELLATION  = \"Schaefer2018_400Parcels_17Networks_order\"\n",
    "\n",
    "print(\"Loading NKI demographics\")\n",
    "nki_id_sex_dict, nki_id_age_dict = {}, {}\n",
    "with open(NKI_DEMO_FILE) as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) == 3:\n",
    "            subj_id, sex, age = parts\n",
    "            nki_id_sex_dict[subj_id] = 1 if sex.strip() == 'Female' else 0\n",
    "            nki_id_age_dict[subj_id] = float(age)\n",
    "print(f\"  {len(nki_id_sex_dict)} subjects from demographics\")\n",
    "\n",
    "print(\"Finding NKI CAT12 files\")\n",
    "nki_cat12_data = {}\n",
    "for subj_id in nki_id_sex_dict.keys():\n",
    "    files = glob.glob(os.path.join(NKI_BIDS_DIR, f\"sub-{subj_id}\", \"ses-BAS1\", \"anat\", \"mri\", \"mwp1*.nii\"))\n",
    "    if files:\n",
    "        nki_cat12_data[subj_id] = files[0]\n",
    "print(f\"  Found {len(nki_cat12_data)} CAT12 subjects\")\n",
    "\n",
    "print(\"Finding NKI TurboPrep files\")\n",
    "nki_turboprep_data = {}\n",
    "for folder in os.listdir(NKI_TURBOPREP_DIR):\n",
    "    folder_path = os.path.join(NKI_TURBOPREP_DIR, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        normalized_file = os.path.join(folder_path, \"normalized.nii.gz\")\n",
    "        if os.path.exists(normalized_file) and folder in nki_id_sex_dict:\n",
    "            nki_turboprep_data[folder] = normalized_file\n",
    "print(f\"  Found {len(nki_turboprep_data)} TurboPrep subjects\")\n",
    "\n",
    "print(\"Loading NKI FreeSurfer Schaefer\")\n",
    "nki_fs_data, nki_fs_region_info = {}, {}\n",
    "for subj_id in nki_id_sex_dict.keys():\n",
    "    stats_file = os.path.join(NKI_FREESURFER_DIR, f\"sub-{subj_id}_ses-BAS1\", f\"sub-{subj_id}_ses-BAS1_regionsurfacestats.tsv\")\n",
    "    if os.path.exists(stats_file):\n",
    "        df = pd.read_csv(stats_file, sep='\\t')\n",
    "        fdf = df[df[\"atlas\"] == NKI_PARCELLATION].sort_values(\"StructName\")\n",
    "        if not fdf.empty and \"SurfArea\" in fdf.columns and \"ThickAvg\" in fdf.columns:\n",
    "            sa = fdf[\"SurfArea\"].values[:400]; th = fdf[\"ThickAvg\"].values[:400]\n",
    "            combined = np.concatenate([sa, th])\n",
    "            if not np.any(np.isnan(combined)) and len(sa) == 400 and len(th) == 400:\n",
    "                nki_fs_data[subj_id] = combined\n",
    "                if not nki_fs_region_info:\n",
    "                    nki_fs_region_info = {'region_names': fdf['StructName'].values[:400].tolist(),\n",
    "                                          'hemisphere': fdf['hemisphere'].values[:400].tolist()}\n",
    "\n",
    "nki_fs_feature_info = []\n",
    "if nki_fs_region_info:\n",
    "    for i in range(400):\n",
    "        nki_fs_feature_info.append({'feature_name': nki_fs_region_info['region_names'][i], 'feature_type': 'Surface_Area', 'hemisphere': nki_fs_region_info['hemisphere'][i]})\n",
    "    for i in range(400):\n",
    "        nki_fs_feature_info.append({'feature_name': nki_fs_region_info['region_names'][i], 'feature_type': 'Thickness', 'hemisphere': nki_fs_region_info['hemisphere'][i]})\n",
    "\n",
    "print(\"Loading NKI FreeSurfer aparc\")\n",
    "nki_aparc_fs_data, nki_aparc_region_info = {}, {}\n",
    "for subj_id in nki_id_sex_dict.keys():\n",
    "    stats_file = os.path.join(NKI_FREESURFER_DIR, f\"sub-{subj_id}_ses-BAS1\", f\"sub-{subj_id}_ses-BAS1_regionsurfacestats.tsv\")\n",
    "    if os.path.exists(stats_file):\n",
    "        df = pd.read_csv(stats_file, sep='\\t')\n",
    "        fdf = df[df[\"atlas\"] == \"aparc\"].sort_values(\"StructName\")\n",
    "        if not fdf.empty and \"SurfArea\" in fdf.columns and \"ThickAvg\" in fdf.columns:\n",
    "            sa = fdf[\"SurfArea\"].values; th = fdf[\"ThickAvg\"].values\n",
    "            combined = np.concatenate([th, sa])\n",
    "            if not np.any(np.isnan(combined)) and len(sa) == 68 and len(th) == 68:\n",
    "                nki_aparc_fs_data[subj_id] = combined\n",
    "                if not nki_aparc_region_info:\n",
    "                    nki_aparc_region_info = {'region_names': fdf['StructName'].values.tolist(),\n",
    "                                             'hemisphere': fdf['hemisphere'].values.tolist()}\n",
    "\n",
    "nki_aparc_fs_feature_info = []\n",
    "if nki_aparc_region_info:\n",
    "    for i in range(68):\n",
    "        nki_aparc_fs_feature_info.append({'feature_name': nki_aparc_region_info['region_names'][i], 'feature_type': 'Thickness', 'hemisphere': nki_aparc_region_info['hemisphere'][i]})\n",
    "    for i in range(68):\n",
    "        nki_aparc_fs_feature_info.append({'feature_name': nki_aparc_region_info['region_names'][i], 'feature_type': 'Surface_Area', 'hemisphere': nki_aparc_region_info['hemisphere'][i]})\n",
    "\n",
    "nki_common_subjects = sorted(list(\n",
    "    set(nki_cat12_data.keys()) & set(nki_turboprep_data.keys()) &\n",
    "    set(nki_fs_data.keys()) & set(nki_aparc_fs_data.keys())\n",
    "))\n",
    "print(f\"Common subjects: {len(nki_common_subjects)}\")\n",
    "\n",
    "nki_cat12_paths      = [nki_cat12_data[s]       for s in nki_common_subjects]\n",
    "nki_turboprep_paths  = [nki_turboprep_data[s]   for s in nki_common_subjects]\n",
    "nki_fs_features      = np.array([nki_fs_data[s]       for s in nki_common_subjects])\n",
    "nki_aparc_fs_features= np.array([nki_aparc_fs_data[s] for s in nki_common_subjects])\n",
    "nki_sex_labels       = np.array([nki_id_sex_dict[s]   for s in nki_common_subjects])\n",
    "nki_age_labels       = np.array([nki_id_age_dict[s]   for s in nki_common_subjects])\n",
    "\n",
    "nki_anatcl_features, nki_anatcl_local_features, nki_brainiac_features, nki_swinbrain_features, nki_cnn_features_dict, nki_simclr_features = \\\n",
    "    extract_features_full(nki_cat12_paths, nki_turboprep_paths, device)\n",
    "\n",
    "nki_features_dict = {\n",
    "    'AnatCL_Global':    nki_anatcl_features,\n",
    "    'AnatCL_Local':     nki_anatcl_local_features,\n",
    "    'BrainIAC':         nki_brainiac_features,\n",
    "    'SwinBrain':        nki_swinbrain_features,\n",
    "    **nki_cnn_features_dict,\n",
    "    '3D-Neuro-SimCLR':  nki_simclr_features,\n",
    "    'FS_Schaefer':      nki_fs_features,\n",
    "    'FS_aparc':         nki_aparc_fs_features,\n",
    "}\n",
    "\n",
    "compute_pairwise_correlations(nki_features_dict, \"NKI\")\n",
    "\n",
    "print(\"\\nComputing NKI correlation matrix...\")\n",
    "nki_corr_matrix, nki_boundaries, nki_model_names, nki_reorder_indices, nki_cluster_dfs = \\\n",
    "    compute_correlation_with_cluster_labels(nki_features_dict,\n",
    "        {'FS_Schaefer': nki_fs_feature_info, 'FS_aparc': nki_aparc_fs_feature_info},\n",
    "        ['FS_Schaefer', 'FS_aparc'], \"NKI\")\n",
    "\n",
    "nki_models = list(nki_features_dict.keys())\n",
    "nki_sex_results = run_classification(nki_features_dict, nki_sex_labels, task_name=\"NKI Sex Classification\")\n",
    "\n",
    "nki_male_indices  = np.where(nki_sex_labels == 0)[0]\n",
    "nki_male_features_dict  = {k: v[nki_male_indices]  for k, v in nki_features_dict.items()}\n",
    "nki_male_age_labels     = nki_age_labels[nki_male_indices]\n",
    "print(f\"\\nNKI Male: {len(nki_male_indices)} subjects\")\n",
    "nki_male_age_results = run_regression(nki_male_features_dict, nki_male_age_labels, task_name=\"NKI Male Age\")\n",
    "\n",
    "nki_female_indices = np.where(nki_sex_labels == 1)[0]\n",
    "nki_female_features_dict  = {k: v[nki_female_indices] for k, v in nki_features_dict.items()}\n",
    "nki_female_age_labels     = nki_age_labels[nki_female_indices]\n",
    "print(f\"\\nNKI Female: {len(nki_female_indices)} subjects\")\n",
    "nki_female_age_results = run_regression(nki_female_features_dict, nki_female_age_labels, task_name=\"NKI Female Age\")\n",
    "\n",
    "# Permutation Tests\n",
    "\n",
    "print(\"\\nPermutation Tests (Bonferroni corrected)\")\n",
    "\n",
    "print(\"\\nPPMI Sex:\")\n",
    "ppmi_sex_perm = run_permutation_test_classification(ppmi_sex_results, 'FreeSurfer', ppmi_models)\n",
    "print(\"\\nPPMI Male Age:\")\n",
    "ppmi_male_age_perm = run_permutation_test_regression(ppmi_male_age_results, 'FreeSurfer', ppmi_models)\n",
    "print(\"\\nPPMI Female Age:\")\n",
    "ppmi_female_age_perm = run_permutation_test_regression(ppmi_female_age_results, 'FreeSurfer', ppmi_models)\n",
    "\n",
    "print(\"\\nHBN Sex (vs Schaefer):\")\n",
    "hbn_sex_perm_schaefer = run_permutation_test_classification(hbn_sex_results, 'FS_Schaefer', hbn_models)\n",
    "print(\"\\nHBN Sex (vs aparc):\")\n",
    "hbn_sex_perm_aparc = run_permutation_test_classification(hbn_sex_results, 'FS_aparc', hbn_models)\n",
    "print(\"\\nHBN Male Age (vs Schaefer):\")\n",
    "hbn_male_perm_schaefer = run_permutation_test_regression(hbn_male_age_results, 'FS_Schaefer', hbn_models)\n",
    "print(\"\\nHBN Male Age (vs aparc):\")\n",
    "hbn_male_perm_aparc = run_permutation_test_regression(hbn_male_age_results, 'FS_aparc', hbn_models)\n",
    "print(\"\\nHBN Female Age (vs Schaefer):\")\n",
    "hbn_female_perm_schaefer = run_permutation_test_regression(hbn_female_age_results, 'FS_Schaefer', hbn_models)\n",
    "print(\"\\nHBN Female Age (vs aparc):\")\n",
    "hbn_female_perm_aparc = run_permutation_test_regression(hbn_female_age_results, 'FS_aparc', hbn_models)\n",
    "\n",
    "print(\"\\nNKI Sex (vs Schaefer):\")\n",
    "nki_sex_perm_schaefer = run_permutation_test_classification(nki_sex_results, 'FS_Schaefer', nki_models)\n",
    "print(\"\\nNKI Sex (vs aparc):\")\n",
    "nki_sex_perm_aparc = run_permutation_test_classification(nki_sex_results, 'FS_aparc', nki_models)\n",
    "print(\"\\nNKI Male Age (vs Schaefer):\")\n",
    "nki_male_perm_schaefer = run_permutation_test_regression(nki_male_age_results, 'FS_Schaefer', nki_models)\n",
    "print(\"\\nNKI Male Age (vs aparc):\")\n",
    "nki_male_perm_aparc = run_permutation_test_regression(nki_male_age_results, 'FS_aparc', nki_models)\n",
    "print(\"\\nNKI Female Age (vs Schaefer):\")\n",
    "nki_female_perm_schaefer = run_permutation_test_regression(nki_female_age_results, 'FS_Schaefer', nki_models)\n",
    "print(\"\\nNKI Female Age (vs aparc):\")\n",
    "nki_female_perm_aparc = run_permutation_test_regression(nki_female_age_results, 'FS_aparc', nki_models)\n",
    "\n",
    "# Correlation Plots & Exports\n",
    "\n",
    "plot_correlation_with_labels(ppmi_corr_matrix, ppmi_boundaries, ppmi_model_names, ppmi_cluster_dfs, ['FreeSurfer'], \"ppmi\")\n",
    "plot_correlation_with_labels(hbn_corr_matrix,  hbn_boundaries,  hbn_model_names,  hbn_cluster_dfs,  ['FS_Schaefer', 'FS_aparc'], \"hbn\")\n",
    "plot_correlation_with_labels(nki_corr_matrix,  nki_boundaries,  nki_model_names,  nki_cluster_dfs,  ['FS_Schaefer', 'FS_aparc'], \"nki\")\n",
    "\n",
    "export_correlation_data(ppmi_corr_matrix, ppmi_boundaries, ppmi_model_names, ppmi_cluster_dfs, \"PPMI\")\n",
    "export_correlation_data(hbn_corr_matrix,  hbn_boundaries,  hbn_model_names,  hbn_cluster_dfs,  \"HBN\")\n",
    "export_correlation_data(nki_corr_matrix,  nki_boundaries,  nki_model_names,  nki_cluster_dfs,  \"NKI\")\n",
    "\n",
    "ppmi_cluster_dfs['FreeSurfer'].to_csv('ppmi_freesurfer_features_reordered.csv', index=False)\n",
    "hbn_cluster_dfs['FS_Schaefer'].to_csv('hbn_schaefer_freesurfer_features_reordered.csv', index=False)\n",
    "hbn_cluster_dfs['FS_aparc'].to_csv('hbn_aparc_freesurfer_features_reordered.csv', index=False)\n",
    "nki_cluster_dfs['FS_Schaefer'].to_csv('nki_schaefer_freesurfer_features_reordered.csv', index=False)\n",
    "nki_cluster_dfs['FS_aparc'].to_csv('nki_aparc_freesurfer_features_reordered.csv', index=False)\n",
    "\n",
    "# CV Results Export\n",
    "ppmi_class_sizes  = get_training_sizes(len(ppmi_sex_labels),    alphas)\n",
    "ppmi_male_sizes   = get_training_sizes(len(ppmi_male_indices),  alphas)\n",
    "ppmi_female_sizes = get_training_sizes(len(ppmi_female_indices),alphas)\n",
    "hbn_class_sizes   = get_training_sizes(len(hbn_sex_labels),     alphas)\n",
    "hbn_male_sizes    = get_training_sizes(len(hbn_male_indices),   alphas)\n",
    "hbn_female_sizes  = get_training_sizes(len(hbn_female_indices), alphas)\n",
    "nki_class_sizes   = get_training_sizes(len(nki_sex_labels),     alphas)\n",
    "nki_male_sizes    = get_training_sizes(len(nki_male_indices),   alphas)\n",
    "nki_female_sizes  = get_training_sizes(len(nki_female_indices), alphas)\n",
    "\n",
    "ppmi_sex_cv_df        = export_cv_results_classification(ppmi_sex_results,       ppmi_class_sizes,  'PPMI', 'Sex',       ppmi_models)\n",
    "ppmi_park_cv_df       = export_cv_results_classification(ppmi_parkinson_results,  ppmi_class_sizes,  'PPMI', 'Parkinson', ppmi_models)\n",
    "ppmi_male_age_cv_df   = export_cv_results_regression    (ppmi_male_age_results,   ppmi_male_sizes,   'PPMI', 'Male_Age',  ppmi_models)\n",
    "ppmi_female_age_cv_df = export_cv_results_regression    (ppmi_female_age_results, ppmi_female_sizes, 'PPMI', 'Female_Age',ppmi_models)\n",
    "\n",
    "hbn_sex_cv_df         = export_cv_results_classification(hbn_sex_results,         hbn_class_sizes,   'HBN',  'Sex',       hbn_models)\n",
    "hbn_male_age_cv_df    = export_cv_results_regression    (hbn_male_age_results,    hbn_male_sizes,    'HBN',  'Male_Age',  hbn_models)\n",
    "hbn_female_age_cv_df  = export_cv_results_regression    (hbn_female_age_results,  hbn_female_sizes,  'HBN',  'Female_Age',hbn_models)\n",
    "\n",
    "nki_sex_cv_df         = export_cv_results_classification(nki_sex_results,         nki_class_sizes,   'NKI',  'Sex',       nki_models)\n",
    "nki_male_age_cv_df    = export_cv_results_regression    (nki_male_age_results,    nki_male_sizes,    'NKI',  'Male_Age',  nki_models)\n",
    "nki_female_age_cv_df  = export_cv_results_regression    (nki_female_age_results,  nki_female_sizes,  'NKI',  'Female_Age',nki_models)\n",
    "\n",
    "ppmi_classification_cv = pd.concat([ppmi_sex_cv_df, ppmi_park_cv_df], ignore_index=True)\n",
    "ppmi_regression_cv     = pd.concat([ppmi_male_age_cv_df, ppmi_female_age_cv_df], ignore_index=True)\n",
    "hbn_classification_cv  = hbn_sex_cv_df\n",
    "hbn_regression_cv      = pd.concat([hbn_male_age_cv_df, hbn_female_age_cv_df], ignore_index=True)\n",
    "nki_classification_cv  = nki_sex_cv_df\n",
    "nki_regression_cv      = pd.concat([nki_male_age_cv_df, nki_female_age_cv_df], ignore_index=True)\n",
    "\n",
    "ppmi_classification_cv.to_csv('ppmi_classification_cv_results.csv', index=False)\n",
    "ppmi_regression_cv.to_csv('ppmi_regression_cv_results.csv', index=False)\n",
    "hbn_classification_cv.to_csv('hbn_classification_cv_results.csv', index=False)\n",
    "hbn_regression_cv.to_csv('hbn_regression_cv_results.csv', index=False)\n",
    "nki_classification_cv.to_csv('nki_classification_cv_results.csv', index=False)\n",
    "nki_regression_cv.to_csv('nki_regression_cv_results.csv', index=False)\n",
    "\n",
    "all_cv_results = pd.concat([\n",
    "    ppmi_classification_cv.assign(task_type='classification'),\n",
    "    ppmi_regression_cv.assign(task_type='regression'),\n",
    "    hbn_classification_cv.assign(task_type='classification'),\n",
    "    hbn_regression_cv.assign(task_type='regression'),\n",
    "    nki_classification_cv.assign(task_type='classification'),\n",
    "    nki_regression_cv.assign(task_type='regression'),\n",
    "], ignore_index=True)\n",
    "all_cv_results.to_csv('all_cv_results.csv', index=False)\n",
    "\n",
    "# Fold-level CV results (5 seeds x 5 folds = 25 per model, alpha=1)\n",
    "\n",
    "fold_rows = []\n",
    "for ds_name, task_results_list in [\n",
    "    ('PPMI', [('Sex', ppmi_sex_results), ('Parkinson', ppmi_parkinson_results),\n",
    "              ('Male_Age', ppmi_male_age_results), ('Female_Age', ppmi_female_age_results)]),\n",
    "    ('HBN',  [('Sex', hbn_sex_results),  ('Male_Age', hbn_male_age_results),\n",
    "              ('Female_Age', hbn_female_age_results)]),\n",
    "    ('NKI',  [('Sex', nki_sex_results),  ('Male_Age', nki_male_age_results),\n",
    "              ('Female_Age', nki_female_age_results)]),\n",
    "]:\n",
    "    for task, res in task_results_list:\n",
    "        for model, r in res.items():\n",
    "            if 'Dummy' in model:\n",
    "                continue\n",
    "            fr = r.get('fold_results_alpha1', {})\n",
    "            if 'val_acc' in fr:\n",
    "                for i, val in enumerate(fr['val_acc']):\n",
    "                    fold_rows.append({'dataset': ds_name, 'task': task, 'model': model, 'fold': i, 'metric': 'val_acc', 'value': val})\n",
    "            if 'val_mae' in fr:\n",
    "                for i, val in enumerate(fr['val_mae']):\n",
    "                    fold_rows.append({'dataset': ds_name, 'task': task, 'model': model, 'fold': i, 'metric': 'val_mae', 'value': val})\n",
    "\n",
    "pd.DataFrame(fold_rows).to_csv('fold_level_results.csv', index=False)\n",
    "print(f\"Saved: fold_level_results.csv ({len(fold_rows)} rows)\")\n",
    "\n",
    "# Held-out test set results (5 seeds x 1 test eval = 5 per model)\n",
    "\n",
    "heldout_rows = []\n",
    "for ds_name, task_results_list in [\n",
    "    ('PPMI', [('Sex', ppmi_sex_results), ('Parkinson', ppmi_parkinson_results),\n",
    "              ('Male_Age', ppmi_male_age_results), ('Female_Age', ppmi_female_age_results)]),\n",
    "    ('HBN',  [('Sex', hbn_sex_results),  ('Male_Age', hbn_male_age_results),\n",
    "              ('Female_Age', hbn_female_age_results)]),\n",
    "    ('NKI',  [('Sex', nki_sex_results),  ('Male_Age', nki_male_age_results),\n",
    "              ('Female_Age', nki_female_age_results)]),\n",
    "]:\n",
    "    for task, res in task_results_list:\n",
    "        for model, r in res.items():\n",
    "            if 'Dummy' in model:\n",
    "                continue\n",
    "            tsr = r.get('test_seed_results', {})\n",
    "            if 'test_acc' in tsr:\n",
    "                for i, val in enumerate(tsr['test_acc']):\n",
    "                    heldout_rows.append({'dataset': ds_name, 'task': task, 'model': model,\n",
    "                                         'seed': SEEDS[i], 'metric': 'test_acc', 'value': val})\n",
    "            if 'test_auc' in tsr:\n",
    "                for i, val in enumerate(tsr['test_auc']):\n",
    "                    heldout_rows.append({'dataset': ds_name, 'task': task, 'model': model,\n",
    "                                         'seed': SEEDS[i], 'metric': 'test_auc', 'value': val})\n",
    "            if 'test_mae' in tsr:\n",
    "                for i, val in enumerate(tsr['test_mae']):\n",
    "                    heldout_rows.append({'dataset': ds_name, 'task': task, 'model': model,\n",
    "                                         'seed': SEEDS[i], 'metric': 'test_mae', 'value': val})\n",
    "            if 'test_r2' in tsr:\n",
    "                for i, val in enumerate(tsr['test_r2']):\n",
    "                    heldout_rows.append({'dataset': ds_name, 'task': task, 'model': model,\n",
    "                                         'seed': SEEDS[i], 'metric': 'test_r2', 'value': val})\n",
    "\n",
    "pd.DataFrame(heldout_rows).to_csv('heldout_seed_results.csv', index=False)\n",
    "print(f\"Saved: heldout_seed_results.csv ({len(heldout_rows)} rows)\")\n",
    "\n",
    "print(\"\\nAll Files Saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
